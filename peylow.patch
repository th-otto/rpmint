diff -rupN gcc-4.6.4.test/README.md gcc-4.6.4-fastcall/README.md
--- gcc-4.6.4.test/README.md	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.6.4-fastcall/README.md	2017-04-30 19:10:52.143947001 +0200
@@ -0,0 +1,46 @@
+gcc-4.6.4
+=========
+
+Patch to gcc for supporting "fastcall" and "regparm" on m68k target.
+
+By default gcc pass all arguments on the stack for the m68k target, 
+this is not always optimal when memory access is slow. This default
+abi is from now an named the `stkparm` abi.
+
+A new abi named `fastcall` has been added, where registers d0-d2, 
+a0-a1 and fp0-fp2 are used to pass registers to functions when possible.
+A third option is to use the `regparam` option to specify the number
+of registers of each class to use.
+
+At a minimum d0-d1, a0-a1, fp0-fp1 are clobbered by function calls.
+Any further register potentially used for the arguments for the
+functions abi is also clobbered, that is that d2 and a2 are also 
+concidered clobbered for `fastcall`.
+
+32 bit float are passed in registers when 68881 is not enabled, when
+enabled fp0-fp2 are used for all floating point types.
+
+All build-in libgcc functions use the `fastcall` abi, this means that
+any library being used must also be rebuilt. This is most useful on
+plain 68000 where short helper functions for 32bit int are frequently
+called.
+
+Only named arguments for variadric functions can use registers, all
+unnamed arguments are always passed on the stack.
+
+Examples
+--------
+
+    // a passed in d0
+    // b passed in d1
+    __attribute__((fastcall)) int foo(int a, int b);
+    
+    // a passed in d0
+    // b passed in a0
+    // c passed in d1
+    __attribute__((fastcall)) int foo(float a, int *b, char c);
+
+    // s passed in a0
+    // rest on stack
+    __attribute__((fastcall)) void printf(char *s, ...);
+
diff -rupN gcc-4.6.4.test/gcc/config/m68k/lb1sf68-fast.asm gcc-4.6.4-fastcall/gcc/config/m68k/lb1sf68-fast.asm
--- gcc-4.6.4.test/gcc/config/m68k/lb1sf68-fast.asm	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.6.4-fastcall/gcc/config/m68k/lb1sf68-fast.asm	2017-04-30 19:10:52.554947001 +0200
@@ -0,0 +1,3834 @@
+/* libgcc routines for 68000 w/o floating-point hardware.
+   Copyright (C) 1994, 1996, 1997, 1998, 2008, 2009 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 3, or (at your option) any
+later version.
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+Under Section 7 of GPL version 3, you are granted additional
+permissions described in the GCC Runtime Library Exception, version
+3.1, as published by the Free Software Foundation.
+
+You should have received a copy of the GNU General Public License and
+a copy of the GCC Runtime Library Exception along with this program;
+see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+<http://www.gnu.org/licenses/>.  */
+
+	.macro	DEBUG cnt
+	movel	d0, sp@-
+	movel	IMM (\cnt), d0
+	illegal
+	movel	sp@+, d0
+	.endm
+
+#ifdef L_floatex
+
+| This is an attempt at a decent floating point (single, double and 
+| extended double) code for the GNU C compiler. It should be easy to
+| adapt to other compilers (but beware of the local labels!).
+
+| Starting date: 21 October, 1990
+
+| It is convenient to introduce the notation (s,e,f) for a floating point
+| number, where s=sign, e=exponent, f=fraction. We will call a floating
+| point number fpn to abbreviate, independently of the precision.
+| Let MAX_EXP be in each case the maximum exponent (255 for floats, 1023 
+| for doubles and 16383 for long doubles). We then have the following 
+| different cases:
+|  1. Normalized fpns have 0 < e < MAX_EXP. They correspond to 
+|     (-1)^s x 1.f x 2^(e-bias-1).
+|  2. Denormalized fpns have e=0. They correspond to numbers of the form
+|     (-1)^s x 0.f x 2^(-bias).
+|  3. +/-INFINITY have e=MAX_EXP, f=0.
+|  4. Quiet NaN (Not a Number) have all bits set.
+|  5. Signaling NaN (Not a Number) have s=0, e=MAX_EXP, f=1.
+
+|=============================================================================
+|                                  exceptions
+|=============================================================================
+
+| This is the floating point condition code register (_fpCCR):
+|
+| struct {
+|   short _exception_bits;	
+|   short _trap_enable_bits;	
+|   short _sticky_bits;
+|   short _rounding_mode;
+|   short _format;
+|   short _last_operation;
+|   union {
+|     float sf;
+|     double df;
+|   } _operand1;
+|   union {
+|     float sf;
+|     double df;
+|   } _operand2;
+| } _fpCCR;
+
+	.data
+	.even
+
+	.globl	SYM (_fpCCR)
+	
+SYM (_fpCCR):
+__exception_bits:
+	.word	0
+__trap_enable_bits:
+	.word	0
+__sticky_bits:
+	.word	0
+__rounding_mode:
+	.word	ROUND_TO_NEAREST
+__format:
+	.word	NIL
+__last_operation:
+	.word	NOOP
+__operand1:
+	.long	0
+	.long	0
+__operand2:
+	.long 	0
+	.long	0
+
+| Offsets:
+EBITS  = __exception_bits - SYM (_fpCCR)
+TRAPE  = __trap_enable_bits - SYM (_fpCCR)
+STICK  = __sticky_bits - SYM (_fpCCR)
+ROUND  = __rounding_mode - SYM (_fpCCR)
+FORMT  = __format - SYM (_fpCCR)
+LASTO  = __last_operation - SYM (_fpCCR)
+OPER1  = __operand1 - SYM (_fpCCR)
+OPER2  = __operand2 - SYM (_fpCCR)
+
+| The following exception types are supported:
+INEXACT_RESULT 		= 0x0001
+UNDERFLOW 		= 0x0002
+OVERFLOW 		= 0x0004
+DIVIDE_BY_ZERO 		= 0x0008
+INVALID_OPERATION 	= 0x0010
+
+| The allowed rounding modes are:
+UNKNOWN           = -1
+ROUND_TO_NEAREST  = 0 | round result to nearest representable value
+ROUND_TO_ZERO     = 1 | round result towards zero
+ROUND_TO_PLUS     = 2 | round result towards plus infinity
+ROUND_TO_MINUS    = 3 | round result towards minus infinity
+
+| The allowed values of format are:
+NIL          = 0
+SINGLE_FLOAT = 1
+DOUBLE_FLOAT = 2
+LONG_FLOAT   = 3
+
+| The allowed values for the last operation are:
+NOOP         = 0
+ADD          = 1
+MULTIPLY     = 2
+DIVIDE       = 3
+NEGATE       = 4
+COMPARE      = 5
+EXTENDSFDF   = 6
+TRUNCDFSF    = 7
+
+|=============================================================================
+|                           __clear_sticky_bits
+|=============================================================================
+
+| The sticky bits are normally not cleared (thus the name), whereas the 
+| exception type and exception value reflect the last computation. 
+| This routine is provided to clear them (you can also write to _fpCCR,
+| since it is globally visible).
+
+	.globl  SYM (__clear_sticky_bit)
+
+	.text
+	.even
+
+| void __clear_sticky_bits(void);
+SYM (__clear_sticky_bit):		
+	PICLEA	SYM (_fpCCR),a0
+#ifndef __mcoldfire__
+	movew	IMM (0),a0@(STICK)
+#else
+	clr.w	a0@(STICK)
+#endif
+	rts
+
+|=============================================================================
+|                           $_exception_handler
+|=============================================================================
+
+	.globl  $_exception_handler
+
+	.text
+	.even
+
+| This is the common exit point if an exception occurs.
+| NOTE: it is NOT callable from C!
+| It expects the exception type in d7, the format (SINGLE_FLOAT,
+| DOUBLE_FLOAT or LONG_FLOAT) in d6, and the last operation code in d5.
+| It sets the corresponding exception and sticky bits, and the format. 
+| Depending on the format if fills the corresponding slots for the 
+| operands which produced the exception (all this information is provided
+| so if you write your own exception handlers you have enough information
+| to deal with the problem).
+| Then checks to see if the corresponding exception is trap-enabled, 
+| in which case it pushes the address of _fpCCR and traps through 
+| trap FPTRAP (15 for the moment).
+
+FPTRAP = 15
+
+$_exception_handler:
+|	DEBUG	1
+	PICLEA	SYM (_fpCCR),a0
+	movew	d7,a0@(EBITS)	| set __exception_bits
+#ifndef __mcoldfire__
+	orw	d7,a0@(STICK)	| and __sticky_bits
+#else
+	movew	a0@(STICK),d4
+	orl	d7,d4
+	movew	d4,a0@(STICK)
+#endif
+	movew	d6,a0@(FORMT)	| and __format
+	movew	d5,a0@(LASTO)	| and __last_operation
+
+| Now put the operands in place:
+#ifndef __mcoldfire__
+	cmpw	IMM (SINGLE_FLOAT),d6
+#else
+	cmpl	IMM (SINGLE_FLOAT),d6
+#endif
+	beq	1f
+	movel	a6@(8),a0@(OPER1)
+	movel	a6@(12),a0@(OPER1+4)
+	movel	a6@(16),a0@(OPER2)
+	movel	a6@(20),a0@(OPER2+4)
+	bra	2f
+1:	movel	a6@(8),a0@(OPER1)
+	movel	a6@(12),a0@(OPER2)
+2:
+| And check whether the exception is trap-enabled:
+#ifndef __mcoldfire__
+	andw	a0@(TRAPE),d7	| is exception trap-enabled?
+#else
+	clrl	d6
+	movew	a0@(TRAPE),d6
+	andl	d6,d7
+#endif
+	beq	1f		| no, exit
+	PICPEA	SYM (_fpCCR),a1	| yes, push address of _fpCCR
+	trap	IMM (FPTRAP)	| and trap
+#ifndef __mcoldfire__
+1:	moveml	sp@+,d3-d7	| restore data registers
+#else
+1:	moveml	sp@,d3-d7
+	lea	a6@(20),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| and return
+	rts
+#endif /* L_floatex */
+
+#ifdef  L_mulsi3
+	.text
+| int __mulsi3(int,int)
+	FUNC(__mulsi3)
+	.globl	SYM (__mulsi3)
+SYM (__mulsi3):
+|	DEBUG	2
+	movel   d0, a0          | d0a0 = x0:x1
+	movel   d1, a1		| d1a1 = y0:y1
+	swap	d0              | d0   = x1:x0 
+	muluw   d1, d0          | d0 = y1*x0 
+	swap	d1		| d1 = y1:y0
+	movel   a0, d2		| d2 = x0:x1
+	muluw   d2, d1          | d1 = x1*y0	
+#ifndef __mcoldfire__
+	addw	d1, d0
+#else
+	addl	d1, d0
+#endif
+	swap	d0
+	clrw	d0
+	movel	a1,d1		| d1 = y0:y1
+	muluw	d2, d1		| d1 = x1*y1
+
+	addl	d1, d0
+
+	rts
+#endif /* L_mulsi3 */
+
+#ifdef  L_udivsi3
+	.text
+| uint __udivsi3(uint, unit)
+	FUNC(__udivsi3)
+	.globl	SYM (__udivsi3)
+SYM (__udivsi3):
+|	DEBUG	3
+#ifndef __mcoldfire__
+	movel	d0, a0
+
+	cmpl	IMM (0x10000), d1 /* divisor >= 2 ^ 16 ?   */
+	jcc	L3		/* then try next algorithm */
+	movel	d0, d2
+	clrw	d2
+	swap	d2
+	divu	d1, d2          /* high quotient in lower word */
+	movew	d2, d0		/* save high quotient */
+	swap	d0
+	movew	a0, d2		/* get low dividend + high rest */
+	divu	d1, d2		/* low quotient */
+	movew	d2, d0
+	jra	L6
+
+L3:	movel	d1, d2		/* use d2 as divisor backup */
+L4:	lsrl	IMM (1), d1	/* shift divisor */
+	lsrl	IMM (1), d0	/* shift dividend */
+	cmpl	IMM (0x10000), d1 /* still divisor >= 2 ^ 16 ?  */
+	jcc	L4
+	divu	d1, d0		/* now we have 16-bit divisor */
+	andl	IMM (0xffff), d0 /* mask out divisor, ignore remainder */
+
+/* Multiply the 16-bit tentative quotient with the 32-bit divisor.  Because of
+   the operand ranges, this might give a 33-bit product.  If this product is
+   greater than the dividend, the tentative quotient was too large. */
+	movel	d2, d1
+	mulu	d0, d1		/* low part, 32 bits */
+	swap	d2
+	mulu	d0, d2		/* high part, at most 17 bits */
+	swap	d2		/* align high part with low part */
+	tstw	d2		/* high part 17 bits? */
+	jne	L5		/* if 17 bits, quotient was too large */
+	addl	d2, d1		/* add parts */
+	jcs	L5		/* if sum is 33 bits, quotient was too large */
+	cmpl	a0, d1		/* compare the sum with the dividend */
+	jls	L6		/* if sum > dividend, quotient was too large */
+L5:	subql	IMM (1), d0	/* adjust quotient */
+
+L6:	rts
+
+#else /* __mcoldfire__ */
+
+/* ColdFire implementation of non-restoring division algorithm from
+   Hennessy & Patterson, Appendix A. */
+	moveml	d3-d4,sp@
+	clrl	d2		| clear p
+	moveq	IMM (31),d4
+L1:	addl	d0,d0		| shift reg pair (p,a) one bit left
+	addxl	d2,d2
+	movl	d2,d3		| subtract b from p, store in tmp.
+	subl	d1,d3
+	jcs	L2		| if no carry,
+	bset	IMM (0),d0	| set the low order bit of a to 1,
+	movl	d3,d2		| and store tmp in p.
+L2:	subql	IMM (1),d4
+	jcc	L1
+	moveml	sp@,d3-d4	| restore data registers
+	rts
+#endif /* __mcoldfire__ */
+
+#endif /* L_udivsi3 */
+
+#ifdef  L_divsi3
+	.text
+| int __divsi3 (int, int)
+	FUNC(__divsi3)
+	.globl	SYM (__divsi3)
+SYM (__divsi3):
+|	DEBUG	4
+	moveq	IMM (1), d2	/* sign of result stored in d2 (=1 or =-1) */
+	tstl	d1
+	jpl	L1
+	negl	d1
+#ifndef __mcoldfire__
+	negw	d2		/* change sign because divisor <0  */
+#else
+	negl	d2		/* change sign because divisor <0  */
+#endif
+L1:	tstl	d0		/* d0 = dividend */
+	jpl	L2
+	negl	d0
+#ifndef __mcoldfire__
+	negw	d2
+#else
+	negl	d2
+#endif
+
+L2:	movew	d2,a1		/* Called function MUST NOT clobber a1 */
+	PICCALL	SYM (__udivsi3)	/* divide abs(dividend) by abs(divisor) */
+
+	movew	a1,d2
+	jpl	L3
+	negl	d0
+
+L3:	rts
+#endif /* L_divsi3 */
+
+#ifdef  L_umodsi3
+	.text
+| uint __umodsi3(uint, uint)
+	FUNC(__umodsi3)
+	.globl	SYM (__umodsi3)
+SYM (__umodsi3):
+|	DEBUG	5
+	movel	d0, d2
+	movel	d1, a1		/* a1 MUST NOT be clobbered by calls*/
+	PICCALL	SYM (__udivsi3)
+	movel	a1, d1		/* d1 = divisor */
+#ifndef __mcoldfire__
+	PICCALL	SYM (__mulsi3)	/* d0 = (a/b)*b */
+#else
+	mulsl	d1,d0
+#endif
+	movel	d2, d1		/* d1 = dividend */
+	subl	d0, d1		/* d1 = a - (a/b)*b */
+	movel	d1, d0
+	rts
+#endif /* L_umodsi3 */
+
+#ifdef  L_modsi3
+	.text
+| int __modsi3(int, int)
+	FUNC(__modsi3)
+	.globl	SYM (__modsi3)
+SYM (__modsi3):
+	movel	d0, sp@-
+	movel	d1, sp@-
+	PICCALL	SYM (__divsi3)
+	movel	sp@+, d1	/* d1 = divisor */
+#ifndef __mcoldfire__
+	PICCALL	SYM (__mulsi3)	/* d0 = (a/b)*b */
+#else
+	mulsl	d1,d0
+#endif
+	movel	sp@+, d1	/* d1 = dividend */
+	subl	d0, d1		/* d1 = a - (a/b)*b */
+	movel	d1, d0
+	rts
+#endif /* L_modsi3 */
+
+
+#ifdef  L_double
+
+	.globl	SYM (_fpCCR)
+	.globl  $_exception_handler
+
+QUIET_NaN      = 0xffffffff
+
+D_MAX_EXP      = 0x07ff
+D_BIAS         = 1022
+DBL_MAX_EXP    = D_MAX_EXP - D_BIAS
+DBL_MIN_EXP    = 1 - D_BIAS
+DBL_MANT_DIG   = 53
+
+INEXACT_RESULT 		= 0x0001
+UNDERFLOW 		= 0x0002
+OVERFLOW 		= 0x0004
+DIVIDE_BY_ZERO 		= 0x0008
+INVALID_OPERATION 	= 0x0010
+
+DOUBLE_FLOAT = 2
+
+NOOP         = 0
+ADD          = 1
+MULTIPLY     = 2
+DIVIDE       = 3
+NEGATE       = 4
+COMPARE      = 5
+EXTENDSFDF   = 6
+TRUNCDFSF    = 7
+
+UNKNOWN           = -1
+ROUND_TO_NEAREST  = 0 | round result to nearest representable value
+ROUND_TO_ZERO     = 1 | round result towards zero
+ROUND_TO_PLUS     = 2 | round result towards plus infinity
+ROUND_TO_MINUS    = 3 | round result towards minus infinity
+
+| Entry points:
+
+	.globl SYM (__adddf3)
+	.globl SYM (__subdf3)
+	.globl SYM (__muldf3)
+	.globl SYM (__divdf3)
+	.globl SYM (__negdf2)
+	.globl SYM (__cmpdf2)
+	.globl SYM (__cmpdf2_internal)
+#ifdef __ELF__
+	.hidden SYM (__cmpdf2_internal)
+#endif
+
+	.text
+	.even
+
+| These are common routines to return and signal exceptions.	
+
+Ld$den:
+| Return and signal a denormalized number
+	orl	d7,d0
+	movew	IMM (INEXACT_RESULT+UNDERFLOW),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Ld$infty:
+Ld$overflow:
+| Return a properly signed INFINITY and set the exception flags 
+	movel	IMM (0x7ff00000),d0
+	movel	IMM (0),d1
+	orl	d7,d0
+	movew	IMM (INEXACT_RESULT+OVERFLOW),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Ld$underflow:
+| Return 0 and set the exception flags 
+	movel	IMM (0),d0
+	movel	d0,d1
+	movew	IMM (INEXACT_RESULT+UNDERFLOW),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Ld$inop:
+| Return a quiet NaN and set the exception flags
+	movel	IMM (QUIET_NaN),d0
+	movel	d0,d1
+	movew	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Ld$div$0:
+| Return a properly signed INFINITY and set the exception flags
+	movel	IMM (0x7ff00000),d0
+	movel	IMM (0),d1
+	orl	d7,d0
+	movew	IMM (INEXACT_RESULT+DIVIDE_BY_ZERO),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+|=============================================================================
+|=============================================================================
+|                         double precision routines
+|=============================================================================
+|=============================================================================
+
+| A double precision floating point number (double) has the format:
+|
+| struct _double {
+|  unsigned int sign      : 1;  /* sign bit */ 
+|  unsigned int exponent  : 11; /* exponent, shifted by 126 */
+|  unsigned int fraction  : 52; /* fraction */
+| } double;
+| 
+| Thus sizeof(double) = 8 (64 bits). 
+|
+| All the routines are callable from C programs, and return the result 
+| in the register pair d0-d1. They also preserve all registers except 
+| d0-d2 and a0-a1.
+
+|=============================================================================
+|                              __subdf3
+|=============================================================================
+
+| double __subdf3(double, double);
+	FUNC(__subdf3)
+SYM (__subdf3):
+	bchg	IMM (31),sp@(4) | change sign of second operand
+				| and fall through, so we always add
+|=============================================================================
+|                              __adddf3
+|=============================================================================
+
+| double __adddf3(double, double);
+	FUNC(__adddf3)
+SYM (__adddf3):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)	| everything will be done in registers
+	moveml	d3-d7/a2-a5,sp@-	| save all data registers and a3-a5 (but d0-d2)
+#else
+	link	a6,IMM (-36)
+	moveml	d3-d7/a2-a5,sp@
+#endif
+	movel	d0,a2
+	movel	d1,a3
+	movel	a6@(8),d2
+	movel	d2,a4
+	movel	a6@(12),d3
+	movel	d3,a5
+
+	movel	d0,d7		| get d0's sign bit in d7 '
+	addl	d1,d1		| check and clear sign bit of a, and gain one
+	addxl	d0,d0		| bit of extra precision
+	beq	Ladddf$b	| if zero return second operand
+
+	movel	d2,d6		| save sign in d6 
+	addl	d3,d3		| get rid of sign bit and gain one bit of
+	addxl	d2,d2		| extra precision
+	beq	Ladddf$a	| if zero return first operand
+
+	andl	IMM (0x80000000),d7 | isolate a's sign bit '
+        swap	d6		| and also b's sign bit '
+#ifndef __mcoldfire__
+	andw	IMM (0x8000),d6	|
+	orw	d6,d7		| and combine them into d7, so that a's sign '
+				| bit is in the high word and b's is in the '
+				| low word, so d6 is free to be used
+#else
+	andl	IMM (0x8000),d6
+	orl	d6,d7
+#endif
+	movel	d7,a0		| now save d7 into a0, so d7 is free to
+                		| be used also
+
+| Get the exponents and check for denormalized and/or infinity.
+
+	movel	IMM (0x001fffff),d6 | mask for the fraction
+	movel	IMM (0x00200000),d7 | mask to put hidden bit back
+
+	movel	d0,d4		| 
+	andl	d6,d0		| get fraction in d0
+	notl	d6		| make d6 into mask for the exponent
+	andl	d6,d4		| get exponent in d4
+	beq	Ladddf$a$den	| branch if a is denormalized
+	cmpl	d6,d4		| check for INFINITY or NaN
+	beq	Ladddf$nf       | 
+	orl	d7,d0		| and put hidden bit back
+Ladddf$1:
+	swap	d4		| shift right exponent so that it starts
+#ifndef __mcoldfire__
+	lsrw	IMM (5),d4	| in bit 0 and not bit 20
+#else
+	lsrl	IMM (5),d4	| in bit 0 and not bit 20
+#endif
+| Now we have a's exponent in d4 and fraction in d0-d1 '
+	movel	d2,d5		| save b to get exponent
+	andl	d6,d5		| get exponent in d5
+	beq	Ladddf$b$den	| branch if b is denormalized
+	cmpl	d6,d5		| check for INFINITY or NaN
+	beq	Ladddf$nf
+	notl	d6		| make d6 into mask for the fraction again
+	andl	d6,d2		| and get fraction in d2
+	orl	d7,d2		| and put hidden bit back
+Ladddf$2:
+	swap	d5		| shift right exponent so that it starts
+#ifndef __mcoldfire__
+	lsrw	IMM (5),d5	| in bit 0 and not bit 20
+#else
+	lsrl	IMM (5),d5	| in bit 0 and not bit 20
+#endif
+
+| Now we have b's exponent in d5 and fraction in d2-d3. '
+
+| The situation now is as follows: the signs are combined in a0, the 
+| numbers are in d0-d1 (a) and d2-d3 (b), and the exponents in d4 (a)
+| and d5 (b). To do the rounding correctly we need to keep all the
+| bits until the end, so we need to use d0-d1-d2-d3 for the first number
+| and d4-d5-d6-d7 for the second. To do this we store (temporarily) the
+| exponents in a2-a3.
+
+#ifndef __mcoldfire__
+	movel	a3,sp@-		| save the address register
+#else
+	movel	a3,sp@-	
+	movel	a4,sp@-	
+#endif
+
+	movel	d4,a2		| save the exponents
+	movel	d5,a3		| 
+
+	movel	IMM (0),d7	| and move the numbers around
+	movel	d7,d6		|
+	movel	d3,d5		|
+	movel	d2,d4		|
+	movel	d7,d3		|
+	movel	d7,d2		|
+
+| Here we shift the numbers until the exponents are the same, and put 
+| the largest exponent in a2.
+#ifndef __mcoldfire__
+	exg	d4,a2		| get exponents back
+	exg	d5,a3		|
+	cmpw	d4,d5		| compare the exponents
+#else
+	movel	d4,a4		| get exponents back
+	movel	a2,d4
+	movel	a4,a2
+	movel	d5,a4
+	movel	a3,d5
+	movel	a4,a3
+	cmpl	d4,d5		| compare the exponents
+#endif
+	beq	Ladddf$3	| if equal don't shift '
+	bhi	9f		| branch if second exponent is higher
+
+| Here we have a's exponent larger than b's, so we have to shift b. We do 
+| this by using as counter d2:
+1:	movew	d4,d2		| move largest exponent to d2
+#ifndef __mcoldfire__
+	subw	d5,d2		| and subtract second exponent
+	exg	d4,a2		| get back the longs we saved
+	exg	d5,a3		|
+#else
+	subl	d5,d2		| and subtract second exponent
+	movel	d4,a4		| get back the longs we saved
+	movel	a2,d4
+	movel	a4,a2
+	movel	d5,a4
+	movel	a3,d5
+	movel	a4,a3
+#endif
+| if difference is too large we don't shift (actually, we can just exit) '
+#ifndef __mcoldfire__
+	cmpw	IMM (DBL_MANT_DIG+2),d2
+#else
+	cmpl	IMM (DBL_MANT_DIG+2),d2
+#endif
+	bge	Ladddf$b$small
+#ifndef __mcoldfire__
+	cmpw	IMM (32),d2	| if difference >= 32, shift by longs
+#else
+	cmpl	IMM (32),d2	| if difference >= 32, shift by longs
+#endif
+	bge	5f
+2:
+#ifndef __mcoldfire__
+	cmpw	IMM (16),d2	| if difference >= 16, shift by words	
+#else
+	cmpl	IMM (16),d2	| if difference >= 16, shift by words	
+#endif
+	bge	6f
+	bra	3f		| enter dbra loop
+
+4:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d4
+	roxrl	IMM (1),d5
+	roxrl	IMM (1),d6
+	roxrl	IMM (1),d7
+#else
+	lsrl	IMM (1),d7
+	btst	IMM (0),d6
+	beq	10f
+	bset	IMM (31),d7
+10:	lsrl	IMM (1),d6
+	btst	IMM (0),d5
+	beq	11f
+	bset	IMM (31),d6
+11:	lsrl	IMM (1),d5
+	btst	IMM (0),d4
+	beq	12f
+	bset	IMM (31),d5
+12:	lsrl	IMM (1),d4
+#endif
+3:
+#ifndef __mcoldfire__
+	dbra	d2,4b
+#else
+	subql	IMM (1),d2
+	bpl	4b	
+#endif
+	movel	IMM (0),d2
+	movel	d2,d3	
+	bra	Ladddf$4
+5:
+	movel	d6,d7
+	movel	d5,d6
+	movel	d4,d5
+	movel	IMM (0),d4
+#ifndef __mcoldfire__
+	subw	IMM (32),d2
+#else
+	subl	IMM (32),d2
+#endif
+	bra	2b
+6:
+	movew	d6,d7
+	swap	d7
+	movew	d5,d6
+	swap	d6
+	movew	d4,d5
+	swap	d5
+	movew	IMM (0),d4
+	swap	d4
+#ifndef __mcoldfire__
+	subw	IMM (16),d2
+#else
+	subl	IMM (16),d2
+#endif
+	bra	3b
+	
+9:
+#ifndef __mcoldfire__
+	exg	d4,d5
+	movew	d4,d6
+	subw	d5,d6		| keep d5 (largest exponent) in d4
+	exg	d4,a2
+	exg	d5,a3
+#else
+	movel	d5,d6
+	movel	d4,d5
+	movel	d6,d4
+	subl	d5,d6
+	movel	d4,a4
+	movel	a2,d4
+	movel	a4,a2
+	movel	d5,a4
+	movel	a3,d5
+	movel	a4,a3
+#endif
+| if difference is too large we don't shift (actually, we can just exit) '
+#ifndef __mcoldfire__
+	cmpw	IMM (DBL_MANT_DIG+2),d6
+#else
+	cmpl	IMM (DBL_MANT_DIG+2),d6
+#endif
+	bge	Ladddf$a$small
+#ifndef __mcoldfire__
+	cmpw	IMM (32),d6	| if difference >= 32, shift by longs
+#else
+	cmpl	IMM (32),d6	| if difference >= 32, shift by longs
+#endif
+	bge	5f
+2:
+#ifndef __mcoldfire__
+	cmpw	IMM (16),d6	| if difference >= 16, shift by words	
+#else
+	cmpl	IMM (16),d6	| if difference >= 16, shift by words	
+#endif
+	bge	6f
+	bra	3f		| enter dbra loop
+
+4:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+#endif
+3:
+#ifndef __mcoldfire__
+	dbra	d6,4b
+#else
+	subql	IMM (1),d6
+	bpl	4b
+#endif
+	movel	IMM (0),d7
+	movel	d7,d6
+	bra	Ladddf$4
+5:
+	movel	d2,d3
+	movel	d1,d2
+	movel	d0,d1
+	movel	IMM (0),d0
+#ifndef __mcoldfire__
+	subw	IMM (32),d6
+#else
+	subl	IMM (32),d6
+#endif
+	bra	2b
+6:
+	movew	d2,d3
+	swap	d3
+	movew	d1,d2
+	swap	d2
+	movew	d0,d1
+	swap	d1
+	movew	IMM (0),d0
+	swap	d0
+#ifndef __mcoldfire__
+	subw	IMM (16),d6
+#else
+	subl	IMM (16),d6
+#endif
+	bra	3b
+Ladddf$3:
+#ifndef __mcoldfire__
+	exg	d4,a2	
+	exg	d5,a3
+#else
+	movel	d4,a4
+	movel	a2,d4
+	movel	a4,a2
+	movel	d5,a4
+	movel	a3,d5
+	movel	a4,a3
+#endif
+Ladddf$4:	
+| Now we have the numbers in d0--d3 and d4--d7, the exponent in a2, and
+| the signs in a4.
+
+| Here we have to decide whether to add or subtract the numbers:
+#ifndef __mcoldfire__
+	exg	d7,a0		| get the signs 
+	exg	d6,a3		| a3 is free to be used
+#else
+	movel	d7,a4
+	movel	a0,d7
+	movel	a4,a0
+	movel	d6,a4
+	movel	a3,d6
+	movel	a4,a3
+#endif
+	movel	d7,d6		|
+	movew	IMM (0),d7	| get a's sign in d7 '
+	swap	d6              |
+	movew	IMM (0),d6	| and b's sign in d6 '
+	eorl	d7,d6		| compare the signs
+	bmi	Lsubdf$0	| if the signs are different we have 
+				| to subtract
+#ifndef __mcoldfire__
+	exg	d7,a0		| else we add the numbers
+	exg	d6,a3		|
+#else
+	movel	d7,a4
+	movel	a0,d7
+	movel	a4,a0
+	movel	d6,a4
+	movel	a3,d6
+	movel	a4,a3
+#endif
+	addl	d7,d3		|
+	addxl	d6,d2		|
+	addxl	d5,d1		| 
+	addxl	d4,d0           |
+
+	movel	a2,d4		| return exponent to d4
+	movel	a0,d7		| 
+	andl	IMM (0x80000000),d7 | d7 now has the sign
+
+#ifndef __mcoldfire__
+	movel	sp@+,a3	
+#else
+	movel	sp@+,a4	
+	movel	sp@+,a3	
+#endif
+
+| Before rounding normalize so bit #DBL_MANT_DIG is set (we will consider
+| the case of denormalized numbers in the rounding routine itself).
+| As in the addition (not in the subtraction!) we could have set 
+| one more bit we check this:
+	btst	IMM (DBL_MANT_DIG+1),d0	
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+1:
+	lea	pc@(Ladddf$5),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Ladddf$5:
+| Put back the exponent and check for overflow
+#ifndef __mcoldfire__
+	cmpw	IMM (0x7ff),d4	| is the exponent big?
+#else
+	cmpl	IMM (0x7ff),d4	| is the exponent big?
+#endif
+	bge	1f
+	bclr	IMM (DBL_MANT_DIG-1),d0
+#ifndef __mcoldfire__
+	lslw	IMM (4),d4	| put exponent back into position
+#else
+	lsll	IMM (4),d4	| put exponent back into position
+#endif
+	swap	d0		| 
+#ifndef __mcoldfire__
+	orw	d4,d0		|
+#else
+	orl	d4,d0		|
+#endif
+	swap	d0		|
+	bra	Ladddf$ret
+1:
+	moveq	IMM (ADD),d5
+	bra	Ld$overflow
+
+Lsubdf$0:
+| Here we do the subtraction.
+#ifndef __mcoldfire__
+	exg	d7,a0		| put sign back in a0
+	exg	d6,a3		|
+#else
+	movel	d7,a4
+	movel	a0,d7
+	movel	a4,a0
+	movel	d6,a4
+	movel	a3,d6
+	movel	a4,a3
+#endif
+	subl	d7,d3		|
+	subxl	d6,d2		|
+	subxl	d5,d1		|
+	subxl	d4,d0		|
+	beq	Ladddf$ret$1	| if zero just exit
+	bpl	1f		| if positive skip the following
+	movel	a0,d7		|
+	bchg	IMM (31),d7	| change sign bit in d7
+	movel	d7,a0		|
+	negl	d3		|
+	negxl	d2		|
+	negxl	d1              | and negate result
+	negxl	d0              |
+1:	
+	movel	a2,d4		| return exponent to d4
+	movel	a0,d7
+	andl	IMM (0x80000000),d7 | isolate sign bit
+#ifndef __mcoldfire__
+	movel	sp@+,a3  	|
+#else
+	movel	sp@+,a4
+	movel	sp@+,a3
+#endif
+
+| Before rounding normalize so bit #DBL_MANT_DIG is set (we will consider
+| the case of denormalized numbers in the rounding routine itself).
+| As in the addition (not in the subtraction!) we could have set 
+| one more bit we check this:
+	btst	IMM (DBL_MANT_DIG+1),d0	
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+1:
+	lea	pc@(Lsubdf$1),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Lsubdf$1:
+| Put back the exponent and sign (we don't have overflow). '
+	bclr	IMM (DBL_MANT_DIG-1),d0	
+#ifndef __mcoldfire__
+	lslw	IMM (4),d4	| put exponent back into position
+#else
+	lsll	IMM (4),d4	| put exponent back into position
+#endif
+	swap	d0		| 
+#ifndef __mcoldfire__
+	orw	d4,d0		|
+#else
+	orl	d4,d0		|
+#endif
+	swap	d0		|
+	bra	Ladddf$ret
+
+| If one of the numbers was too small (difference of exponents >= 
+| DBL_MANT_DIG+1) we return the other (and now we don't have to '
+| check for finiteness or zero).
+Ladddf$a$small:
+#ifndef __mcoldfire__
+	movel	sp@+,a3	
+#else
+	movel	sp@+,a4
+	movel	sp@+,a3
+#endif
+	movel	a4,d0
+	movel	a5,d1
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7/a2-a5	| restore data registers
+#else
+	moveml	sp@,d3-d7/a2-a5
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| and return
+	rts
+
+Ladddf$b$small:
+#ifndef __mcoldfire__
+	movel	sp@+,a3	
+#else
+	movel	sp@+,a4	
+	movel	sp@+,a3	
+#endif
+	movel	a2,d0
+	movel	a3,d1
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7/a2-a5	| restore data registers
+#else
+	moveml	sp@,d3-d7/a2-a5
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| and return
+	rts
+
+Ladddf$a$den:
+	movel	d7,d4		| d7 contains 0x00200000
+	bra	Ladddf$1
+
+Ladddf$b$den:
+	movel	d7,d5           | d7 contains 0x00200000
+	notl	d6
+	bra	Ladddf$2
+
+Ladddf$b:
+| Return b (if a is zero)
+	movel	d2,d0
+	movel	d3,d1
+	bne	1f			| Check if b is -0
+	cmpl	IMM (0x80000000),d0
+	bne	1f
+	andl	IMM (0x80000000),d7	| Use the sign of a
+	clrl	d0
+	bra	Ladddf$ret
+Ladddf$a:
+	movel	a2,d0
+	movel	a3,d1
+1:
+	moveq	IMM (ADD),d5
+| Check for NaN and +/-INFINITY.
+	movel	d0,d7         		|
+	andl	IMM (0x80000000),d7	|
+	bclr	IMM (31),d0		|
+	cmpl	IMM (0x7ff00000),d0	|
+	bge	2f			|
+	movel	d0,d0           	| check for zero, since we don't  '
+	bne	Ladddf$ret		| want to return -0 by mistake
+	bclr	IMM (31),d7		|
+	bra	Ladddf$ret		|
+2:
+	andl	IMM (0x000fffff),d0	| check for NaN (nonzero fraction)
+	orl	d1,d0			|
+	bne	Ld$inop         	|
+	bra	Ld$infty		|
+	
+Ladddf$ret$1:
+#ifndef __mcoldfire__
+	movel	sp@+,a3	| restore regs and exit
+#else
+	movel	sp@+,a4
+	movel	sp@+,a3
+#endif
+
+Ladddf$ret:
+| Normal exit.
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+	orl	d7,d0		| put sign bit back
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7/a2-a5
+#else
+	moveml	sp@,d3-d7/a2-a5
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+Ladddf$ret$den:
+| Return a denormalized number.
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0	| shift right once more
+	roxrl	IMM (1),d1	|
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+#endif
+	bra	Ladddf$ret
+
+Ladddf$nf:
+	moveq	IMM (ADD),d5
+| This could be faster but it is not worth the effort, since it is not
+| executed very often. We sacrifice speed for clarity here.
+	movel	a2,d0	| get the numbers back (remember that we
+	movel	a3,d1	| did some processing already)
+	movel	a4,d2	| 
+	movel	a5,d3	| 
+	movel	IMM (0x7ff00000),d4 | useful constant (INFINITY)
+	movel	d0,d7		| save sign bits
+	movel	d2,d6		| 
+	bclr	IMM (31),d0	| clear sign bits
+	bclr	IMM (31),d2	| 
+| We know that one of them is either NaN of +/-INFINITY
+| Check for NaN (if either one is NaN return NaN)
+	cmpl	d4,d0		| check first a (d0)
+	bhi	Ld$inop		| if d0 > 0x7ff00000 or equal and
+	bne	2f
+	tstl	d1		| d1 > 0, a is NaN
+	bne	Ld$inop		| 
+2:	cmpl	d4,d2		| check now b (d1)
+	bhi	Ld$inop		| 
+	bne	3f
+	tstl	d3		| 
+	bne	Ld$inop		| 
+3:
+| Now comes the check for +/-INFINITY. We know that both are (maybe not
+| finite) numbers, but we have to check if both are infinite whether we
+| are adding or subtracting them.
+	eorl	d7,d6		| to check sign bits
+	bmi	1f
+	andl	IMM (0x80000000),d7 | get (common) sign bit
+	bra	Ld$infty
+1:
+| We know one (or both) are infinite, so we test for equality between the
+| two numbers (if they are equal they have to be infinite both, so we
+| return NaN).
+	cmpl	d2,d0		| are both infinite?
+	bne	1f		| if d0 <> d2 they are not equal
+	cmpl	d3,d1		| if d0 == d2 test d3 and d1
+	beq	Ld$inop		| if equal return NaN
+1:	
+	andl	IMM (0x80000000),d7 | get a's sign bit '
+	cmpl	d4,d0		| test now for infinity
+	beq	Ld$infty	| if a is INFINITY return with this sign
+	bchg	IMM (31),d7	| else we know b is INFINITY and has
+	bra	Ld$infty	| the opposite sign
+
+|=============================================================================
+|                              __muldf3
+|=============================================================================
+
+| double __muldf3(double, double);
+	FUNC(__muldf3)
+SYM (__muldf3):
+|	DEBUG	8
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d3-d7,sp@-
+#else
+	link	a6,IMM (-20)
+	moveml	d3-d7,sp@
+#endif
+	movel	a6@(8),d0		| get a into d0-d1
+	movel	a6@(12),d1		| 
+	movel	a6@(16),d2		| and b into d2-d3
+	movel	a6@(20),d3		|
+	movel	d0,d7			| d7 will hold the sign of the product
+	eorl	d2,d7			|
+	andl	IMM (0x80000000),d7	|
+	movel	d7,a0			| save sign bit into a0 
+	movel	IMM (0x7ff00000),d7	| useful constant (+INFINITY)
+	movel	d7,d6			| another (mask for fraction)
+	notl	d6			|
+	bclr	IMM (31),d0		| get rid of a's sign bit '
+	movel	d0,d4			| 
+	orl	d1,d4			| 
+	beq	Lmuldf$a$0		| branch if a is zero
+	movel	d0,d4			|
+	bclr	IMM (31),d2		| get rid of b's sign bit '
+	movel	d2,d5			|
+	orl	d3,d5			| 
+	beq	Lmuldf$b$0		| branch if b is zero
+	movel	d2,d5			| 
+	cmpl	d7,d0			| is a big?
+	bhi	Lmuldf$inop		| if a is NaN return NaN
+	beq	Lmuldf$a$nf		| we still have to check d1 and b ...
+	cmpl	d7,d2			| now compare b with INFINITY
+	bhi	Lmuldf$inop		| is b NaN?
+	beq	Lmuldf$b$nf 		| we still have to check d3 ...
+| Here we have both numbers finite and nonzero (and with no sign bit).
+| Now we get the exponents into d4 and d5.
+	andl	d7,d4			| isolate exponent in d4
+	beq	Lmuldf$a$den		| if exponent zero, have denormalized
+	andl	d6,d0			| isolate fraction
+	orl	IMM (0x00100000),d0	| and put hidden bit back
+	swap	d4			| I like exponents in the first byte
+#ifndef __mcoldfire__
+	lsrw	IMM (4),d4		| 
+#else
+	lsrl	IMM (4),d4		| 
+#endif
+Lmuldf$1:			
+	andl	d7,d5			|
+	beq	Lmuldf$b$den		|
+	andl	d6,d2			|
+	orl	IMM (0x00100000),d2	| and put hidden bit back
+	swap	d5			|
+#ifndef __mcoldfire__
+	lsrw	IMM (4),d5		|
+#else
+	lsrl	IMM (4),d5		|
+#endif
+Lmuldf$2:				|
+#ifndef __mcoldfire__
+	addw	d5,d4			| add exponents
+	subw	IMM (D_BIAS+1),d4	| and subtract bias (plus one)
+#else
+	addl	d5,d4			| add exponents
+	subl	IMM (D_BIAS+1),d4	| and subtract bias (plus one)
+#endif
+
+| We are now ready to do the multiplication. The situation is as follows:
+| both a and b have bit 52 ( bit 20 of d0 and d2) set (even if they were 
+| denormalized to start with!), which means that in the product bit 104 
+| (which will correspond to bit 8 of the fourth long) is set.
+
+| Here we have to do the product.
+| To do it we have to juggle the registers back and forth, as there are not
+| enough to keep everything in them. So we use the address registers to keep
+| some intermediate data.
+
+	movel	a3,sp@-		| save a3 for temporary use
+	movel	IMM (0),a2	| a2 is a null register
+	movel	d4,a3		| and a3 will preserve the exponent
+
+| First, shift d2-d3 so bit 20 becomes bit 31:
+#ifndef __mcoldfire__
+	rorl	IMM (5),d2	| rotate d2 5 places right
+	swap	d2		| and swap it
+	rorl	IMM (5),d3	| do the same thing with d3
+	swap	d3		|
+	movew	d3,d6		| get the rightmost 11 bits of d3
+	andw	IMM (0x07ff),d6	|
+	orw	d6,d2		| and put them into d2
+	andw	IMM (0xf800),d3	| clear those bits in d3
+#else
+	moveq	IMM (11),d7	| left shift d2 11 bits
+	lsll	d7,d2
+	movel	d3,d6		| get a copy of d3
+	lsll	d7,d3		| left shift d3 11 bits
+	andl	IMM (0xffe00000),d6 | get the top 11 bits of d3
+	moveq	IMM (21),d7	| right shift them 21 bits
+	lsrl	d7,d6
+	orl	d6,d2		| stick them at the end of d2
+#endif
+
+	movel	d2,d6		| move b into d6-d7
+	movel	d3,d7           | move a into d4-d5
+	movel	d0,d4           | and clear d0-d1-d2-d3 (to put result)
+	movel	d1,d5           |
+	movel	IMM (0),d3	|
+	movel	d3,d2           |
+	movel	d3,d1           |
+	movel	d3,d0	        |
+
+| We use a1 as counter:	
+	movel	IMM (DBL_MANT_DIG-1),a1		
+#ifndef __mcoldfire__
+	exg	d7,a1
+#else
+	movel	d7,a4
+	movel	a1,d7
+	movel	a4,a1
+#endif
+
+1:
+#ifndef __mcoldfire__
+	exg	d7,a1		| put counter back in a1
+#else
+	movel	d7,a4
+	movel	a1,d7
+	movel	a4,a1
+#endif
+	addl	d3,d3		| shift sum once left
+	addxl	d2,d2           |
+	addxl	d1,d1           |
+	addxl	d0,d0           |
+	addl	d7,d7		|
+	addxl	d6,d6		|
+	bcc	2f		| if bit clear skip the following
+#ifndef __mcoldfire__
+	exg	d7,a2		|
+#else
+	movel	d7,a4
+	movel	a2,d7
+	movel	a4,a2
+#endif
+	addl	d5,d3		| else add a to the sum
+	addxl	d4,d2		|
+	addxl	d7,d1		|
+	addxl	d7,d0		|
+#ifndef __mcoldfire__
+	exg	d7,a2		| 
+#else
+	movel	d7,a4
+	movel	a2,d7
+	movel	a4,a2
+#endif
+2:
+#ifndef __mcoldfire__
+	exg	d7,a1		| put counter in d7
+	dbf	d7,1b		| decrement and branch
+#else
+	movel	d7,a4
+	movel	a1,d7
+	movel	a4,a1
+	subql	IMM (1),d7
+	bpl	1b
+#endif
+
+	movel	a3,d4		| restore exponent
+#ifndef __mcoldfire__
+	movel	sp@+,a3
+#else
+	movel	sp@+,a4
+	movel	sp@+,a3
+#endif
+
+| Now we have the product in d0-d1-d2-d3, with bit 8 of d0 set. The 
+| first thing to do now is to normalize it so bit 8 becomes bit 
+| DBL_MANT_DIG-32 (to do the rounding); later we will shift right.
+	swap	d0
+	swap	d1
+	movew	d1,d0
+	swap	d2
+	movew	d2,d1
+	swap	d3
+	movew	d3,d2
+	movew	IMM (0),d3
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+#else
+	moveq	IMM (29),d6
+	lsrl	IMM (3),d3
+	movel	d2,d7
+	lsll	d6,d7
+	orl	d7,d3
+	lsrl	IMM (3),d2
+	movel	d1,d7
+	lsll	d6,d7
+	orl	d7,d2
+	lsrl	IMM (3),d1
+	movel	d0,d7
+	lsll	d6,d7
+	orl	d7,d1
+	lsrl	IMM (3),d0
+#endif
+	
+| Now round, check for over- and underflow, and exit.
+	movel	a0,d7		| get sign bit back into d7
+	moveq	IMM (MULTIPLY),d5
+
+	btst	IMM (DBL_MANT_DIG+1-32),d0
+	beq	Lround$exit
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+	bra	Lround$exit
+
+Lmuldf$inop:
+	moveq	IMM (MULTIPLY),d5
+	bra	Ld$inop
+
+Lmuldf$b$nf:
+	moveq	IMM (MULTIPLY),d5
+	movel	a0,d7		| get sign bit back into d7
+	tstl	d3		| we know d2 == 0x7ff00000, so check d3
+	bne	Ld$inop		| if d3 <> 0 b is NaN
+	bra	Ld$overflow	| else we have overflow (since a is finite)
+
+Lmuldf$a$nf:
+	moveq	IMM (MULTIPLY),d5
+	movel	a0,d7		| get sign bit back into d7
+	tstl	d1		| we know d0 == 0x7ff00000, so check d1
+	bne	Ld$inop		| if d1 <> 0 a is NaN
+	bra	Ld$overflow	| else signal overflow
+
+| If either number is zero return zero, unless the other is +/-INFINITY or
+| NaN, in which case we return NaN.
+Lmuldf$b$0:
+	moveq	IMM (MULTIPLY),d5
+#ifndef __mcoldfire__
+	exg	d2,d0		| put b (==0) into d0-d1
+	exg	d3,d1		| and a (with sign bit cleared) into d2-d3
+	movel	a0,d0		| set result sign
+#else
+	movel	d0,d2		| put a into d2-d3
+	movel	d1,d3
+	movel	a0,d0		| put result zero into d0-d1
+	movq	IMM(0),d1
+#endif
+	bra	1f
+Lmuldf$a$0:
+	movel	a0,d0		| set result sign
+	movel	a6@(16),d2	| put b into d2-d3 again
+	movel	a6@(20),d3	|
+	bclr	IMM (31),d2	| clear sign bit
+1:	cmpl	IMM (0x7ff00000),d2 | check for non-finiteness
+	bge	Ld$inop		| in case NaN or +/-INFINITY return NaN
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7
+#else
+	moveml	sp@,d3-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+| If a number is denormalized we put an exponent of 1 but do not put the 
+| hidden bit back into the fraction; instead we shift left until bit 21
+| (the hidden bit) is set, adjusting the exponent accordingly. We do this
+| to ensure that the product of the fractions is close to 1.
+Lmuldf$a$den:
+	movel	IMM (1),d4
+	andl	d6,d0
+1:	addl	d1,d1           | shift a left until bit 20 is set
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	subw	IMM (1),d4	| and adjust exponent
+#else
+	subl	IMM (1),d4	| and adjust exponent
+#endif
+	btst	IMM (20),d0	|
+	bne	Lmuldf$1        |
+	bra	1b
+
+Lmuldf$b$den:
+	movel	IMM (1),d5
+	andl	d6,d2
+1:	addl	d3,d3		| shift b left until bit 20 is set
+	addxl	d2,d2		|
+#ifndef __mcoldfire__
+	subw	IMM (1),d5	| and adjust exponent
+#else
+	subql	IMM (1),d5	| and adjust exponent
+#endif
+	btst	IMM (20),d2	|
+	bne	Lmuldf$2	|
+	bra	1b
+
+
+|=============================================================================
+|                              __divdf3
+|=============================================================================
+
+| double __divdf3(double, double);
+	FUNC(__divdf3)
+SYM (__divdf3):
+|	DEBUG	9
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d3-d7,sp@-
+#else
+	link	a6,IMM (-20)
+	moveml	d3-d7,sp@
+#endif
+	movel	a6@(8),d0	| get a into d0-d1
+	movel	a6@(12),d1	| 
+	movel	a6@(16),d2	| and b into d2-d3
+	movel	a6@(20),d3	|
+	movel	d0,d7		| d7 will hold the sign of the result
+	eorl	d2,d7		|
+	andl	IMM (0x80000000),d7
+	movel	d7,a0		| save sign into a0
+	movel	IMM (0x7ff00000),d7 | useful constant (+INFINITY)
+	movel	d7,d6		| another (mask for fraction)
+	notl	d6		|
+	bclr	IMM (31),d0	| get rid of a's sign bit '
+	movel	d0,d4		|
+	orl	d1,d4		|
+	beq	Ldivdf$a$0	| branch if a is zero
+	movel	d0,d4		|
+	bclr	IMM (31),d2	| get rid of b's sign bit '
+	movel	d2,d5		|
+	orl	d3,d5		|
+	beq	Ldivdf$b$0	| branch if b is zero
+	movel	d2,d5
+	cmpl	d7,d0		| is a big?
+	bhi	Ldivdf$inop	| if a is NaN return NaN
+	beq	Ldivdf$a$nf	| if d0 == 0x7ff00000 we check d1
+	cmpl	d7,d2		| now compare b with INFINITY 
+	bhi	Ldivdf$inop	| if b is NaN return NaN
+	beq	Ldivdf$b$nf	| if d2 == 0x7ff00000 we check d3
+| Here we have both numbers finite and nonzero (and with no sign bit).
+| Now we get the exponents into d4 and d5 and normalize the numbers to
+| ensure that the ratio of the fractions is around 1. We do this by
+| making sure that both numbers have bit #DBL_MANT_DIG-32-1 (hidden bit)
+| set, even if they were denormalized to start with.
+| Thus, the result will satisfy: 2 > result > 1/2.
+	andl	d7,d4		| and isolate exponent in d4
+	beq	Ldivdf$a$den	| if exponent is zero we have a denormalized
+	andl	d6,d0		| and isolate fraction
+	orl	IMM (0x00100000),d0 | and put hidden bit back
+	swap	d4		| I like exponents in the first byte
+#ifndef __mcoldfire__
+	lsrw	IMM (4),d4	| 
+#else
+	lsrl	IMM (4),d4	| 
+#endif
+Ldivdf$1:			| 
+	andl	d7,d5		|
+	beq	Ldivdf$b$den	|
+	andl	d6,d2		|
+	orl	IMM (0x00100000),d2
+	swap	d5		|
+#ifndef __mcoldfire__
+	lsrw	IMM (4),d5	|
+#else
+	lsrl	IMM (4),d5	|
+#endif
+Ldivdf$2:			|
+#ifndef __mcoldfire__
+	subw	d5,d4		| subtract exponents
+	addw	IMM (D_BIAS),d4	| and add bias
+#else
+	subl	d5,d4		| subtract exponents
+	addl	IMM (D_BIAS),d4	| and add bias
+#endif
+
+| We are now ready to do the division. We have prepared things in such a way
+| that the ratio of the fractions will be less than 2 but greater than 1/2.
+| At this point the registers in use are:
+| d0-d1	hold a (first operand, bit DBL_MANT_DIG-32=0, bit 
+| DBL_MANT_DIG-1-32=1)
+| d2-d3	hold b (second operand, bit DBL_MANT_DIG-32=1)
+| d4	holds the difference of the exponents, corrected by the bias
+| a0	holds the sign of the ratio
+
+| To do the rounding correctly we need to keep information about the
+| nonsignificant bits. One way to do this would be to do the division
+| using four registers; another is to use two registers (as originally
+| I did), but use a sticky bit to preserve information about the 
+| fractional part. Note that we can keep that info in a1, which is not
+| used.
+	movel	IMM (0),d6	| d6-d7 will hold the result
+	movel	d6,d7		| 
+	movel	IMM (0),a1	| and a1 will hold the sticky bit
+
+	movel	IMM (DBL_MANT_DIG-32+1),d5	
+	
+1:	cmpl	d0,d2		| is a < b?
+	bhi	3f		| if b > a skip the following
+	beq	4f		| if d0==d2 check d1 and d3
+2:	subl	d3,d1		| 
+	subxl	d2,d0		| a <-- a - b
+	bset	d5,d6		| set the corresponding bit in d6
+3:	addl	d1,d1		| shift a by 1
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d5,1b		| and branch back
+#else
+	subql	IMM (1), d5
+	bpl	1b
+#endif
+	bra	5f			
+4:	cmpl	d1,d3		| here d0==d2, so check d1 and d3
+	bhi	3b		| if d1 > d2 skip the subtraction
+	bra	2b		| else go do it
+5:
+| Here we have to start setting the bits in the second long.
+	movel	IMM (31),d5	| again d5 is counter
+
+1:	cmpl	d0,d2		| is a < b?
+	bhi	3f		| if b > a skip the following
+	beq	4f		| if d0==d2 check d1 and d3
+2:	subl	d3,d1		| 
+	subxl	d2,d0		| a <-- a - b
+	bset	d5,d7		| set the corresponding bit in d7
+3:	addl	d1,d1		| shift a by 1
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d5,1b		| and branch back
+#else
+	subql	IMM (1), d5
+	bpl	1b
+#endif
+	bra	5f			
+4:	cmpl	d1,d3		| here d0==d2, so check d1 and d3
+	bhi	3b		| if d1 > d2 skip the subtraction
+	bra	2b		| else go do it
+5:
+| Now go ahead checking until we hit a one, which we store in d2.
+	movel	IMM (DBL_MANT_DIG),d5
+1:	cmpl	d2,d0		| is a < b?
+	bhi	4f		| if b < a, exit
+	beq	3f		| if d0==d2 check d1 and d3
+2:	addl	d1,d1		| shift a by 1
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d5,1b		| and branch back
+#else
+	subql	IMM (1), d5
+	bpl	1b
+#endif
+	movel	IMM (0),d2	| here no sticky bit was found
+	movel	d2,d3
+	bra	5f			
+3:	cmpl	d1,d3		| here d0==d2, so check d1 and d3
+	bhi	2b		| if d1 > d2 go back
+4:
+| Here put the sticky bit in d2-d3 (in the position which actually corresponds
+| to it; if you don't do this the algorithm loses in some cases). '
+	movel	IMM (0),d2
+	movel	d2,d3
+#ifndef __mcoldfire__
+	subw	IMM (DBL_MANT_DIG),d5
+	addw	IMM (63),d5
+	cmpw	IMM (31),d5
+#else
+	subl	IMM (DBL_MANT_DIG),d5
+	addl	IMM (63),d5
+	cmpl	IMM (31),d5
+#endif
+	bhi	2f
+1:	bset	d5,d3
+	bra	5f
+#ifndef __mcoldfire__
+	subw	IMM (32),d5
+#else
+	subl	IMM (32),d5
+#endif
+2:	bset	d5,d2
+5:
+| Finally we are finished! Move the longs in the address registers to
+| their final destination:
+	movel	d6,d0
+	movel	d7,d1
+	movel	IMM (0),d3
+
+| Here we have finished the division, with the result in d0-d1-d2-d3, with
+| 2^21 <= d6 < 2^23. Thus bit 23 is not set, but bit 22 could be set.
+| If it is not, then definitely bit 21 is set. Normalize so bit 22 is
+| not set:
+	btst	IMM (DBL_MANT_DIG-32+1),d0
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+1:
+| Now round, check for over- and underflow, and exit.
+	movel	a0,d7		| restore sign bit to d7
+	moveq	IMM (DIVIDE),d5
+	bra	Lround$exit
+
+Ldivdf$inop:
+	moveq	IMM (DIVIDE),d5
+	bra	Ld$inop
+
+Ldivdf$a$0:
+| If a is zero check to see whether b is zero also. In that case return
+| NaN; then check if b is NaN, and return NaN also in that case. Else
+| return a properly signed zero.
+	moveq	IMM (DIVIDE),d5
+	bclr	IMM (31),d2	|
+	movel	d2,d4		| 
+	orl	d3,d4		| 
+	beq	Ld$inop		| if b is also zero return NaN
+	cmpl	IMM (0x7ff00000),d2 | check for NaN
+	bhi	Ld$inop		| 
+	blt	1f		|
+	tstl	d3		|
+	bne	Ld$inop		|
+1:	movel	a0,d0		| else return signed zero
+	moveq	IMM(0),d1	| 
+	PICLEA	SYM (_fpCCR),a0	| clear exception flags
+	movew	IMM (0),a0@	|
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7	| 
+#else
+	moveml	sp@,d3-d7	| 
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| 
+	rts			| 	
+
+Ldivdf$b$0:
+	moveq	IMM (DIVIDE),d5
+| If we got here a is not zero. Check if a is NaN; in that case return NaN,
+| else return +/-INFINITY. Remember that a is in d0 with the sign bit 
+| cleared already.
+	movel	a0,d7		| put a's sign bit back in d7 '
+	cmpl	IMM (0x7ff00000),d0 | compare d0 with INFINITY
+	bhi	Ld$inop		| if larger it is NaN
+	tstl	d1		| 
+	bne	Ld$inop		| 
+	bra	Ld$div$0	| else signal DIVIDE_BY_ZERO
+
+Ldivdf$b$nf:
+	moveq	IMM (DIVIDE),d5
+| If d2 == 0x7ff00000 we have to check d3.
+	tstl	d3		|
+	bne	Ld$inop		| if d3 <> 0, b is NaN
+	bra	Ld$underflow	| else b is +/-INFINITY, so signal underflow
+
+Ldivdf$a$nf:
+	moveq	IMM (DIVIDE),d5
+| If d0 == 0x7ff00000 we have to check d1.
+	tstl	d1		|
+	bne	Ld$inop		| if d1 <> 0, a is NaN
+| If a is INFINITY we have to check b
+	cmpl	d7,d2		| compare b with INFINITY 
+	bge	Ld$inop		| if b is NaN or INFINITY return NaN
+	tstl	d3		|
+	bne	Ld$inop		| 
+	bra	Ld$overflow	| else return overflow
+
+| If a number is denormalized we put an exponent of 1 but do not put the 
+| bit back into the fraction.
+Ldivdf$a$den:
+	movel	IMM (1),d4
+	andl	d6,d0
+1:	addl	d1,d1		| shift a left until bit 20 is set
+	addxl	d0,d0
+#ifndef __mcoldfire__
+	subw	IMM (1),d4	| and adjust exponent
+#else
+	subl	IMM (1),d4	| and adjust exponent
+#endif
+	btst	IMM (DBL_MANT_DIG-32-1),d0
+	bne	Ldivdf$1
+	bra	1b
+
+Ldivdf$b$den:
+	movel	IMM (1),d5
+	andl	d6,d2
+1:	addl	d3,d3		| shift b left until bit 20 is set
+	addxl	d2,d2
+#ifndef __mcoldfire__
+	subw	IMM (1),d5	| and adjust exponent
+#else
+	subql	IMM (1),d5	| and adjust exponent
+#endif
+	btst	IMM (DBL_MANT_DIG-32-1),d2
+	bne	Ldivdf$2
+	bra	1b
+
+Lround$exit:
+| This is a common exit point for __muldf3 and __divdf3. When they enter
+| this point the sign of the result is in d7, the result in d0-d1, normalized
+| so that 2^21 <= d0 < 2^22, and the exponent is in the lower byte of d4.
+
+| First check for underlow in the exponent:
+#ifndef __mcoldfire__
+	cmpw	IMM (-DBL_MANT_DIG-1),d4		
+#else
+	cmpl	IMM (-DBL_MANT_DIG-1),d4		
+#endif
+	blt	Ld$underflow	
+| It could happen that the exponent is less than 1, in which case the 
+| number is denormalized. In this case we shift right and adjust the 
+| exponent until it becomes 1 or the fraction is zero (in the latter case 
+| we signal underflow and return zero).
+	movel	d7,a0		|
+	movel	IMM (0),d6	| use d6-d7 to collect bits flushed right
+	movel	d6,d7		| use d6-d7 to collect bits flushed right
+#ifndef __mcoldfire__
+	cmpw	IMM (1),d4	| if the exponent is less than 1 we 
+#else
+	cmpl	IMM (1),d4	| if the exponent is less than 1 we 
+#endif
+	bge	2f		| have to shift right (denormalize)
+1:
+#ifndef __mcoldfire__
+	addw	IMM (1),d4	| adjust the exponent
+	lsrl	IMM (1),d0	| shift right once 
+	roxrl	IMM (1),d1	|
+	roxrl	IMM (1),d2	|
+	roxrl	IMM (1),d3	|
+	roxrl	IMM (1),d6	| 
+	roxrl	IMM (1),d7	|
+	cmpw	IMM (1),d4	| is the exponent 1 already?
+#else
+	addl	IMM (1),d4	| adjust the exponent
+	lsrl	IMM (1),d7
+	btst	IMM (0),d6
+	beq	13f
+	bset	IMM (31),d7
+13:	lsrl	IMM (1),d6
+	btst	IMM (0),d3
+	beq	14f
+	bset	IMM (31),d6
+14:	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+	cmpl	IMM (1),d4	| is the exponent 1 already?
+#endif
+	beq	2f		| if not loop back
+	bra	1b              |
+	bra	Ld$underflow	| safety check, shouldn't execute '
+2:	orl	d6,d2		| this is a trick so we don't lose  '
+	orl	d7,d3		| the bits which were flushed right
+	movel	a0,d7		| get back sign bit into d7
+| Now call the rounding routine (which takes care of denormalized numbers):
+	lea	pc@(Lround$0),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Lround$0:
+| Here we have a correctly rounded result (either normalized or denormalized).
+
+| Here we should have either a normalized number or a denormalized one, and
+| the exponent is necessarily larger or equal to 1 (so we don't have to  '
+| check again for underflow!). We have to check for overflow or for a 
+| denormalized number (which also signals underflow).
+| Check for overflow (i.e., exponent >= 0x7ff).
+#ifndef __mcoldfire__
+	cmpw	IMM (0x07ff),d4
+#else
+	cmpl	IMM (0x07ff),d4
+#endif
+	bge	Ld$overflow
+| Now check for a denormalized number (exponent==0):
+	movew	d4,d4
+	beq	Ld$den
+1:
+| Put back the exponents and sign and return.
+#ifndef __mcoldfire__
+	lslw	IMM (4),d4	| exponent back to fourth byte
+#else
+	lsll	IMM (4),d4	| exponent back to fourth byte
+#endif
+	bclr	IMM (DBL_MANT_DIG-32-1),d0
+	swap	d0		| and put back exponent
+#ifndef __mcoldfire__
+	orw	d4,d0		| 
+#else
+	orl	d4,d0		| 
+#endif
+	swap	d0		|
+	orl	d7,d0		| and sign also
+
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7
+#else
+	moveml	sp@,d3-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+|=============================================================================
+|                              __negdf2
+|=============================================================================
+
+| double __negdf2(double, double);
+	FUNC(__negdf2)
+SYM (__negdf2):
+|	DEBUG	10
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d3-d7,sp@-
+#else
+	link	a6,IMM (-20)
+	moveml	d3-d7,sp@
+#endif
+	moveq	IMM (NEGATE),d5
+	movel	a6@(8),d0	| get number to negate in d0-d1
+	movel	a6@(12),d1	|
+	bchg	IMM (31),d0	| negate
+	movel	d0,d2		| make a positive copy (for the tests)
+	bclr	IMM (31),d2	|
+	movel	d2,d4		| check for zero
+	orl	d1,d4		|
+	beq	2f		| if zero (either sign) return +zero
+	cmpl	IMM (0x7ff00000),d2 | compare to +INFINITY
+	blt	1f		| if finite, return
+	bhi	Ld$inop		| if larger (fraction not zero) is NaN
+	tstl	d1		| if d2 == 0x7ff00000 check d1
+	bne	Ld$inop		|
+	movel	d0,d7		| else get sign and return INFINITY
+	andl	IMM (0x80000000),d7
+	bra	Ld$infty		
+1:	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7
+#else
+	moveml	sp@,d3-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+2:	bclr	IMM (31),d0
+	bra	1b
+
+|=============================================================================
+|                              __cmpdf2
+|=============================================================================
+
+GREATER =  1
+LESS    = -1
+EQUAL   =  0
+
+| Last int arg is actually sent in a0!
+| int __cmpdf2_internal(double, double, int);
+SYM (__cmpdf2_internal):
+|	DEBUG	11
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d3-d7,sp@- 	| save registers
+#else
+	link	a6,IMM (-20)
+	moveml	d3-d7,sp@
+#endif
+	moveq	IMM (COMPARE),d5
+	movel	a6@(8),d0	| get first operand
+	movel	a6@(12),d1	|
+	movel	a6@(16),d2	| get second operand
+	movel	a6@(20),d3	|
+| First check if a and/or b are (+/-) zero and in that case clear
+| the sign bit.
+	movel	d0,d6		| copy signs into d6 (a) and d7(b)
+	bclr	IMM (31),d0	| and clear signs in d0 and d2
+	movel	d2,d7		|
+	bclr	IMM (31),d2	|
+	cmpl	IMM (0x7ff00000),d0 | check for a == NaN
+	bhi	Lcmpd$inop		| if d0 > 0x7ff00000, a is NaN
+	beq	Lcmpdf$a$nf	| if equal can be INFINITY, so check d1
+	movel	d0,d4		| copy into d4 to test for zero
+	orl	d1,d4		|
+	beq	Lcmpdf$a$0	|
+Lcmpdf$0:
+	cmpl	IMM (0x7ff00000),d2 | check for b == NaN
+	bhi	Lcmpd$inop		| if d2 > 0x7ff00000, b is NaN
+	beq	Lcmpdf$b$nf	| if equal can be INFINITY, so check d3
+	movel	d2,d4		|
+	orl	d3,d4		|
+	beq	Lcmpdf$b$0	|
+Lcmpdf$1:
+| Check the signs
+	eorl	d6,d7
+	bpl	1f
+| If the signs are not equal check if a >= 0
+	tstl	d6
+	bpl	Lcmpdf$a$gt$b	| if (a >= 0 && b < 0) => a > b
+	bmi	Lcmpdf$b$gt$a	| if (a < 0 && b >= 0) => a < b
+1:
+| If the signs are equal check for < 0
+	tstl	d6
+	bpl	1f
+| If both are negative exchange them
+#ifndef __mcoldfire__
+	exg	d0,d2
+	exg	d1,d3
+#else
+	movel	d0,d7
+	movel	d2,d0
+	movel	d7,d2
+	movel	d1,d7
+	movel	d3,d1
+	movel	d7,d3
+#endif
+1:
+| Now that they are positive we just compare them as longs (does this also
+| work for denormalized numbers?).
+	cmpl	d0,d2
+	bhi	Lcmpdf$b$gt$a	| |b| > |a|
+	bne	Lcmpdf$a$gt$b	| |b| < |a|
+| If we got here d0 == d2, so we compare d1 and d3.
+	cmpl	d1,d3
+	bhi	Lcmpdf$b$gt$a	| |b| > |a|
+	bne	Lcmpdf$a$gt$b	| |b| < |a|
+| If we got here a == b.
+	movel	IMM (EQUAL),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7 	| put back the registers
+#else
+	moveml	sp@,d3-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+Lcmpdf$a$gt$b:
+	movel	IMM (GREATER),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7 	| put back the registers
+#else
+	moveml	sp@,d3-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+Lcmpdf$b$gt$a:
+	movel	IMM (LESS),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7 	| put back the registers
+#else
+	moveml	sp@,d3-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+Lcmpdf$a$0:	
+	bclr	IMM (31),d6
+	bra	Lcmpdf$0
+Lcmpdf$b$0:
+	bclr	IMM (31),d7
+	bra	Lcmpdf$1
+
+Lcmpdf$a$nf:
+	tstl	d1
+	bne	Ld$inop
+	bra	Lcmpdf$0
+
+Lcmpdf$b$nf:
+	tstl	d3
+	bne	Ld$inop
+	bra	Lcmpdf$1
+
+Lcmpd$inop:
+| a0 MUST be untouches until now.
+	moveq	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+| int __cmpdf2(double, double);
+	FUNC(__cmpdf2)
+SYM (__cmpdf2):
+| 	DEBUG	12 
+	lea	1.w,a0
+	bra	SYM (__cmpdf2_internal)
+
+|=============================================================================
+|                           rounding routines
+|=============================================================================
+
+| The rounding routines expect the number to be normalized in registers
+| d0-d1-d2-d3, with the exponent in register d4. They assume that the 
+| exponent is larger or equal to 1. They return a properly normalized number
+| if possible, and a denormalized number otherwise. The exponent is returned
+| in d4.
+
+Lround$to$nearest:
+| We now normalize as suggested by D. Knuth ("Seminumerical Algorithms"):
+| Here we assume that the exponent is not too small (this should be checked
+| before entering the rounding routine), but the number could be denormalized.
+
+| Check for denormalized numbers:
+1:	btst	IMM (DBL_MANT_DIG-32),d0
+	bne	2f		| if set the number is normalized
+| Normalize shifting left until bit #DBL_MANT_DIG-32 is set or the exponent 
+| is one (remember that a denormalized number corresponds to an 
+| exponent of -D_BIAS+1).
+#ifndef __mcoldfire__
+	cmpw	IMM (1),d4	| remember that the exponent is at least one
+#else
+	cmpl	IMM (1),d4	| remember that the exponent is at least one
+#endif
+ 	beq	2f		| an exponent of one means denormalized
+	addl	d3,d3		| else shift and adjust the exponent
+	addxl	d2,d2		|
+	addxl	d1,d1		|
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d4,1b		|
+#else
+	subql	IMM (1), d4
+	bpl	1b
+#endif
+2:
+| Now round: we do it as follows: after the shifting we can write the
+| fraction part as f + delta, where 1 < f < 2^25, and 0 <= delta <= 2.
+| If delta < 1, do nothing. If delta > 1, add 1 to f. 
+| If delta == 1, we make sure the rounded number will be even (odd?) 
+| (after shifting).
+	btst	IMM (0),d1	| is delta < 1?
+	beq	2f		| if so, do not do anything
+	orl	d2,d3		| is delta == 1?
+	bne	1f		| if so round to even
+	movel	d1,d3		| 
+	andl	IMM (2),d3	| bit 1 is the last significant bit
+	movel	IMM (0),d2	|
+	addl	d3,d1		|
+	addxl	d2,d0		|
+	bra	2f		| 
+1:	movel	IMM (1),d3	| else add 1 
+	movel	IMM (0),d2	|
+	addl	d3,d1		|
+	addxl	d2,d0
+| Shift right once (because we used bit #DBL_MANT_DIG-32!).
+2:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1		
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+#endif
+
+| Now check again bit #DBL_MANT_DIG-32 (rounding could have produced a
+| 'fraction overflow' ...).
+	btst	IMM (DBL_MANT_DIG-32),d0	
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+1:
+| If bit #DBL_MANT_DIG-32-1 is clear we have a denormalized number, so we 
+| have to put the exponent to zero and return a denormalized number.
+	btst	IMM (DBL_MANT_DIG-32-1),d0
+	beq	1f
+	jmp	a0@
+1:	movel	IMM (0),d4
+	jmp	a0@
+
+Lround$to$zero:
+Lround$to$plus:
+Lround$to$minus:
+	jmp	a0@
+#endif /* L_double */
+
+#ifdef  L_float
+
+	.globl	SYM (_fpCCR)
+	.globl  $_exception_handler
+
+QUIET_NaN    = 0xffffffff
+SIGNL_NaN    = 0x7f800001
+INFINITY     = 0x7f800000
+
+F_MAX_EXP      = 0xff
+F_BIAS         = 126
+FLT_MAX_EXP    = F_MAX_EXP - F_BIAS
+FLT_MIN_EXP    = 1 - F_BIAS
+FLT_MANT_DIG   = 24
+
+INEXACT_RESULT 		= 0x0001
+UNDERFLOW 		= 0x0002
+OVERFLOW 		= 0x0004
+DIVIDE_BY_ZERO 		= 0x0008
+INVALID_OPERATION 	= 0x0010
+
+SINGLE_FLOAT = 1
+
+NOOP         = 0
+ADD          = 1
+MULTIPLY     = 2
+DIVIDE       = 3
+NEGATE       = 4
+COMPARE      = 5
+EXTENDSFDF   = 6
+TRUNCDFSF    = 7
+
+UNKNOWN           = -1
+ROUND_TO_NEAREST  = 0 | round result to nearest representable value
+ROUND_TO_ZERO     = 1 | round result towards zero
+ROUND_TO_PLUS     = 2 | round result towards plus infinity
+ROUND_TO_MINUS    = 3 | round result towards minus infinity
+
+| Entry points:
+
+	.globl SYM (__addsf3)
+	.globl SYM (__subsf3)
+	.globl SYM (__mulsf3)
+	.globl SYM (__divsf3)
+	.globl SYM (__negsf2)
+	.globl SYM (__cmpsf2)
+	.globl SYM (__cmpsf2_internal)
+#ifdef __ELF__
+	.hidden SYM (__cmpsf2_internal)
+#endif
+
+| These are common routines to return and signal exceptions.	
+
+	.text
+	.even
+
+Lf$den:
+| Return and signal a denormalized number
+	orl	d7,d0
+	moveq	IMM (INEXACT_RESULT+UNDERFLOW),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Lf$infty:
+Lf$overflow:
+| Return a properly signed INFINITY and set the exception flags 
+	movel	IMM (INFINITY),d0
+	orl	d7,d0
+	moveq	IMM (INEXACT_RESULT+OVERFLOW),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Lf$underflow:
+| Return 0 and set the exception flags 
+	moveq	IMM (0),d0
+	moveq	IMM (INEXACT_RESULT+UNDERFLOW),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Lf$inop:
+| Return a quiet NaN and set the exception flags
+	movel	IMM (QUIET_NaN),d0
+	moveq	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Lf$div$0:
+| Return a properly signed INFINITY and set the exception flags
+	movel	IMM (INFINITY),d0
+	orl	d7,d0
+	moveq	IMM (INEXACT_RESULT+DIVIDE_BY_ZERO),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+|=============================================================================
+|=============================================================================
+|                         single precision routines
+|=============================================================================
+|=============================================================================
+
+| A single precision floating point number (float) has the format:
+|
+| struct _float {
+|  unsigned int sign      : 1;  /* sign bit */ 
+|  unsigned int exponent  : 8;  /* exponent, shifted by 126 */
+|  unsigned int fraction  : 23; /* fraction */
+| } float;
+| 
+| Thus sizeof(float) = 4 (32 bits). 
+|
+| All the routines are callable from C programs, and return the result 
+| in the single register d0. They also preserve all registers except 
+| d0-d1 and a0-a1.
+
+|=============================================================================
+|                              __subsf3
+|=============================================================================
+
+| float __subsf3(float, float);
+	FUNC(__subsf3)
+SYM (__subsf3):
+| 	DEBUG	13
+	negl	d1       	| change sign of second operand
+				| and fall through
+|=============================================================================
+|                              __addsf3
+|=============================================================================
+
+| float __addsf3(float, float);
+	FUNC(__addsf3)
+SYM (__addsf3):
+|	DEBUG	14
+#ifndef __mcoldfire__
+	moveml	d3-d7/a2-a3,sp@-	| save all data registers but d0-d1
+#else
+	lea	a6@(-28),a6	
+	moveml	d3-d7/a2-a3,sp@
+#endif
+	movel	d0,a2		| store first operand
+	movel	d1,a3		| store second operand
+	movel	d0,a0		| get d0's sign bit '
+	addl	d0,d0		| check and clear sign bit of a
+	beq	Laddsf$b	| if zero return second operand
+	movel	d1,a1		| save b's sign bit '
+	addl	d1,d1		| get rid of sign bit
+	beq	Laddsf$a	| if zero return first operand
+
+| Get the exponents and check for denormalized and/or infinity.
+
+	movel	IMM (0x00ffffff),d4	| mask to get fraction
+	movel	IMM (0x01000000),d5	| mask to put hidden bit back
+
+	movel	d0,d6		| save a to get exponent
+	andl	d4,d0		| get fraction in d0
+	notl 	d4		| make d4 into a mask for the exponent
+	andl	d4,d6		| get exponent in d6
+	beq	Laddsf$a$den	| branch if a is denormalized
+	cmpl	d4,d6		| check for INFINITY or NaN
+	beq	Laddsf$nf
+	swap	d6		| put exponent into first word
+	orl	d5,d0		| and put hidden bit back
+Laddsf$1:
+| Now we have a's exponent in d6 (second byte) and the mantissa in d0. '
+	movel	d1,d7		| get exponent in d7
+	andl	d4,d7		| 
+	beq	Laddsf$b$den	| branch if b is denormalized
+	cmpl	d4,d7		| check for INFINITY or NaN
+	beq	Laddsf$nf
+	swap	d7		| put exponent into first word
+	notl 	d4		| make d4 into a mask for the fraction
+	andl	d4,d1		| get fraction in d1
+	orl	d5,d1		| and put hidden bit back
+Laddsf$2:
+| Now we have b's exponent in d7 (second byte) and the mantissa in d1. '
+
+| Note that the hidden bit corresponds to bit #FLT_MANT_DIG-1, and we 
+| shifted right once, so bit #FLT_MANT_DIG is set (so we have one extra
+| bit).
+
+	movel	d1,d2		| move b to d2, since we want to use
+				| two registers to do the sum
+	movel	IMM (0),d1	| and clear the new ones
+	movel	d1,d3		|
+
+| Here we shift the numbers in registers d0 and d1 so the exponents are the
+| same, and put the largest exponent in d6. Note that we are using two
+| registers for each number (see the discussion by D. Knuth in "Seminumerical 
+| Algorithms").
+#ifndef __mcoldfire__
+	cmpw	d6,d7		| compare exponents
+#else
+	cmpl	d6,d7		| compare exponents
+#endif
+	beq	Laddsf$3	| if equal don't shift '
+	bhi	5f		| branch if second exponent largest
+1:
+	subl	d6,d7		| keep the largest exponent
+	negl	d7
+#ifndef __mcoldfire__
+	lsrw	IMM (8),d7	| put difference in lower byte
+#else
+	lsrl	IMM (8),d7	| put difference in lower byte
+#endif
+| if difference is too large we don't shift (actually, we can just exit) '
+#ifndef __mcoldfire__
+	cmpw	IMM (FLT_MANT_DIG+2),d7		
+#else
+	cmpl	IMM (FLT_MANT_DIG+2),d7		
+#endif
+	bge	Laddsf$b$small
+#ifndef __mcoldfire__
+	cmpw	IMM (16),d7	| if difference >= 16 swap
+#else
+	cmpl	IMM (16),d7	| if difference >= 16 swap
+#endif
+	bge	4f
+2:
+#ifndef __mcoldfire__
+	subw	IMM (1),d7
+#else
+	subql	IMM (1), d7
+#endif
+3:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d2	| shift right second operand
+	roxrl	IMM (1),d3
+	dbra	d7,3b
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	subql	IMM (1), d7
+	bpl	3b
+#endif
+	bra	Laddsf$3
+4:
+	movew	d2,d3
+	swap	d3
+	movew	d3,d2
+	swap	d2
+#ifndef __mcoldfire__
+	subw	IMM (16),d7
+#else
+	subl	IMM (16),d7
+#endif
+	bne	2b		| if still more bits, go back to normal case
+	bra	Laddsf$3
+5:
+#ifndef __mcoldfire__
+	exg	d6,d7		| exchange the exponents
+#else
+	eorl	d6,d7
+	eorl	d7,d6
+	eorl	d6,d7
+#endif
+	subl	d6,d7		| keep the largest exponent
+	negl	d7		|
+#ifndef __mcoldfire__
+	lsrw	IMM (8),d7	| put difference in lower byte
+#else
+	lsrl	IMM (8),d7	| put difference in lower byte
+#endif
+| if difference is too large we don't shift (and exit!) '
+#ifndef __mcoldfire__
+	cmpw	IMM (FLT_MANT_DIG+2),d7		
+#else
+	cmpl	IMM (FLT_MANT_DIG+2),d7		
+#endif
+	bge	Laddsf$a$small
+#ifndef __mcoldfire__
+	cmpw	IMM (16),d7	| if difference >= 16 swap
+#else
+	cmpl	IMM (16),d7	| if difference >= 16 swap
+#endif
+	bge	8f
+6:
+#ifndef __mcoldfire__
+	subw	IMM (1),d7
+#else
+	subl	IMM (1),d7
+#endif
+7:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0	| shift right first operand
+	roxrl	IMM (1),d1
+	dbra	d7,7b
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	subql	IMM (1),d7
+	bpl	7b
+#endif
+	bra	Laddsf$3
+8:
+	movew	d0,d1
+	swap	d1
+	movew	d1,d0
+	swap	d0
+#ifndef __mcoldfire__
+	subw	IMM (16),d7
+#else
+	subl	IMM (16),d7
+#endif
+	bne	6b		| if still more bits, go back to normal case
+				| otherwise we fall through
+
+| Now we have a in d0-d1, b in d2-d3, and the largest exponent in d6 (the
+| signs are stored in a0 and a1).
+
+Laddsf$3:
+| Here we have to decide whether to add or subtract the numbers
+#ifndef __mcoldfire__
+	exg	d6,a0		| get signs back
+	exg	d7,a1		| and save the exponents
+#else
+	movel	d6,d4
+	movel	a0,d6
+	movel	d4,a0
+	movel	d7,d4
+	movel	a1,d7
+	movel	d4,a1
+#endif
+	eorl	d6,d7		| combine sign bits
+	bmi	Lsubsf$0	| if negative a and b have opposite 
+				| sign so we actually subtract the
+				| numbers
+
+| Here we have both positive or both negative
+#ifndef __mcoldfire__
+	exg	d6,a0		| now we have the exponent in d6
+#else
+	movel	d6,d4
+	movel	a0,d6
+	movel	d4,a0
+#endif
+	movel	a0,d7		| and sign in d7
+	andl	IMM (0x80000000),d7
+| Here we do the addition.
+	addl	d3,d1
+	addxl	d2,d0
+| Note: now we have d2, d3, d4 and d5 to play with! 
+
+| Put the exponent, in the first byte, in d2, to use the "standard" rounding
+| routines:
+	movel	d6,d2
+#ifndef __mcoldfire__
+	lsrw	IMM (8),d2
+#else
+	lsrl	IMM (8),d2
+#endif
+
+| Before rounding normalize so bit #FLT_MANT_DIG is set (we will consider
+| the case of denormalized numbers in the rounding routine itself).
+| As in the addition (not in the subtraction!) we could have set 
+| one more bit we check this:
+	btst	IMM (FLT_MANT_DIG+1),d0	
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+#endif
+	addl	IMM (1),d2
+1:
+	lea	pc@(Laddsf$4),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Laddsf$4:
+| Put back the exponent, but check for overflow.
+#ifndef __mcoldfire__
+	cmpw	IMM (0xff),d2
+#else
+	cmpl	IMM (0xff),d2
+#endif
+	bhi	1f
+	bclr	IMM (FLT_MANT_DIG-1),d0
+#ifndef __mcoldfire__
+	lslw	IMM (7),d2
+#else
+	lsll	IMM (7),d2
+#endif
+	swap	d2
+	orl	d2,d0
+	bra	Laddsf$ret
+1:
+	moveq	IMM (ADD),d5
+	bra	Lf$overflow
+
+Lsubsf$0:
+| We are here if a > 0 and b < 0 (sign bits cleared).
+| Here we do the subtraction.
+	movel	d6,d7		| put sign in d7
+	andl	IMM (0x80000000),d7
+
+	subl	d3,d1		| result in d0-d1
+	subxl	d2,d0		|
+	beq	Laddsf$ret	| if zero just exit
+	bpl	1f		| if positive skip the following
+	bchg	IMM (31),d7	| change sign bit in d7
+	negl	d1
+	negxl	d0
+1:
+#ifndef __mcoldfire__
+	exg	d2,a0		| now we have the exponent in d2
+	lsrw	IMM (8),d2	| put it in the first byte
+#else
+	movel	d2,d4
+	movel	a0,d2
+	movel	d4,a0
+	lsrl	IMM (8),d2	| put it in the first byte
+#endif
+
+| Now d0-d1 is positive and the sign bit is in d7.
+
+| Note that we do not have to normalize, since in the subtraction bit
+| #FLT_MANT_DIG+1 is never set, and denormalized numbers are handled by
+| the rounding routines themselves.
+	lea	pc@(Lsubsf$1),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Lsubsf$1:
+| Put back the exponent (we can't have overflow!). '
+	bclr	IMM (FLT_MANT_DIG-1),d0
+#ifndef __mcoldfire__
+	lslw	IMM (7),d2
+#else
+	lsll	IMM (7),d2
+#endif
+	swap	d2
+	orl	d2,d0
+	bra	Laddsf$ret
+
+| If one of the numbers was too small (difference of exponents >= 
+| FLT_MANT_DIG+2) we return the other (and now we don't have to '
+| check for finiteness or zero).
+Laddsf$a$small:
+	movel	a3,d0
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7/a2-a3	| restore data registers
+#else
+	moveml	sp@,d3-d7/a2-a3
+	lea	a6@(28),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	rts		| and return
+
+Laddsf$b$small:
+	movel	a2,d0
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7/a2-a3	| restore data registers
+#else
+	moveml	sp@,d3-d7/a2-a3
+	lea	a6@(28),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	rts		| and return
+
+| If the numbers are denormalized remember to put exponent equal to 1.
+
+Laddsf$a$den:
+	movel	d5,d6		| d5 contains 0x01000000
+	swap	d6
+	bra	Laddsf$1
+
+Laddsf$b$den:
+	movel	d5,d7
+	swap	d7
+	notl 	d4		| make d4 into a mask for the fraction
+				| (this was not executed after the jump)
+	bra	Laddsf$2
+
+| The rest is mainly code for the different results which can be 
+| returned (checking always for +/-INFINITY and NaN).
+
+Laddsf$b:
+| Return b (if a is zero).
+	movel	a3,d0
+	cmpl	IMM (0x80000000),d0	| Check if b is -0
+	bne	1f
+	movel	a0,d7
+	andl	IMM (0x80000000),d7	| Use the sign of a
+	clrl	d0
+	bra	Laddsf$ret
+Laddsf$a:
+| Return a (if b is zero).
+	movel	a2,d0
+1:
+	moveq	IMM (ADD),d5
+| We have to check for NaN and +/-infty.
+	movel	d0,d7
+	andl	IMM (0x80000000),d7	| put sign in d7
+	bclr	IMM (31),d0		| clear sign
+	cmpl	IMM (INFINITY),d0	| check for infty or NaN
+	bge	2f
+	movel	d0,d0		| check for zero (we do this because we don't '
+	bne	Laddsf$ret	| want to return -0 by mistake
+	bclr	IMM (31),d7	| if zero be sure to clear sign
+	bra	Laddsf$ret	| if everything OK just return
+2:
+| The value to be returned is either +/-infty or NaN
+	andl	IMM (0x007fffff),d0	| check for NaN
+	bne	Lf$inop			| if mantissa not zero is NaN
+	bra	Lf$infty
+
+Laddsf$ret:
+| Normal exit (a and b nonzero, result is not NaN nor +/-infty).
+| We have to clear the exception flags (just the exception type).
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+	orl	d7,d0		| put sign bit
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7/a2-a3	| restore data registers
+#else
+	moveml	sp@,d3-d7/a2-a3
+	lea	a6@(28),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	rts		| and return
+
+Laddsf$ret$den:
+| Return a denormalized number (for addition we don't signal underflow) '
+	lsrl	IMM (1),d0	| remember to shift right back once
+	bra	Laddsf$ret	| and return
+
+| Note: when adding two floats of the same sign if either one is 
+| NaN we return NaN without regard to whether the other is finite or 
+| not. When subtracting them (i.e., when adding two numbers of 
+| opposite signs) things are more complicated: if both are INFINITY 
+| we return NaN, if only one is INFINITY and the other is NaN we return
+| NaN, but if it is finite we return INFINITY with the corresponding sign.
+
+Laddsf$nf:
+	moveq	IMM (ADD),d5
+| This could be faster but it is not worth the effort, since it is not
+| executed very often. We sacrifice speed for clarity here.
+	movel	a2,d0	| get the numbers back (remember that we
+	movel	a3,d1	| did some processing already)
+	movel	IMM (INFINITY),d4 | useful constant (INFINITY)
+	movel	d0,d2		| save sign bits
+	movel	d1,d3
+	bclr	IMM (31),d0	| clear sign bits
+	bclr	IMM (31),d1
+| We know that one of them is either NaN of +/-INFINITY
+| Check for NaN (if either one is NaN return NaN)
+	cmpl	d4,d0		| check first a (d0)
+	bhi	Lf$inop		
+	cmpl	d4,d1		| check now b (d1)
+	bhi	Lf$inop		
+| Now comes the check for +/-INFINITY. We know that both are (maybe not
+| finite) numbers, but we have to check if both are infinite whether we
+| are adding or subtracting them.
+	eorl	d3,d2		| to check sign bits
+	bmi	1f
+	movel	d0,d7
+	andl	IMM (0x80000000),d7	| get (common) sign bit
+	bra	Lf$infty
+1:
+| We know one (or both) are infinite, so we test for equality between the
+| two numbers (if they are equal they have to be infinite both, so we
+| return NaN).
+	cmpl	d1,d0		| are both infinite?
+	beq	Lf$inop		| if so return NaN
+
+	movel	d0,d7
+	andl	IMM (0x80000000),d7 | get a's sign bit '
+	cmpl	d4,d0		| test now for infinity
+	beq	Lf$infty	| if a is INFINITY return with this sign
+	bchg	IMM (31),d7	| else we know b is INFINITY and has
+	bra	Lf$infty	| the opposite sign
+
+|=============================================================================
+|                             __mulsf3
+|=============================================================================
+
+| float __mulsf3(float, float);
+	FUNC(__mulsf3)
+SYM (__mulsf3):
+|	DEBUG	15
+#ifndef __mcoldfire__
+	moveml	d3-d7/a2-a3,sp@-
+#else
+	lea	a6@(-28),a6	
+	moveml	d3-d7/a2-a3,sp@
+#endif
+	movel	d0,a2		| store a into a0
+	movel	d1,a3		| store b into a1
+	movel	d0,d7		| d7 will hold the sign of the product
+	eorl	d1,d7		|
+	andl	IMM (0x80000000),d7
+	movel	IMM (INFINITY),d6	| useful constant (+INFINITY)
+	movel	d6,d5			| another (mask for fraction)
+	notl	d5			|
+	movel	IMM (0x00800000),d4	| this is to put hidden bit back
+	bclr	IMM (31),d0		| get rid of a's sign bit '
+	movel	d0,d2			|
+	beq	Lmulsf$a$0		| branch if a is zero
+	bclr	IMM (31),d1		| get rid of b's sign bit '
+	movel	d1,d3		|
+	beq	Lmulsf$b$0	| branch if b is zero
+	cmpl	d6,d0		| is a big?
+	bhi	Lmulsf$inop	| if a is NaN return NaN
+	beq	Lmulsf$inf	| if a is INFINITY we have to check b
+	cmpl	d6,d1		| now compare b with INFINITY
+	bhi	Lmulsf$inop	| is b NaN?
+	beq	Lmulsf$overflow | is b INFINITY?
+| Here we have both numbers finite and nonzero (and with no sign bit).
+| Now we get the exponents into d2 and d3.
+	andl	d6,d2		| and isolate exponent in d2
+	beq	Lmulsf$a$den	| if exponent is zero we have a denormalized
+	andl	d5,d0		| and isolate fraction
+	orl	d4,d0		| and put hidden bit back
+	swap	d2		| I like exponents in the first byte
+#ifndef __mcoldfire__
+	lsrw	IMM (7),d2	| 
+#else
+	lsrl	IMM (7),d2	| 
+#endif
+Lmulsf$1:			| number
+	andl	d6,d3		|
+	beq	Lmulsf$b$den	|
+	andl	d5,d1		|
+	orl	d4,d1		|
+	swap	d3		|
+#ifndef __mcoldfire__
+	lsrw	IMM (7),d3	|
+#else
+	lsrl	IMM (7),d3	|
+#endif
+Lmulsf$2:			|
+#ifndef __mcoldfire__
+	addw	d3,d2		| add exponents
+	subw	IMM (F_BIAS+1),d2 | and subtract bias (plus one)
+#else
+	addl	d3,d2		| add exponents
+	subl	IMM (F_BIAS+1),d2 | and subtract bias (plus one)
+#endif
+
+| We are now ready to do the multiplication. The situation is as follows:
+| both a and b have bit FLT_MANT_DIG-1 set (even if they were 
+| denormalized to start with!), which means that in the product 
+| bit 2*(FLT_MANT_DIG-1) (that is, bit 2*FLT_MANT_DIG-2-32 of the 
+| high long) is set. 
+
+| To do the multiplication let us move the number a little bit around ...
+	movel	d1,d6		| second operand in d6
+	movel	d0,d5		| first operand in d4-d5
+	movel	IMM (0),d4
+	movel	d4,d1		| the sums will go in d0-d1
+	movel	d4,d0
+
+| now bit FLT_MANT_DIG-1 becomes bit 31:
+	lsll	IMM (31-FLT_MANT_DIG+1),d6		
+
+| Start the loop (we loop #FLT_MANT_DIG times):
+	moveq	IMM (FLT_MANT_DIG-1),d3	
+1:	addl	d1,d1		| shift sum 
+	addxl	d0,d0
+	lsll	IMM (1),d6	| get bit bn
+	bcc	2f		| if not set skip sum
+	addl	d5,d1		| add a
+	addxl	d4,d0
+2:
+#ifndef __mcoldfire__
+	dbf	d3,1b		| loop back
+#else
+	subql	IMM (1),d3
+	bpl	1b
+#endif
+
+| Now we have the product in d0-d1, with bit (FLT_MANT_DIG - 1) + FLT_MANT_DIG
+| (mod 32) of d0 set. The first thing to do now is to normalize it so bit 
+| FLT_MANT_DIG is set (to do the rounding).
+#ifndef __mcoldfire__
+	rorl	IMM (6),d1
+	swap	d1
+	movew	d1,d3
+	andw	IMM (0x03ff),d3
+	andw	IMM (0xfd00),d1
+#else
+	movel	d1,d3
+	lsll	IMM (8),d1
+	addl	d1,d1
+	addl	d1,d1
+	moveq	IMM (22),d5
+	lsrl	d5,d3
+	orl	d3,d1
+	andl	IMM (0xfffffd00),d1
+#endif
+	lsll	IMM (8),d0
+	addl	d0,d0
+	addl	d0,d0
+#ifndef __mcoldfire__
+	orw	d3,d0
+#else
+	orl	d3,d0
+#endif
+
+	moveq	IMM (MULTIPLY),d5
+	
+	btst	IMM (FLT_MANT_DIG+1),d0
+	beq	Lround$exit
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	addw	IMM (1),d2
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	addql	IMM (1),d2
+#endif
+	bra	Lround$exit
+
+Lmulsf$inop:
+	moveq	IMM (MULTIPLY),d5
+	bra	Lf$inop
+
+Lmulsf$overflow:
+	moveq	IMM (MULTIPLY),d5
+	bra	Lf$overflow
+
+Lmulsf$inf:
+	moveq	IMM (MULTIPLY),d5
+| If either is NaN return NaN; else both are (maybe infinite) numbers, so
+| return INFINITY with the correct sign (which is in d7).
+	cmpl	d6,d1		| is b NaN?
+	bhi	Lf$inop		| if so return NaN
+	bra	Lf$overflow	| else return +/-INFINITY
+
+| If either number is zero return zero, unless the other is +/-INFINITY, 
+| or NaN, in which case we return NaN.
+Lmulsf$b$0:
+| Here d1 (==b) is zero.
+	movel	a2,d1		| get a again to check for non-finiteness
+	bra	1f
+Lmulsf$a$0:
+	movel	a3,d1	| get b again to check for non-finiteness
+1:	bclr	IMM (31),d1	| clear sign bit 
+	cmpl	IMM (INFINITY),d1 | and check for a large exponent
+	bge	Lf$inop		| if b is +/-INFINITY or NaN return NaN
+	movel	d7,d0		| else return signed zero
+	PICLEA	SYM (_fpCCR),a0	|
+	movew	IMM (0),a0@	| 
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7/a2-a3	| 
+#else
+	moveml	sp@,d3-d7/a2-a3
+	lea	a6@(28),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	rts			| 
+
+| If a number is denormalized we put an exponent of 1 but do not put the 
+| hidden bit back into the fraction; instead we shift left until bit 23
+| (the hidden bit) is set, adjusting the exponent accordingly. We do this
+| to ensure that the product of the fractions is close to 1.
+Lmulsf$a$den:
+	movel	IMM (1),d2
+	andl	d5,d0
+1:	addl	d0,d0		| shift a left (until bit 23 is set)
+#ifndef __mcoldfire__
+	subw	IMM (1),d2	| and adjust exponent
+#else
+	subql	IMM (1),d2	| and adjust exponent
+#endif
+	btst	IMM (FLT_MANT_DIG-1),d0
+	bne	Lmulsf$1	|
+	bra	1b		| else loop back
+
+Lmulsf$b$den:
+	movel	IMM (1),d3
+	andl	d5,d1
+1:	addl	d1,d1		| shift b left until bit 23 is set
+#ifndef __mcoldfire__
+	subw	IMM (1),d3	| and adjust exponent
+#else
+	subql	IMM (1),d3	| and adjust exponent
+#endif
+	btst	IMM (FLT_MANT_DIG-1),d1
+	bne	Lmulsf$2	|
+	bra	1b		| else loop back
+
+|=============================================================================
+|                             __divsf3
+|=============================================================================
+
+| float __divsf3(float, float);
+	FUNC(__divsf3)
+SYM (__divsf3):
+|	DEBUG	16
+#ifndef __mcoldfire__
+	moveml	d3-d7,sp@-
+#else
+	lea	a6@(-20),a6	
+	moveml	d3-d7,sp@
+#endif
+	movel	d0,d7			| d7 will hold the sign of the result
+	eorl	d1,d7			|
+	andl	IMM (0x80000000),d7	| 
+	movel	IMM (INFINITY),d6	| useful constant (+INFINITY)
+	movel	d6,d5			| another (mask for fraction)
+	notl	d5			|
+	movel	IMM (0x00800000),d4	| this is to put hidden bit back
+	bclr	IMM (31),d0		| get rid of a's sign bit '
+	movel	d0,d2			|
+	beq	Ldivsf$a$0		| branch if a is zero
+	bclr	IMM (31),d1		| get rid of b's sign bit '
+	movel	d1,d3			|
+	beq	Ldivsf$b$0		| branch if b is zero
+	cmpl	d6,d0			| is a big?
+	bhi	Ldivsf$inop		| if a is NaN return NaN
+	beq	Ldivsf$inf		| if a is INFINITY we have to check b
+	cmpl	d6,d1			| now compare b with INFINITY 
+	bhi	Ldivsf$inop		| if b is NaN return NaN
+	beq	Ldivsf$underflow
+| Here we have both numbers finite and nonzero (and with no sign bit).
+| Now we get the exponents into d2 and d3 and normalize the numbers to
+| ensure that the ratio of the fractions is close to 1. We do this by
+| making sure that bit #FLT_MANT_DIG-1 (hidden bit) is set.
+	andl	d6,d2		| and isolate exponent in d2
+	beq	Ldivsf$a$den	| if exponent is zero we have a denormalized
+	andl	d5,d0		| and isolate fraction
+	orl	d4,d0		| and put hidden bit back
+	swap	d2		| I like exponents in the first byte
+#ifndef __mcoldfire__
+	lsrw	IMM (7),d2	| 
+#else
+	lsrl	IMM (7),d2	| 
+#endif
+Ldivsf$1:			| 
+	andl	d6,d3		|
+	beq	Ldivsf$b$den	|
+	andl	d5,d1		|
+	orl	d4,d1		|
+	swap	d3		|
+#ifndef __mcoldfire__
+	lsrw	IMM (7),d3	|
+#else
+	lsrl	IMM (7),d3	|
+#endif
+Ldivsf$2:			|
+#ifndef __mcoldfire__
+	subw	d3,d2		| subtract exponents
+ 	addw	IMM (F_BIAS),d2	| and add bias
+#else
+	subl	d3,d2		| subtract exponents
+ 	addl	IMM (F_BIAS),d2	| and add bias
+#endif
+ 
+| We are now ready to do the division. We have prepared things in such a way
+| that the ratio of the fractions will be less than 2 but greater than 1/2.
+| At this point the registers in use are:
+| d0	holds a (first operand, bit FLT_MANT_DIG=0, bit FLT_MANT_DIG-1=1)
+| d1	holds b (second operand, bit FLT_MANT_DIG=1)
+| d2	holds the difference of the exponents, corrected by the bias
+| d7	holds the sign of the ratio
+| d4, d5, d6 hold some constants
+	movel	d7,a0		| d6-d7 will hold the ratio of the fractions
+	movel	IMM (0),d6	| 
+	movel	d6,d7
+
+	moveq	IMM (FLT_MANT_DIG+1),d3
+1:	cmpl	d0,d1		| is a < b?
+	bhi	2f		|
+	bset	d3,d6		| set a bit in d6
+	subl	d1,d0		| if a >= b  a <-- a-b
+	beq	3f		| if a is zero, exit
+2:	addl	d0,d0		| multiply a by 2
+#ifndef __mcoldfire__
+	dbra	d3,1b
+#else
+	subql	IMM (1),d3
+	bpl	1b
+#endif
+
+| Now we keep going to set the sticky bit ...
+	moveq	IMM (FLT_MANT_DIG),d3
+1:	cmpl	d0,d1
+	ble	2f
+	addl	d0,d0
+#ifndef __mcoldfire__
+	dbra	d3,1b
+#else
+	subql	IMM(1),d3
+	bpl	1b
+#endif
+	movel	IMM (0),d1
+	bra	3f
+2:	movel	IMM (0),d1
+#ifndef __mcoldfire__
+	subw	IMM (FLT_MANT_DIG),d3
+	addw	IMM (31),d3
+#else
+	subl	IMM (FLT_MANT_DIG),d3
+	addl	IMM (31),d3
+#endif
+	bset	d3,d1
+3:
+	movel	d6,d0		| put the ratio in d0-d1
+	movel	a0,d7		| get sign back
+
+| Because of the normalization we did before we are guaranteed that 
+| d0 is smaller than 2^26 but larger than 2^24. Thus bit 26 is not set,
+| bit 25 could be set, and if it is not set then bit 24 is necessarily set.
+	btst	IMM (FLT_MANT_DIG+1),d0		
+	beq	1f              | if it is not set, then bit 24 is set
+	lsrl	IMM (1),d0	|
+#ifndef __mcoldfire__
+	addw	IMM (1),d2	|
+#else
+	addl	IMM (1),d2	|
+#endif
+1:
+| Now round, check for over- and underflow, and exit.
+	moveq	IMM (DIVIDE),d5
+	bra	Lround$exit
+
+Ldivsf$inop:
+	moveq	IMM (DIVIDE),d5
+	bra	Lf$inop
+
+Ldivsf$overflow:
+	moveq	IMM (DIVIDE),d5
+	bra	Lf$overflow
+
+Ldivsf$underflow:
+	moveq	IMM (DIVIDE),d5
+	bra	Lf$underflow
+
+Ldivsf$a$0:
+	moveq	IMM (DIVIDE),d5
+| If a is zero check to see whether b is zero also. In that case return
+| NaN; then check if b is NaN, and return NaN also in that case. Else
+| return a properly signed zero.
+	andl	IMM (0x7fffffff),d1	| clear sign bit and test b
+	beq	Lf$inop			| if b is also zero return NaN
+	cmpl	IMM (INFINITY),d1	| check for NaN
+	bhi	Lf$inop			| 
+	movel	d7,d0			| else return signed zero
+	PICLEA	SYM (_fpCCR),a0		|
+	movew	IMM (0),a0@		|
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7		| 
+#else
+	moveml	sp@,d3-d7		| 
+	lea	a6@(20),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	rts				| 
+	
+Ldivsf$b$0:
+	moveq	IMM (DIVIDE),d5
+| If we got here a is not zero. Check if a is NaN; in that case return NaN,
+| else return +/-INFINITY. Remember that a is in d0 with the sign bit 
+| cleared already.
+	cmpl	IMM (INFINITY),d0	| compare d0 with INFINITY
+	bhi	Lf$inop			| if larger it is NaN
+	bra	Lf$div$0		| else signal DIVIDE_BY_ZERO
+
+Ldivsf$inf:
+	moveq	IMM (DIVIDE),d5
+| If a is INFINITY we have to check b
+	cmpl	IMM (INFINITY),d1	| compare b with INFINITY 
+	bge	Lf$inop			| if b is NaN or INFINITY return NaN
+	bra	Lf$overflow		| else return overflow
+
+| If a number is denormalized we put an exponent of 1 but do not put the 
+| bit back into the fraction.
+Ldivsf$a$den:
+	movel	IMM (1),d2
+	andl	d5,d0
+1:	addl	d0,d0		| shift a left until bit FLT_MANT_DIG-1 is set
+#ifndef __mcoldfire__
+	subw	IMM (1),d2	| and adjust exponent
+#else
+	subl	IMM (1),d2	| and adjust exponent
+#endif
+	btst	IMM (FLT_MANT_DIG-1),d0
+	bne	Ldivsf$1
+	bra	1b
+
+Ldivsf$b$den:
+	movel	IMM (1),d3
+	andl	d5,d1
+1:	addl	d1,d1		| shift b left until bit FLT_MANT_DIG is set
+#ifndef __mcoldfire__
+	subw	IMM (1),d3	| and adjust exponent
+#else
+	subl	IMM (1),d3	| and adjust exponent
+#endif
+	btst	IMM (FLT_MANT_DIG-1),d1
+	bne	Ldivsf$2
+	bra	1b
+
+Lround$exit:
+| This is a common exit point for __mulsf3 and __divsf3. 
+
+| First check for underlow in the exponent:
+#ifndef __mcoldfire__
+	cmpw	IMM (-FLT_MANT_DIG-1),d2		
+#else
+	cmpl	IMM (-FLT_MANT_DIG-1),d2		
+#endif
+	blt	Lf$underflow	
+| It could happen that the exponent is less than 1, in which case the 
+| number is denormalized. In this case we shift right and adjust the 
+| exponent until it becomes 1 or the fraction is zero (in the latter case 
+| we signal underflow and return zero).
+	movel	IMM (0),d6	| d6 is used temporarily
+#ifndef __mcoldfire__
+	cmpw	IMM (1),d2	| if the exponent is less than 1 we 
+#else
+	cmpl	IMM (1),d2	| if the exponent is less than 1 we 
+#endif
+	bge	2f		| have to shift right (denormalize)
+1:
+#ifndef __mcoldfire__
+	addw	IMM (1),d2	| adjust the exponent
+	lsrl	IMM (1),d0	| shift right once 
+	roxrl	IMM (1),d1	|
+	roxrl	IMM (1),d6	| d6 collect bits we would lose otherwise
+	cmpw	IMM (1),d2	| is the exponent 1 already?
+#else
+	addql	IMM (1),d2	| adjust the exponent
+	lsrl	IMM (1),d6
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d6
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	cmpl	IMM (1),d2	| is the exponent 1 already?
+#endif
+	beq	2f		| if not loop back
+	bra	1b              |
+	bra	Lf$underflow	| safety check, shouldn't execute '
+2:	orl	d6,d1		| this is a trick so we don't lose  '
+				| the extra bits which were flushed right
+| Now call the rounding routine (which takes care of denormalized numbers):
+	lea	pc@(Lround$0),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Lround$0:
+| Here we have a correctly rounded result (either normalized or denormalized).
+
+| Here we should have either a normalized number or a denormalized one, and
+| the exponent is necessarily larger or equal to 1 (so we don't have to  '
+| check again for underflow!). We have to check for overflow or for a 
+| denormalized number (which also signals underflow).
+| Check for overflow (i.e., exponent >= 255).
+#ifndef __mcoldfire__
+	cmpw	IMM (0x00ff),d2
+#else
+	cmpl	IMM (0x00ff),d2
+#endif
+	bge	Lf$overflow
+| Now check for a denormalized number (exponent==0).
+	movew	d2,d2
+	beq	Lf$den
+1:
+| Put back the exponents and sign and return.
+#ifndef __mcoldfire__
+	lslw	IMM (7),d2	| exponent back to fourth byte
+#else
+	lsll	IMM (7),d2	| exponent back to fourth byte
+#endif
+	bclr	IMM (FLT_MANT_DIG-1),d0
+	swap	d0		| and put back exponent
+#ifndef __mcoldfire__
+	orw	d2,d0		| 
+#else
+	orl	d2,d0
+#endif
+	swap	d0		|
+	orl	d7,d0		| and sign also
+
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7
+#else
+	moveml	sp@,d3-d7
+	lea	a6@(20),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	rts
+
+|=============================================================================
+|                             __negsf2
+|=============================================================================
+
+| This is trivial and could be shorter if we didn't bother checking for NaN '
+| and +/-INFINITY.
+
+| float __negsf2(float);
+	FUNC(__negsf2)
+SYM (__negsf2):
+|	DEBUG	17
+#ifndef __mcoldfire__
+	moveml	d3-d7,sp@-
+#else
+	lea	a6@(-20),a6	
+	moveml	d3-d7,sp@
+#endif
+	moveq	IMM (NEGATE),d5
+	bchg	IMM (31),d0	| negate
+	movel	d0,d1		| make a positive copy
+	bclr	IMM (31),d1	|
+	tstl	d1		| check for zero
+	beq	2f		| if zero (either sign) return +zero
+	cmpl	IMM (INFINITY),d1 | compare to +INFINITY
+	blt	1f		|
+	bhi	Lf$inop		| if larger (fraction not zero) is NaN
+	movel	d0,d7		| else get sign and return INFINITY
+	andl	IMM (0x80000000),d7
+	bra	Lf$infty		
+1:	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7
+#else
+	moveml	sp@,d3-d7
+	lea	a6@(20),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	rts
+2:	bclr	IMM (31),d0
+	bra	1b
+
+|=============================================================================
+|                             __cmpsf2
+|=============================================================================
+
+GREATER =  1
+LESS    = -1
+EQUAL   =  0
+
+| Last int arg is actually sent in a0!
+| int __cmpsf2_internal(float, float, int);
+SYM (__cmpsf2_internal):
+|	DEBUG	18
+#ifndef __mcoldfire__
+	moveml	d3-d7,sp@- 	| save registers
+#else
+	lea	a6@(-20),a6	
+	moveml	d3-d7,sp@
+#endif
+	moveq	IMM (COMPARE),d5
+| Check if either is NaN, and in that case return garbage and signal
+| INVALID_OPERATION. Check also if either is zero, and clear the signs
+| if necessary.
+	movel	d0,d6
+	andl	IMM (0x7fffffff),d0
+	beq	Lcmpsf$a$0
+	cmpl	IMM (0x7f800000),d0
+	bhi	Lcmpf$inop
+Lcmpsf$1:
+	movel	d1,d7
+	andl	IMM (0x7fffffff),d1
+	beq	Lcmpsf$b$0
+	cmpl	IMM (0x7f800000),d1
+	bhi	Lcmpf$inop
+Lcmpsf$2:
+| Check the signs
+	eorl	d6,d7
+	bpl	1f
+| If the signs are not equal check if a >= 0
+	tstl	d6
+	bpl	Lcmpsf$a$gt$b	| if (a >= 0 && b < 0) => a > b
+	bmi	Lcmpsf$b$gt$a	| if (a < 0 && b >= 0) => a < b
+1:
+| If the signs are equal check for < 0
+	tstl	d6
+	bpl	1f
+| If both are negative exchange them
+#ifndef __mcoldfire__
+	exg	d0,d1
+#else
+	movel	d0,d7
+	movel	d1,d0
+	movel	d7,d1
+#endif
+1:
+| Now that they are positive we just compare them as longs (does this also
+| work for denormalized numbers?).
+	cmpl	d0,d1
+	bhi	Lcmpsf$b$gt$a	| |b| > |a|
+	bne	Lcmpsf$a$gt$b	| |b| < |a|
+| If we got here a == b.
+	movel	IMM (EQUAL),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7 	| put back the registers
+#else
+	moveml	sp@,d3-d7
+	lea	a6@(20),a6	
+#endif
+	rts
+Lcmpsf$a$gt$b:
+	movel	IMM (GREATER),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7 	| put back the registers
+#else
+	moveml	sp@,d3-d7
+	lea	a6@(20),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	rts
+Lcmpsf$b$gt$a:
+	movel	IMM (LESS),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d3-d7 	| put back the registers
+#else
+	moveml	sp@,d3-d7
+	lea	a6@(20),a6	
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	rts
+
+Lcmpsf$a$0:	
+	bclr	IMM (31),d6
+	bra	Lcmpsf$1
+Lcmpsf$b$0:
+	bclr	IMM (31),d7
+	bra	Lcmpsf$2
+
+Lcmpf$inop:
+	movel	a0,d0
+	moveq	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+| int __cmpsf2(float, float);
+	FUNC(__cmpsf2)
+SYM (__cmpsf2):
+|	DEBUG	19
+	lea	1.w,a0
+	bra	SYM (__cmpsf2_internal)
+
+|=============================================================================
+|                           rounding routines
+|=============================================================================
+
+| The rounding routines expect the number to be normalized in registers
+| d0-d1, with the exponent in register d2. They assume that the 
+| exponent is larger or equal to 1. They return a properly normalized number
+| if possible, and a denormalized number otherwise. The exponent is returned
+| in d2.
+
+Lround$to$nearest:
+| We now normalize as suggested by D. Knuth ("Seminumerical Algorithms"):
+| Here we assume that the exponent is not too small (this should be checked
+| before entering the rounding routine), but the number could be denormalized.
+
+| Check for denormalized numbers:
+1:	btst	IMM (FLT_MANT_DIG),d0
+	bne	2f		| if set the number is normalized
+| Normalize shifting left until bit #FLT_MANT_DIG is set or the exponent 
+| is one (remember that a denormalized number corresponds to an 
+| exponent of -F_BIAS+1).
+#ifndef __mcoldfire__
+	cmpw	IMM (1),d2	| remember that the exponent is at least one
+#else
+	cmpl	IMM (1),d2	| remember that the exponent is at least one
+#endif
+ 	beq	2f		| an exponent of one means denormalized
+	addl	d1,d1		| else shift and adjust the exponent
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d2,1b		|
+#else
+	subql	IMM (1),d2
+	bpl	1b
+#endif
+2:
+| Now round: we do it as follows: after the shifting we can write the
+| fraction part as f + delta, where 1 < f < 2^25, and 0 <= delta <= 2.
+| If delta < 1, do nothing. If delta > 1, add 1 to f. 
+| If delta == 1, we make sure the rounded number will be even (odd?) 
+| (after shifting).
+	btst	IMM (0),d0	| is delta < 1?
+	beq	2f		| if so, do not do anything
+	tstl	d1		| is delta == 1?
+	bne	1f		| if so round to even
+	movel	d0,d1		| 
+	andl	IMM (2),d1	| bit 1 is the last significant bit
+	addl	d1,d0		| 
+	bra	2f		| 
+1:	movel	IMM (1),d1	| else add 1 
+	addl	d1,d0		|
+| Shift right once (because we used bit #FLT_MANT_DIG!).
+2:	lsrl	IMM (1),d0		
+| Now check again bit #FLT_MANT_DIG (rounding could have produced a
+| 'fraction overflow' ...).
+	btst	IMM (FLT_MANT_DIG),d0	
+	beq	1f
+	lsrl	IMM (1),d0
+#ifndef __mcoldfire__
+	addw	IMM (1),d2
+#else
+	addql	IMM (1),d2
+#endif
+1:
+| If bit #FLT_MANT_DIG-1 is clear we have a denormalized number, so we 
+| have to put the exponent to zero and return a denormalized number.
+	btst	IMM (FLT_MANT_DIG-1),d0
+	beq	1f
+	jmp	a0@
+1:	movel	IMM (0),d2
+	jmp	a0@
+
+Lround$to$zero:
+Lround$to$plus:
+Lround$to$minus:
+	jmp	a0@
+#endif /* L_float */
+
+| gcc expects the routines __eqdf2, __nedf2, __gtdf2, __gedf2,
+| __ledf2, __ltdf2 to all return the same value as a direct call to
+| __cmpdf2 would.  In this implementation, each of these routines
+| simply calls __cmpdf2.  It would be more efficient to give the
+| __cmpdf2 routine several names, but separating them out will make it
+| easier to write efficient versions of these routines someday.
+| If the operands recompare unordered unordered __gtdf2 and __gedf2 return -1.
+| The other routines return 1.
+
+#ifdef  L_eqdf2
+	.text
+	FUNC(__eqdf2)
+	.globl	SYM (__eqdf2)
+SYM (__eqdf2):
+|	DEBUG	20
+	lea	1.w,a0
+	bra	SYM (__cmpdf2_internal)
+#endif /* L_eqdf2 */
+
+#ifdef  L_nedf2
+	.text
+	FUNC(__nedf2)
+	.globl	SYM (__nedf2)
+SYM (__nedf2):
+|	DEBUG	21
+	lea	1.w,a0
+	bra	SYM (__cmpdf2_internal)
+#endif /* L_nedf2 */
+
+#ifdef  L_gtdf2
+	.text
+	FUNC(__gtdf2)
+	.globl	SYM (__gtdf2)
+SYM (__gtdf2):
+|	DEBUG	22
+	lea	-1.w,a0
+	bra	SYM (__cmpdf2_internal)
+#endif /* L_gtdf2 */
+
+#ifdef  L_gedf2
+	.text
+	FUNC(__gedf2)
+	.globl	SYM (__gedf2)
+SYM (__gedf2):
+|	DEBUG	23
+	lea	-1.w,a0
+	bra	SYM (__cmpdf2_internal)
+#endif /* L_gedf2 */
+
+#ifdef  L_ltdf2
+	.text
+	FUNC(__ltdf2)
+	.globl	SYM (__ltdf2)
+SYM (__ltdf2):
+|	DEBUG	24
+	lea	1.w,a0
+	bra	SYM (__cmpdf2_internal)
+#endif /* L_ltdf2 */
+
+#ifdef  L_ledf2
+	.text
+	FUNC(__ledf2)
+	.globl	SYM (__ledf2)
+SYM (__ledf2):
+|	DEBUG	25
+	lea	1.w,a0
+	bra	SYM (__cmpdf2_internal)
+#endif /* L_ledf2 */
+
+| The comments above about __eqdf2, et. al., also apply to __eqsf2,
+| et. al., except that the latter call __cmpsf2 rather than __cmpdf2.
+
+#ifdef  L_eqsf2
+	.text
+	FUNC(__eqsf2)
+	.globl	SYM (__eqsf2)
+SYM (__eqsf2):
+|	DEBUG	26
+	lea	1.w,a0
+	bra	SYM (__cmpsf2_internal)
+
+#endif /* L_eqsf2 */
+
+#ifdef  L_nesf2
+	.text
+	FUNC(__nesf2)
+	.globl	SYM (__nesf2)
+SYM (__nesf2):
+|	DEBUG	27
+	lea	1.w,a0
+	bra	SYM (__cmpsf2_internal)
+#endif /* L_nesf2 */
+
+#ifdef  L_gtsf2
+	.text
+	FUNC(__gtsf2)
+	.globl	SYM (__gtsf2)
+SYM (__gtsf2):
+|	DEBUG	28
+	lea	-1.w,a0
+	bra	SYM (__cmpsf2_internal)
+#endif /* L_gtsf2 */
+
+#ifdef  L_gesf2
+	.text
+	FUNC(__gesf2)
+	.globl	SYM (__gesf2)
+SYM (__gesf2):
+|	DEBUG	29
+	lea	-1.w,a0
+	bra	SYM (__cmpsf2_internal)
+#endif /* L_gesf2 */
+
+#ifdef  L_ltsf2
+	.text
+	FUNC(__ltsf2)
+	.globl	SYM (__ltsf2)
+SYM (__ltsf2):
+|	DEBUG	30
+	lea	1.w,a0
+	bra	SYM (__cmpsf2_internal)
+#endif /* L_ltsf2 */
+
+#ifdef  L_lesf2
+	.text
+	FUNC(__lesf2)
+	.globl	SYM (__lesf2)
+SYM (__lesf2):
+|	DEBUG	31
+	lea	1.w,a0
+	bra	SYM (__cmpsf2_internal)
+#endif /* L_lesf2 */
diff -rupN gcc-4.6.4.test/gcc/config/m68k/lb1sf68-std.asm gcc-4.6.4-fastcall/gcc/config/m68k/lb1sf68-std.asm
--- gcc-4.6.4.test/gcc/config/m68k/lb1sf68-std.asm	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.6.4-fastcall/gcc/config/m68k/lb1sf68-std.asm	2017-04-30 19:10:52.554947001 +0200
@@ -0,0 +1,3916 @@
+/* libgcc routines for 68000 w/o floating-point hardware.
+   Copyright (C) 1994, 1996, 1997, 1998, 2008, 2009 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it
+under the terms of the GNU General Public License as published by the
+Free Software Foundation; either version 3, or (at your option) any
+later version.
+
+This file is distributed in the hope that it will be useful, but
+WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+General Public License for more details.
+
+Under Section 7 of GPL version 3, you are granted additional
+permissions described in the GCC Runtime Library Exception, version
+3.1, as published by the Free Software Foundation.
+
+You should have received a copy of the GNU General Public License and
+a copy of the GCC Runtime Library Exception along with this program;
+see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
+<http://www.gnu.org/licenses/>.  */
+
+
+#ifdef L_floatex
+
+| This is an attempt at a decent floating point (single, double and 
+| extended double) code for the GNU C compiler. It should be easy to
+| adapt to other compilers (but beware of the local labels!).
+
+| Starting date: 21 October, 1990
+
+| It is convenient to introduce the notation (s,e,f) for a floating point
+| number, where s=sign, e=exponent, f=fraction. We will call a floating
+| point number fpn to abbreviate, independently of the precision.
+| Let MAX_EXP be in each case the maximum exponent (255 for floats, 1023 
+| for doubles and 16383 for long doubles). We then have the following 
+| different cases:
+|  1. Normalized fpns have 0 < e < MAX_EXP. They correspond to 
+|     (-1)^s x 1.f x 2^(e-bias-1).
+|  2. Denormalized fpns have e=0. They correspond to numbers of the form
+|     (-1)^s x 0.f x 2^(-bias).
+|  3. +/-INFINITY have e=MAX_EXP, f=0.
+|  4. Quiet NaN (Not a Number) have all bits set.
+|  5. Signaling NaN (Not a Number) have s=0, e=MAX_EXP, f=1.
+
+|=============================================================================
+|                                  exceptions
+|=============================================================================
+
+| This is the floating point condition code register (_fpCCR):
+|
+| struct {
+|   short _exception_bits;	
+|   short _trap_enable_bits;	
+|   short _sticky_bits;
+|   short _rounding_mode;
+|   short _format;
+|   short _last_operation;
+|   union {
+|     float sf;
+|     double df;
+|   } _operand1;
+|   union {
+|     float sf;
+|     double df;
+|   } _operand2;
+| } _fpCCR;
+
+	.data
+	.even
+
+	.globl	SYM (_fpCCR)
+	
+SYM (_fpCCR):
+__exception_bits:
+	.word	0
+__trap_enable_bits:
+	.word	0
+__sticky_bits:
+	.word	0
+__rounding_mode:
+	.word	ROUND_TO_NEAREST
+__format:
+	.word	NIL
+__last_operation:
+	.word	NOOP
+__operand1:
+	.long	0
+	.long	0
+__operand2:
+	.long 	0
+	.long	0
+
+| Offsets:
+EBITS  = __exception_bits - SYM (_fpCCR)
+TRAPE  = __trap_enable_bits - SYM (_fpCCR)
+STICK  = __sticky_bits - SYM (_fpCCR)
+ROUND  = __rounding_mode - SYM (_fpCCR)
+FORMT  = __format - SYM (_fpCCR)
+LASTO  = __last_operation - SYM (_fpCCR)
+OPER1  = __operand1 - SYM (_fpCCR)
+OPER2  = __operand2 - SYM (_fpCCR)
+
+| The following exception types are supported:
+INEXACT_RESULT 		= 0x0001
+UNDERFLOW 		= 0x0002
+OVERFLOW 		= 0x0004
+DIVIDE_BY_ZERO 		= 0x0008
+INVALID_OPERATION 	= 0x0010
+
+| The allowed rounding modes are:
+UNKNOWN           = -1
+ROUND_TO_NEAREST  = 0 | round result to nearest representable value
+ROUND_TO_ZERO     = 1 | round result towards zero
+ROUND_TO_PLUS     = 2 | round result towards plus infinity
+ROUND_TO_MINUS    = 3 | round result towards minus infinity
+
+| The allowed values of format are:
+NIL          = 0
+SINGLE_FLOAT = 1
+DOUBLE_FLOAT = 2
+LONG_FLOAT   = 3
+
+| The allowed values for the last operation are:
+NOOP         = 0
+ADD          = 1
+MULTIPLY     = 2
+DIVIDE       = 3
+NEGATE       = 4
+COMPARE      = 5
+EXTENDSFDF   = 6
+TRUNCDFSF    = 7
+
+|=============================================================================
+|                           __clear_sticky_bits
+|=============================================================================
+
+| The sticky bits are normally not cleared (thus the name), whereas the 
+| exception type and exception value reflect the last computation. 
+| This routine is provided to clear them (you can also write to _fpCCR,
+| since it is globally visible).
+
+	.globl  SYM (__clear_sticky_bit)
+
+	.text
+	.even
+
+| void __clear_sticky_bits(void);
+SYM (__clear_sticky_bit):		
+	PICLEA	SYM (_fpCCR),a0
+#ifndef __mcoldfire__
+	movew	IMM (0),a0@(STICK)
+#else
+	clr.w	a0@(STICK)
+#endif
+	rts
+
+|=============================================================================
+|                           $_exception_handler
+|=============================================================================
+
+	.globl  $_exception_handler
+
+	.text
+	.even
+
+| This is the common exit point if an exception occurs.
+| NOTE: it is NOT callable from C!
+| It expects the exception type in d7, the format (SINGLE_FLOAT,
+| DOUBLE_FLOAT or LONG_FLOAT) in d6, and the last operation code in d5.
+| It sets the corresponding exception and sticky bits, and the format. 
+| Depending on the format if fills the corresponding slots for the 
+| operands which produced the exception (all this information is provided
+| so if you write your own exception handlers you have enough information
+| to deal with the problem).
+| Then checks to see if the corresponding exception is trap-enabled, 
+| in which case it pushes the address of _fpCCR and traps through 
+| trap FPTRAP (15 for the moment).
+
+FPTRAP = 15
+
+$_exception_handler:
+	PICLEA	SYM (_fpCCR),a0
+	movew	d7,a0@(EBITS)	| set __exception_bits
+#ifndef __mcoldfire__
+	orw	d7,a0@(STICK)	| and __sticky_bits
+#else
+	movew	a0@(STICK),d4
+	orl	d7,d4
+	movew	d4,a0@(STICK)
+#endif
+	movew	d6,a0@(FORMT)	| and __format
+	movew	d5,a0@(LASTO)	| and __last_operation
+
+| Now put the operands in place:
+#ifndef __mcoldfire__
+	cmpw	IMM (SINGLE_FLOAT),d6
+#else
+	cmpl	IMM (SINGLE_FLOAT),d6
+#endif
+	beq	1f
+	movel	a6@(8),a0@(OPER1)
+	movel	a6@(12),a0@(OPER1+4)
+	movel	a6@(16),a0@(OPER2)
+	movel	a6@(20),a0@(OPER2+4)
+	bra	2f
+1:	movel	a6@(8),a0@(OPER1)
+	movel	a6@(12),a0@(OPER2)
+2:
+| And check whether the exception is trap-enabled:
+#ifndef __mcoldfire__
+	andw	a0@(TRAPE),d7	| is exception trap-enabled?
+#else
+	clrl	d6
+	movew	a0@(TRAPE),d6
+	andl	d6,d7
+#endif
+	beq	1f		| no, exit
+	PICPEA	SYM (_fpCCR),a1	| yes, push address of _fpCCR
+	trap	IMM (FPTRAP)	| and trap
+#ifndef __mcoldfire__
+1:	moveml	sp@+,d2-d7	| restore data registers
+#else
+1:	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| and return
+	rts
+#endif /* L_floatex */
+
+#ifdef  L_mulsi3
+	.text
+	FUNC(__mulsi3)
+	.globl	SYM (__mulsi3)
+SYM (__mulsi3):
+	movew	sp@(4), d0	/* x0 -> d0 */
+	muluw	sp@(10), d0	/* x0*y1 */
+	movew	sp@(6), d1	/* x1 -> d1 */
+	muluw	sp@(8), d1	/* x1*y0 */
+#ifndef __mcoldfire__
+	addw	d1, d0
+#else
+	addl	d1, d0
+#endif
+	swap	d0
+	clrw	d0
+	movew	sp@(6), d1	/* x1 -> d1 */
+	muluw	sp@(10), d1	/* x1*y1 */
+	addl	d1, d0
+
+	rts
+#endif /* L_mulsi3 */
+
+#ifdef  L_udivsi3
+	.text
+	FUNC(__udivsi3)
+	.globl	SYM (__udivsi3)
+SYM (__udivsi3):
+#ifndef __mcoldfire__
+	movel	d2, sp@-
+	movel	sp@(12), d1	/* d1 = divisor */
+	movel	sp@(8), d0	/* d0 = dividend */
+
+	cmpl	IMM (0x10000), d1 /* divisor >= 2 ^ 16 ?   */
+	jcc	L3		/* then try next algorithm */
+	movel	d0, d2
+	clrw	d2
+	swap	d2
+	divu	d1, d2          /* high quotient in lower word */
+	movew	d2, d0		/* save high quotient */
+	swap	d0
+	movew	sp@(10), d2	/* get low dividend + high rest */
+	divu	d1, d2		/* low quotient */
+	movew	d2, d0
+	jra	L6
+
+L3:	movel	d1, d2		/* use d2 as divisor backup */
+L4:	lsrl	IMM (1), d1	/* shift divisor */
+	lsrl	IMM (1), d0	/* shift dividend */
+	cmpl	IMM (0x10000), d1 /* still divisor >= 2 ^ 16 ?  */
+	jcc	L4
+	divu	d1, d0		/* now we have 16-bit divisor */
+	andl	IMM (0xffff), d0 /* mask out divisor, ignore remainder */
+
+/* Multiply the 16-bit tentative quotient with the 32-bit divisor.  Because of
+   the operand ranges, this might give a 33-bit product.  If this product is
+   greater than the dividend, the tentative quotient was too large. */
+	movel	d2, d1
+	mulu	d0, d1		/* low part, 32 bits */
+	swap	d2
+	mulu	d0, d2		/* high part, at most 17 bits */
+	swap	d2		/* align high part with low part */
+	tstw	d2		/* high part 17 bits? */
+	jne	L5		/* if 17 bits, quotient was too large */
+	addl	d2, d1		/* add parts */
+	jcs	L5		/* if sum is 33 bits, quotient was too large */
+	cmpl	sp@(8), d1	/* compare the sum with the dividend */
+	jls	L6		/* if sum > dividend, quotient was too large */
+L5:	subql	IMM (1), d0	/* adjust quotient */
+
+L6:	movel	sp@+, d2
+	rts
+
+#else /* __mcoldfire__ */
+
+/* ColdFire implementation of non-restoring division algorithm from
+   Hennessy & Patterson, Appendix A. */
+	link	a6,IMM (-12)
+	moveml	d2-d4,sp@
+	movel	a6@(8),d0
+	movel	a6@(12),d1
+	clrl	d2		| clear p
+	moveq	IMM (31),d4
+L1:	addl	d0,d0		| shift reg pair (p,a) one bit left
+	addxl	d2,d2
+	movl	d2,d3		| subtract b from p, store in tmp.
+	subl	d1,d3
+	jcs	L2		| if no carry,
+	bset	IMM (0),d0	| set the low order bit of a to 1,
+	movl	d3,d2		| and store tmp in p.
+L2:	subql	IMM (1),d4
+	jcc	L1
+	moveml	sp@,d2-d4	| restore data registers
+	unlk	a6		| and return
+	rts
+#endif /* __mcoldfire__ */
+
+#endif /* L_udivsi3 */
+
+#ifdef  L_divsi3
+	.text
+	FUNC(__divsi3)
+	.globl	SYM (__divsi3)
+SYM (__divsi3):
+	movel	d2, sp@-
+
+	moveq	IMM (1), d2	/* sign of result stored in d2 (=1 or =-1) */
+	movel	sp@(12), d1	/* d1 = divisor */
+	jpl	L1
+	negl	d1
+#ifndef __mcoldfire__
+	negb	d2		/* change sign because divisor <0  */
+#else
+	negl	d2		/* change sign because divisor <0  */
+#endif
+L1:	movel	sp@(8), d0	/* d0 = dividend */
+	jpl	L2
+	negl	d0
+#ifndef __mcoldfire__
+	negb	d2
+#else
+	negl	d2
+#endif
+
+L2:	movel	d1, sp@-
+	movel	d0, sp@-
+	PICCALL	SYM (__udivsi3)	/* divide abs(dividend) by abs(divisor) */
+	addql	IMM (8), sp
+
+	tstb	d2
+	jpl	L3
+	negl	d0
+
+L3:	movel	sp@+, d2
+	rts
+#endif /* L_divsi3 */
+
+#ifdef  L_umodsi3
+	.text
+	FUNC(__umodsi3)
+	.globl	SYM (__umodsi3)
+SYM (__umodsi3):
+	movel	sp@(8), d1	/* d1 = divisor */
+	movel	sp@(4), d0	/* d0 = dividend */
+	movel	d1, sp@-
+	movel	d0, sp@-
+	PICCALL	SYM (__udivsi3)
+	addql	IMM (8), sp
+	movel	sp@(8), d1	/* d1 = divisor */
+#ifndef __mcoldfire__
+	movel	d1, sp@-
+	movel	d0, sp@-
+	PICCALL	SYM (__mulsi3)	/* d0 = (a/b)*b */
+	addql	IMM (8), sp
+#else
+	mulsl	d1,d0
+#endif
+	movel	sp@(4), d1	/* d1 = dividend */
+	subl	d0, d1		/* d1 = a - (a/b)*b */
+	movel	d1, d0
+	rts
+#endif /* L_umodsi3 */
+
+#ifdef  L_modsi3
+	.text
+	FUNC(__modsi3)
+	.globl	SYM (__modsi3)
+SYM (__modsi3):
+	movel	sp@(8), d1	/* d1 = divisor */
+	movel	sp@(4), d0	/* d0 = dividend */
+	movel	d1, sp@-
+	movel	d0, sp@-
+	PICCALL	SYM (__divsi3)
+	addql	IMM (8), sp
+	movel	sp@(8), d1	/* d1 = divisor */
+#ifndef __mcoldfire__
+	movel	d1, sp@-
+	movel	d0, sp@-
+	PICCALL	SYM (__mulsi3)	/* d0 = (a/b)*b */
+	addql	IMM (8), sp
+#else
+	mulsl	d1,d0
+#endif
+	movel	sp@(4), d1	/* d1 = dividend */
+	subl	d0, d1		/* d1 = a - (a/b)*b */
+	movel	d1, d0
+	rts
+#endif /* L_modsi3 */
+
+
+#ifdef  L_double
+
+	.globl	SYM (_fpCCR)
+	.globl  $_exception_handler
+
+QUIET_NaN      = 0xffffffff
+
+D_MAX_EXP      = 0x07ff
+D_BIAS         = 1022
+DBL_MAX_EXP    = D_MAX_EXP - D_BIAS
+DBL_MIN_EXP    = 1 - D_BIAS
+DBL_MANT_DIG   = 53
+
+INEXACT_RESULT 		= 0x0001
+UNDERFLOW 		= 0x0002
+OVERFLOW 		= 0x0004
+DIVIDE_BY_ZERO 		= 0x0008
+INVALID_OPERATION 	= 0x0010
+
+DOUBLE_FLOAT = 2
+
+NOOP         = 0
+ADD          = 1
+MULTIPLY     = 2
+DIVIDE       = 3
+NEGATE       = 4
+COMPARE      = 5
+EXTENDSFDF   = 6
+TRUNCDFSF    = 7
+
+UNKNOWN           = -1
+ROUND_TO_NEAREST  = 0 | round result to nearest representable value
+ROUND_TO_ZERO     = 1 | round result towards zero
+ROUND_TO_PLUS     = 2 | round result towards plus infinity
+ROUND_TO_MINUS    = 3 | round result towards minus infinity
+
+| Entry points:
+
+	.globl SYM (__adddf3)
+	.globl SYM (__subdf3)
+	.globl SYM (__muldf3)
+	.globl SYM (__divdf3)
+	.globl SYM (__negdf2)
+	.globl SYM (__cmpdf2)
+	.globl SYM (__cmpdf2_internal)
+#ifdef __ELF__
+	.hidden SYM (__cmpdf2_internal)
+#endif
+
+	.text
+	.even
+
+| These are common routines to return and signal exceptions.	
+
+Ld$den:
+| Return and signal a denormalized number
+	orl	d7,d0
+	movew	IMM (INEXACT_RESULT+UNDERFLOW),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Ld$infty:
+Ld$overflow:
+| Return a properly signed INFINITY and set the exception flags 
+	movel	IMM (0x7ff00000),d0
+	movel	IMM (0),d1
+	orl	d7,d0
+	movew	IMM (INEXACT_RESULT+OVERFLOW),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Ld$underflow:
+| Return 0 and set the exception flags 
+	movel	IMM (0),d0
+	movel	d0,d1
+	movew	IMM (INEXACT_RESULT+UNDERFLOW),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Ld$inop:
+| Return a quiet NaN and set the exception flags
+	movel	IMM (QUIET_NaN),d0
+	movel	d0,d1
+	movew	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Ld$div$0:
+| Return a properly signed INFINITY and set the exception flags
+	movel	IMM (0x7ff00000),d0
+	movel	IMM (0),d1
+	orl	d7,d0
+	movew	IMM (INEXACT_RESULT+DIVIDE_BY_ZERO),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+|=============================================================================
+|=============================================================================
+|                         double precision routines
+|=============================================================================
+|=============================================================================
+
+| A double precision floating point number (double) has the format:
+|
+| struct _double {
+|  unsigned int sign      : 1;  /* sign bit */ 
+|  unsigned int exponent  : 11; /* exponent, shifted by 126 */
+|  unsigned int fraction  : 52; /* fraction */
+| } double;
+| 
+| Thus sizeof(double) = 8 (64 bits). 
+|
+| All the routines are callable from C programs, and return the result 
+| in the register pair d0-d1. They also preserve all registers except 
+| d0-d1 and a0-a1.
+
+|=============================================================================
+|                              __subdf3
+|=============================================================================
+
+| double __subdf3(double, double);
+	FUNC(__subdf3)
+SYM (__subdf3):
+	bchg	IMM (31),sp@(12) | change sign of second operand
+				| and fall through, so we always add
+|=============================================================================
+|                              __adddf3
+|=============================================================================
+
+| double __adddf3(double, double);
+	FUNC(__adddf3)
+SYM (__adddf3):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)	| everything will be done in registers
+	moveml	d2-d7,sp@-	| save all data registers and a2 (but d0-d1)
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	movel	a6@(8),d0	| get first operand
+	movel	a6@(12),d1	| 
+	movel	a6@(16),d2	| get second operand
+	movel	a6@(20),d3	| 
+
+	movel	d0,d7		| get d0's sign bit in d7 '
+	addl	d1,d1		| check and clear sign bit of a, and gain one
+	addxl	d0,d0		| bit of extra precision
+	beq	Ladddf$b	| if zero return second operand
+
+	movel	d2,d6		| save sign in d6 
+	addl	d3,d3		| get rid of sign bit and gain one bit of
+	addxl	d2,d2		| extra precision
+	beq	Ladddf$a	| if zero return first operand
+
+	andl	IMM (0x80000000),d7 | isolate a's sign bit '
+        swap	d6		| and also b's sign bit '
+#ifndef __mcoldfire__
+	andw	IMM (0x8000),d6	|
+	orw	d6,d7		| and combine them into d7, so that a's sign '
+				| bit is in the high word and b's is in the '
+				| low word, so d6 is free to be used
+#else
+	andl	IMM (0x8000),d6
+	orl	d6,d7
+#endif
+	movel	d7,a0		| now save d7 into a0, so d7 is free to
+                		| be used also
+
+| Get the exponents and check for denormalized and/or infinity.
+
+	movel	IMM (0x001fffff),d6 | mask for the fraction
+	movel	IMM (0x00200000),d7 | mask to put hidden bit back
+
+	movel	d0,d4		| 
+	andl	d6,d0		| get fraction in d0
+	notl	d6		| make d6 into mask for the exponent
+	andl	d6,d4		| get exponent in d4
+	beq	Ladddf$a$den	| branch if a is denormalized
+	cmpl	d6,d4		| check for INFINITY or NaN
+	beq	Ladddf$nf       | 
+	orl	d7,d0		| and put hidden bit back
+Ladddf$1:
+	swap	d4		| shift right exponent so that it starts
+#ifndef __mcoldfire__
+	lsrw	IMM (5),d4	| in bit 0 and not bit 20
+#else
+	lsrl	IMM (5),d4	| in bit 0 and not bit 20
+#endif
+| Now we have a's exponent in d4 and fraction in d0-d1 '
+	movel	d2,d5		| save b to get exponent
+	andl	d6,d5		| get exponent in d5
+	beq	Ladddf$b$den	| branch if b is denormalized
+	cmpl	d6,d5		| check for INFINITY or NaN
+	beq	Ladddf$nf
+	notl	d6		| make d6 into mask for the fraction again
+	andl	d6,d2		| and get fraction in d2
+	orl	d7,d2		| and put hidden bit back
+Ladddf$2:
+	swap	d5		| shift right exponent so that it starts
+#ifndef __mcoldfire__
+	lsrw	IMM (5),d5	| in bit 0 and not bit 20
+#else
+	lsrl	IMM (5),d5	| in bit 0 and not bit 20
+#endif
+
+| Now we have b's exponent in d5 and fraction in d2-d3. '
+
+| The situation now is as follows: the signs are combined in a0, the 
+| numbers are in d0-d1 (a) and d2-d3 (b), and the exponents in d4 (a)
+| and d5 (b). To do the rounding correctly we need to keep all the
+| bits until the end, so we need to use d0-d1-d2-d3 for the first number
+| and d4-d5-d6-d7 for the second. To do this we store (temporarily) the
+| exponents in a2-a3.
+
+#ifndef __mcoldfire__
+	moveml	a2-a3,sp@-	| save the address registers
+#else
+	movel	a2,sp@-	
+	movel	a3,sp@-	
+	movel	a4,sp@-	
+#endif
+
+	movel	d4,a2		| save the exponents
+	movel	d5,a3		| 
+
+	movel	IMM (0),d7	| and move the numbers around
+	movel	d7,d6		|
+	movel	d3,d5		|
+	movel	d2,d4		|
+	movel	d7,d3		|
+	movel	d7,d2		|
+
+| Here we shift the numbers until the exponents are the same, and put 
+| the largest exponent in a2.
+#ifndef __mcoldfire__
+	exg	d4,a2		| get exponents back
+	exg	d5,a3		|
+	cmpw	d4,d5		| compare the exponents
+#else
+	movel	d4,a4		| get exponents back
+	movel	a2,d4
+	movel	a4,a2
+	movel	d5,a4
+	movel	a3,d5
+	movel	a4,a3
+	cmpl	d4,d5		| compare the exponents
+#endif
+	beq	Ladddf$3	| if equal don't shift '
+	bhi	9f		| branch if second exponent is higher
+
+| Here we have a's exponent larger than b's, so we have to shift b. We do 
+| this by using as counter d2:
+1:	movew	d4,d2		| move largest exponent to d2
+#ifndef __mcoldfire__
+	subw	d5,d2		| and subtract second exponent
+	exg	d4,a2		| get back the longs we saved
+	exg	d5,a3		|
+#else
+	subl	d5,d2		| and subtract second exponent
+	movel	d4,a4		| get back the longs we saved
+	movel	a2,d4
+	movel	a4,a2
+	movel	d5,a4
+	movel	a3,d5
+	movel	a4,a3
+#endif
+| if difference is too large we don't shift (actually, we can just exit) '
+#ifndef __mcoldfire__
+	cmpw	IMM (DBL_MANT_DIG+2),d2
+#else
+	cmpl	IMM (DBL_MANT_DIG+2),d2
+#endif
+	bge	Ladddf$b$small
+#ifndef __mcoldfire__
+	cmpw	IMM (32),d2	| if difference >= 32, shift by longs
+#else
+	cmpl	IMM (32),d2	| if difference >= 32, shift by longs
+#endif
+	bge	5f
+2:
+#ifndef __mcoldfire__
+	cmpw	IMM (16),d2	| if difference >= 16, shift by words	
+#else
+	cmpl	IMM (16),d2	| if difference >= 16, shift by words	
+#endif
+	bge	6f
+	bra	3f		| enter dbra loop
+
+4:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d4
+	roxrl	IMM (1),d5
+	roxrl	IMM (1),d6
+	roxrl	IMM (1),d7
+#else
+	lsrl	IMM (1),d7
+	btst	IMM (0),d6
+	beq	10f
+	bset	IMM (31),d7
+10:	lsrl	IMM (1),d6
+	btst	IMM (0),d5
+	beq	11f
+	bset	IMM (31),d6
+11:	lsrl	IMM (1),d5
+	btst	IMM (0),d4
+	beq	12f
+	bset	IMM (31),d5
+12:	lsrl	IMM (1),d4
+#endif
+3:
+#ifndef __mcoldfire__
+	dbra	d2,4b
+#else
+	subql	IMM (1),d2
+	bpl	4b	
+#endif
+	movel	IMM (0),d2
+	movel	d2,d3	
+	bra	Ladddf$4
+5:
+	movel	d6,d7
+	movel	d5,d6
+	movel	d4,d5
+	movel	IMM (0),d4
+#ifndef __mcoldfire__
+	subw	IMM (32),d2
+#else
+	subl	IMM (32),d2
+#endif
+	bra	2b
+6:
+	movew	d6,d7
+	swap	d7
+	movew	d5,d6
+	swap	d6
+	movew	d4,d5
+	swap	d5
+	movew	IMM (0),d4
+	swap	d4
+#ifndef __mcoldfire__
+	subw	IMM (16),d2
+#else
+	subl	IMM (16),d2
+#endif
+	bra	3b
+	
+9:
+#ifndef __mcoldfire__
+	exg	d4,d5
+	movew	d4,d6
+	subw	d5,d6		| keep d5 (largest exponent) in d4
+	exg	d4,a2
+	exg	d5,a3
+#else
+	movel	d5,d6
+	movel	d4,d5
+	movel	d6,d4
+	subl	d5,d6
+	movel	d4,a4
+	movel	a2,d4
+	movel	a4,a2
+	movel	d5,a4
+	movel	a3,d5
+	movel	a4,a3
+#endif
+| if difference is too large we don't shift (actually, we can just exit) '
+#ifndef __mcoldfire__
+	cmpw	IMM (DBL_MANT_DIG+2),d6
+#else
+	cmpl	IMM (DBL_MANT_DIG+2),d6
+#endif
+	bge	Ladddf$a$small
+#ifndef __mcoldfire__
+	cmpw	IMM (32),d6	| if difference >= 32, shift by longs
+#else
+	cmpl	IMM (32),d6	| if difference >= 32, shift by longs
+#endif
+	bge	5f
+2:
+#ifndef __mcoldfire__
+	cmpw	IMM (16),d6	| if difference >= 16, shift by words	
+#else
+	cmpl	IMM (16),d6	| if difference >= 16, shift by words	
+#endif
+	bge	6f
+	bra	3f		| enter dbra loop
+
+4:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+#endif
+3:
+#ifndef __mcoldfire__
+	dbra	d6,4b
+#else
+	subql	IMM (1),d6
+	bpl	4b
+#endif
+	movel	IMM (0),d7
+	movel	d7,d6
+	bra	Ladddf$4
+5:
+	movel	d2,d3
+	movel	d1,d2
+	movel	d0,d1
+	movel	IMM (0),d0
+#ifndef __mcoldfire__
+	subw	IMM (32),d6
+#else
+	subl	IMM (32),d6
+#endif
+	bra	2b
+6:
+	movew	d2,d3
+	swap	d3
+	movew	d1,d2
+	swap	d2
+	movew	d0,d1
+	swap	d1
+	movew	IMM (0),d0
+	swap	d0
+#ifndef __mcoldfire__
+	subw	IMM (16),d6
+#else
+	subl	IMM (16),d6
+#endif
+	bra	3b
+Ladddf$3:
+#ifndef __mcoldfire__
+	exg	d4,a2	
+	exg	d5,a3
+#else
+	movel	d4,a4
+	movel	a2,d4
+	movel	a4,a2
+	movel	d5,a4
+	movel	a3,d5
+	movel	a4,a3
+#endif
+Ladddf$4:	
+| Now we have the numbers in d0--d3 and d4--d7, the exponent in a2, and
+| the signs in a4.
+
+| Here we have to decide whether to add or subtract the numbers:
+#ifndef __mcoldfire__
+	exg	d7,a0		| get the signs 
+	exg	d6,a3		| a3 is free to be used
+#else
+	movel	d7,a4
+	movel	a0,d7
+	movel	a4,a0
+	movel	d6,a4
+	movel	a3,d6
+	movel	a4,a3
+#endif
+	movel	d7,d6		|
+	movew	IMM (0),d7	| get a's sign in d7 '
+	swap	d6              |
+	movew	IMM (0),d6	| and b's sign in d6 '
+	eorl	d7,d6		| compare the signs
+	bmi	Lsubdf$0	| if the signs are different we have 
+				| to subtract
+#ifndef __mcoldfire__
+	exg	d7,a0		| else we add the numbers
+	exg	d6,a3		|
+#else
+	movel	d7,a4
+	movel	a0,d7
+	movel	a4,a0
+	movel	d6,a4
+	movel	a3,d6
+	movel	a4,a3
+#endif
+	addl	d7,d3		|
+	addxl	d6,d2		|
+	addxl	d5,d1		| 
+	addxl	d4,d0           |
+
+	movel	a2,d4		| return exponent to d4
+	movel	a0,d7		| 
+	andl	IMM (0x80000000),d7 | d7 now has the sign
+
+#ifndef __mcoldfire__
+	moveml	sp@+,a2-a3	
+#else
+	movel	sp@+,a4	
+	movel	sp@+,a3	
+	movel	sp@+,a2	
+#endif
+
+| Before rounding normalize so bit #DBL_MANT_DIG is set (we will consider
+| the case of denormalized numbers in the rounding routine itself).
+| As in the addition (not in the subtraction!) we could have set 
+| one more bit we check this:
+	btst	IMM (DBL_MANT_DIG+1),d0	
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+1:
+	lea	pc@(Ladddf$5),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Ladddf$5:
+| Put back the exponent and check for overflow
+#ifndef __mcoldfire__
+	cmpw	IMM (0x7ff),d4	| is the exponent big?
+#else
+	cmpl	IMM (0x7ff),d4	| is the exponent big?
+#endif
+	bge	1f
+	bclr	IMM (DBL_MANT_DIG-1),d0
+#ifndef __mcoldfire__
+	lslw	IMM (4),d4	| put exponent back into position
+#else
+	lsll	IMM (4),d4	| put exponent back into position
+#endif
+	swap	d0		| 
+#ifndef __mcoldfire__
+	orw	d4,d0		|
+#else
+	orl	d4,d0		|
+#endif
+	swap	d0		|
+	bra	Ladddf$ret
+1:
+	moveq	IMM (ADD),d5
+	bra	Ld$overflow
+
+Lsubdf$0:
+| Here we do the subtraction.
+#ifndef __mcoldfire__
+	exg	d7,a0		| put sign back in a0
+	exg	d6,a3		|
+#else
+	movel	d7,a4
+	movel	a0,d7
+	movel	a4,a0
+	movel	d6,a4
+	movel	a3,d6
+	movel	a4,a3
+#endif
+	subl	d7,d3		|
+	subxl	d6,d2		|
+	subxl	d5,d1		|
+	subxl	d4,d0		|
+	beq	Ladddf$ret$1	| if zero just exit
+	bpl	1f		| if positive skip the following
+	movel	a0,d7		|
+	bchg	IMM (31),d7	| change sign bit in d7
+	movel	d7,a0		|
+	negl	d3		|
+	negxl	d2		|
+	negxl	d1              | and negate result
+	negxl	d0              |
+1:	
+	movel	a2,d4		| return exponent to d4
+	movel	a0,d7
+	andl	IMM (0x80000000),d7 | isolate sign bit
+#ifndef __mcoldfire__
+	moveml	sp@+,a2-a3	|
+#else
+	movel	sp@+,a4
+	movel	sp@+,a3
+	movel	sp@+,a2
+#endif
+
+| Before rounding normalize so bit #DBL_MANT_DIG is set (we will consider
+| the case of denormalized numbers in the rounding routine itself).
+| As in the addition (not in the subtraction!) we could have set 
+| one more bit we check this:
+	btst	IMM (DBL_MANT_DIG+1),d0	
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+1:
+	lea	pc@(Lsubdf$1),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Lsubdf$1:
+| Put back the exponent and sign (we don't have overflow). '
+	bclr	IMM (DBL_MANT_DIG-1),d0	
+#ifndef __mcoldfire__
+	lslw	IMM (4),d4	| put exponent back into position
+#else
+	lsll	IMM (4),d4	| put exponent back into position
+#endif
+	swap	d0		| 
+#ifndef __mcoldfire__
+	orw	d4,d0		|
+#else
+	orl	d4,d0		|
+#endif
+	swap	d0		|
+	bra	Ladddf$ret
+
+| If one of the numbers was too small (difference of exponents >= 
+| DBL_MANT_DIG+1) we return the other (and now we don't have to '
+| check for finiteness or zero).
+Ladddf$a$small:
+#ifndef __mcoldfire__
+	moveml	sp@+,a2-a3	
+#else
+	movel	sp@+,a4
+	movel	sp@+,a3
+	movel	sp@+,a2
+#endif
+	movel	a6@(16),d0
+	movel	a6@(20),d1
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7	| restore data registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| and return
+	rts
+
+Ladddf$b$small:
+#ifndef __mcoldfire__
+	moveml	sp@+,a2-a3	
+#else
+	movel	sp@+,a4	
+	movel	sp@+,a3	
+	movel	sp@+,a2	
+#endif
+	movel	a6@(8),d0
+	movel	a6@(12),d1
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7	| restore data registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| and return
+	rts
+
+Ladddf$a$den:
+	movel	d7,d4		| d7 contains 0x00200000
+	bra	Ladddf$1
+
+Ladddf$b$den:
+	movel	d7,d5           | d7 contains 0x00200000
+	notl	d6
+	bra	Ladddf$2
+
+Ladddf$b:
+| Return b (if a is zero)
+	movel	d2,d0
+	movel	d3,d1
+	bne	1f			| Check if b is -0
+	cmpl	IMM (0x80000000),d0
+	bne	1f
+	andl	IMM (0x80000000),d7	| Use the sign of a
+	clrl	d0
+	bra	Ladddf$ret
+Ladddf$a:
+	movel	a6@(8),d0
+	movel	a6@(12),d1
+1:
+	moveq	IMM (ADD),d5
+| Check for NaN and +/-INFINITY.
+	movel	d0,d7         		|
+	andl	IMM (0x80000000),d7	|
+	bclr	IMM (31),d0		|
+	cmpl	IMM (0x7ff00000),d0	|
+	bge	2f			|
+	movel	d0,d0           	| check for zero, since we don't  '
+	bne	Ladddf$ret		| want to return -0 by mistake
+	bclr	IMM (31),d7		|
+	bra	Ladddf$ret		|
+2:
+	andl	IMM (0x000fffff),d0	| check for NaN (nonzero fraction)
+	orl	d1,d0			|
+	bne	Ld$inop         	|
+	bra	Ld$infty		|
+	
+Ladddf$ret$1:
+#ifndef __mcoldfire__
+	moveml	sp@+,a2-a3	| restore regs and exit
+#else
+	movel	sp@+,a4
+	movel	sp@+,a3
+	movel	sp@+,a2
+#endif
+
+Ladddf$ret:
+| Normal exit.
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+	orl	d7,d0		| put sign bit back
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+Ladddf$ret$den:
+| Return a denormalized number.
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0	| shift right once more
+	roxrl	IMM (1),d1	|
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+#endif
+	bra	Ladddf$ret
+
+Ladddf$nf:
+	moveq	IMM (ADD),d5
+| This could be faster but it is not worth the effort, since it is not
+| executed very often. We sacrifice speed for clarity here.
+	movel	a6@(8),d0	| get the numbers back (remember that we
+	movel	a6@(12),d1	| did some processing already)
+	movel	a6@(16),d2	| 
+	movel	a6@(20),d3	| 
+	movel	IMM (0x7ff00000),d4 | useful constant (INFINITY)
+	movel	d0,d7		| save sign bits
+	movel	d2,d6		| 
+	bclr	IMM (31),d0	| clear sign bits
+	bclr	IMM (31),d2	| 
+| We know that one of them is either NaN of +/-INFINITY
+| Check for NaN (if either one is NaN return NaN)
+	cmpl	d4,d0		| check first a (d0)
+	bhi	Ld$inop		| if d0 > 0x7ff00000 or equal and
+	bne	2f
+	tstl	d1		| d1 > 0, a is NaN
+	bne	Ld$inop		| 
+2:	cmpl	d4,d2		| check now b (d1)
+	bhi	Ld$inop		| 
+	bne	3f
+	tstl	d3		| 
+	bne	Ld$inop		| 
+3:
+| Now comes the check for +/-INFINITY. We know that both are (maybe not
+| finite) numbers, but we have to check if both are infinite whether we
+| are adding or subtracting them.
+	eorl	d7,d6		| to check sign bits
+	bmi	1f
+	andl	IMM (0x80000000),d7 | get (common) sign bit
+	bra	Ld$infty
+1:
+| We know one (or both) are infinite, so we test for equality between the
+| two numbers (if they are equal they have to be infinite both, so we
+| return NaN).
+	cmpl	d2,d0		| are both infinite?
+	bne	1f		| if d0 <> d2 they are not equal
+	cmpl	d3,d1		| if d0 == d2 test d3 and d1
+	beq	Ld$inop		| if equal return NaN
+1:	
+	andl	IMM (0x80000000),d7 | get a's sign bit '
+	cmpl	d4,d0		| test now for infinity
+	beq	Ld$infty	| if a is INFINITY return with this sign
+	bchg	IMM (31),d7	| else we know b is INFINITY and has
+	bra	Ld$infty	| the opposite sign
+
+|=============================================================================
+|                              __muldf3
+|=============================================================================
+
+| double __muldf3(double, double);
+	FUNC(__muldf3)
+SYM (__muldf3):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d2-d7,sp@-
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	movel	a6@(8),d0		| get a into d0-d1
+	movel	a6@(12),d1		| 
+	movel	a6@(16),d2		| and b into d2-d3
+	movel	a6@(20),d3		|
+	movel	d0,d7			| d7 will hold the sign of the product
+	eorl	d2,d7			|
+	andl	IMM (0x80000000),d7	|
+	movel	d7,a0			| save sign bit into a0 
+	movel	IMM (0x7ff00000),d7	| useful constant (+INFINITY)
+	movel	d7,d6			| another (mask for fraction)
+	notl	d6			|
+	bclr	IMM (31),d0		| get rid of a's sign bit '
+	movel	d0,d4			| 
+	orl	d1,d4			| 
+	beq	Lmuldf$a$0		| branch if a is zero
+	movel	d0,d4			|
+	bclr	IMM (31),d2		| get rid of b's sign bit '
+	movel	d2,d5			|
+	orl	d3,d5			| 
+	beq	Lmuldf$b$0		| branch if b is zero
+	movel	d2,d5			| 
+	cmpl	d7,d0			| is a big?
+	bhi	Lmuldf$inop		| if a is NaN return NaN
+	beq	Lmuldf$a$nf		| we still have to check d1 and b ...
+	cmpl	d7,d2			| now compare b with INFINITY
+	bhi	Lmuldf$inop		| is b NaN?
+	beq	Lmuldf$b$nf 		| we still have to check d3 ...
+| Here we have both numbers finite and nonzero (and with no sign bit).
+| Now we get the exponents into d4 and d5.
+	andl	d7,d4			| isolate exponent in d4
+	beq	Lmuldf$a$den		| if exponent zero, have denormalized
+	andl	d6,d0			| isolate fraction
+	orl	IMM (0x00100000),d0	| and put hidden bit back
+	swap	d4			| I like exponents in the first byte
+#ifndef __mcoldfire__
+	lsrw	IMM (4),d4		| 
+#else
+	lsrl	IMM (4),d4		| 
+#endif
+Lmuldf$1:			
+	andl	d7,d5			|
+	beq	Lmuldf$b$den		|
+	andl	d6,d2			|
+	orl	IMM (0x00100000),d2	| and put hidden bit back
+	swap	d5			|
+#ifndef __mcoldfire__
+	lsrw	IMM (4),d5		|
+#else
+	lsrl	IMM (4),d5		|
+#endif
+Lmuldf$2:				|
+#ifndef __mcoldfire__
+	addw	d5,d4			| add exponents
+	subw	IMM (D_BIAS+1),d4	| and subtract bias (plus one)
+#else
+	addl	d5,d4			| add exponents
+	subl	IMM (D_BIAS+1),d4	| and subtract bias (plus one)
+#endif
+
+| We are now ready to do the multiplication. The situation is as follows:
+| both a and b have bit 52 ( bit 20 of d0 and d2) set (even if they were 
+| denormalized to start with!), which means that in the product bit 104 
+| (which will correspond to bit 8 of the fourth long) is set.
+
+| Here we have to do the product.
+| To do it we have to juggle the registers back and forth, as there are not
+| enough to keep everything in them. So we use the address registers to keep
+| some intermediate data.
+
+#ifndef __mcoldfire__
+	moveml	a2-a3,sp@-	| save a2 and a3 for temporary use
+#else
+	movel	a2,sp@-
+	movel	a3,sp@-
+	movel	a4,sp@-
+#endif
+	movel	IMM (0),a2	| a2 is a null register
+	movel	d4,a3		| and a3 will preserve the exponent
+
+| First, shift d2-d3 so bit 20 becomes bit 31:
+#ifndef __mcoldfire__
+	rorl	IMM (5),d2	| rotate d2 5 places right
+	swap	d2		| and swap it
+	rorl	IMM (5),d3	| do the same thing with d3
+	swap	d3		|
+	movew	d3,d6		| get the rightmost 11 bits of d3
+	andw	IMM (0x07ff),d6	|
+	orw	d6,d2		| and put them into d2
+	andw	IMM (0xf800),d3	| clear those bits in d3
+#else
+	moveq	IMM (11),d7	| left shift d2 11 bits
+	lsll	d7,d2
+	movel	d3,d6		| get a copy of d3
+	lsll	d7,d3		| left shift d3 11 bits
+	andl	IMM (0xffe00000),d6 | get the top 11 bits of d3
+	moveq	IMM (21),d7	| right shift them 21 bits
+	lsrl	d7,d6
+	orl	d6,d2		| stick them at the end of d2
+#endif
+
+	movel	d2,d6		| move b into d6-d7
+	movel	d3,d7           | move a into d4-d5
+	movel	d0,d4           | and clear d0-d1-d2-d3 (to put result)
+	movel	d1,d5           |
+	movel	IMM (0),d3	|
+	movel	d3,d2           |
+	movel	d3,d1           |
+	movel	d3,d0	        |
+
+| We use a1 as counter:	
+	movel	IMM (DBL_MANT_DIG-1),a1		
+#ifndef __mcoldfire__
+	exg	d7,a1
+#else
+	movel	d7,a4
+	movel	a1,d7
+	movel	a4,a1
+#endif
+
+1:
+#ifndef __mcoldfire__
+	exg	d7,a1		| put counter back in a1
+#else
+	movel	d7,a4
+	movel	a1,d7
+	movel	a4,a1
+#endif
+	addl	d3,d3		| shift sum once left
+	addxl	d2,d2           |
+	addxl	d1,d1           |
+	addxl	d0,d0           |
+	addl	d7,d7		|
+	addxl	d6,d6		|
+	bcc	2f		| if bit clear skip the following
+#ifndef __mcoldfire__
+	exg	d7,a2		|
+#else
+	movel	d7,a4
+	movel	a2,d7
+	movel	a4,a2
+#endif
+	addl	d5,d3		| else add a to the sum
+	addxl	d4,d2		|
+	addxl	d7,d1		|
+	addxl	d7,d0		|
+#ifndef __mcoldfire__
+	exg	d7,a2		| 
+#else
+	movel	d7,a4
+	movel	a2,d7
+	movel	a4,a2
+#endif
+2:
+#ifndef __mcoldfire__
+	exg	d7,a1		| put counter in d7
+	dbf	d7,1b		| decrement and branch
+#else
+	movel	d7,a4
+	movel	a1,d7
+	movel	a4,a1
+	subql	IMM (1),d7
+	bpl	1b
+#endif
+
+	movel	a3,d4		| restore exponent
+#ifndef __mcoldfire__
+	moveml	sp@+,a2-a3
+#else
+	movel	sp@+,a4
+	movel	sp@+,a3
+	movel	sp@+,a2
+#endif
+
+| Now we have the product in d0-d1-d2-d3, with bit 8 of d0 set. The 
+| first thing to do now is to normalize it so bit 8 becomes bit 
+| DBL_MANT_DIG-32 (to do the rounding); later we will shift right.
+	swap	d0
+	swap	d1
+	movew	d1,d0
+	swap	d2
+	movew	d2,d1
+	swap	d3
+	movew	d3,d2
+	movew	IMM (0),d3
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+#else
+	moveq	IMM (29),d6
+	lsrl	IMM (3),d3
+	movel	d2,d7
+	lsll	d6,d7
+	orl	d7,d3
+	lsrl	IMM (3),d2
+	movel	d1,d7
+	lsll	d6,d7
+	orl	d7,d2
+	lsrl	IMM (3),d1
+	movel	d0,d7
+	lsll	d6,d7
+	orl	d7,d1
+	lsrl	IMM (3),d0
+#endif
+	
+| Now round, check for over- and underflow, and exit.
+	movel	a0,d7		| get sign bit back into d7
+	moveq	IMM (MULTIPLY),d5
+
+	btst	IMM (DBL_MANT_DIG+1-32),d0
+	beq	Lround$exit
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+	bra	Lround$exit
+
+Lmuldf$inop:
+	moveq	IMM (MULTIPLY),d5
+	bra	Ld$inop
+
+Lmuldf$b$nf:
+	moveq	IMM (MULTIPLY),d5
+	movel	a0,d7		| get sign bit back into d7
+	tstl	d3		| we know d2 == 0x7ff00000, so check d3
+	bne	Ld$inop		| if d3 <> 0 b is NaN
+	bra	Ld$overflow	| else we have overflow (since a is finite)
+
+Lmuldf$a$nf:
+	moveq	IMM (MULTIPLY),d5
+	movel	a0,d7		| get sign bit back into d7
+	tstl	d1		| we know d0 == 0x7ff00000, so check d1
+	bne	Ld$inop		| if d1 <> 0 a is NaN
+	bra	Ld$overflow	| else signal overflow
+
+| If either number is zero return zero, unless the other is +/-INFINITY or
+| NaN, in which case we return NaN.
+Lmuldf$b$0:
+	moveq	IMM (MULTIPLY),d5
+#ifndef __mcoldfire__
+	exg	d2,d0		| put b (==0) into d0-d1
+	exg	d3,d1		| and a (with sign bit cleared) into d2-d3
+	movel	a0,d0		| set result sign
+#else
+	movel	d0,d2		| put a into d2-d3
+	movel	d1,d3
+	movel	a0,d0		| put result zero into d0-d1
+	movq	IMM(0),d1
+#endif
+	bra	1f
+Lmuldf$a$0:
+	movel	a0,d0		| set result sign
+	movel	a6@(16),d2	| put b into d2-d3 again
+	movel	a6@(20),d3	|
+	bclr	IMM (31),d2	| clear sign bit
+1:	cmpl	IMM (0x7ff00000),d2 | check for non-finiteness
+	bge	Ld$inop		| in case NaN or +/-INFINITY return NaN
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+| If a number is denormalized we put an exponent of 1 but do not put the 
+| hidden bit back into the fraction; instead we shift left until bit 21
+| (the hidden bit) is set, adjusting the exponent accordingly. We do this
+| to ensure that the product of the fractions is close to 1.
+Lmuldf$a$den:
+	movel	IMM (1),d4
+	andl	d6,d0
+1:	addl	d1,d1           | shift a left until bit 20 is set
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	subw	IMM (1),d4	| and adjust exponent
+#else
+	subl	IMM (1),d4	| and adjust exponent
+#endif
+	btst	IMM (20),d0	|
+	bne	Lmuldf$1        |
+	bra	1b
+
+Lmuldf$b$den:
+	movel	IMM (1),d5
+	andl	d6,d2
+1:	addl	d3,d3		| shift b left until bit 20 is set
+	addxl	d2,d2		|
+#ifndef __mcoldfire__
+	subw	IMM (1),d5	| and adjust exponent
+#else
+	subql	IMM (1),d5	| and adjust exponent
+#endif
+	btst	IMM (20),d2	|
+	bne	Lmuldf$2	|
+	bra	1b
+
+
+|=============================================================================
+|                              __divdf3
+|=============================================================================
+
+| double __divdf3(double, double);
+	FUNC(__divdf3)
+SYM (__divdf3):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d2-d7,sp@-
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	movel	a6@(8),d0	| get a into d0-d1
+	movel	a6@(12),d1	| 
+	movel	a6@(16),d2	| and b into d2-d3
+	movel	a6@(20),d3	|
+	movel	d0,d7		| d7 will hold the sign of the result
+	eorl	d2,d7		|
+	andl	IMM (0x80000000),d7
+	movel	d7,a0		| save sign into a0
+	movel	IMM (0x7ff00000),d7 | useful constant (+INFINITY)
+	movel	d7,d6		| another (mask for fraction)
+	notl	d6		|
+	bclr	IMM (31),d0	| get rid of a's sign bit '
+	movel	d0,d4		|
+	orl	d1,d4		|
+	beq	Ldivdf$a$0	| branch if a is zero
+	movel	d0,d4		|
+	bclr	IMM (31),d2	| get rid of b's sign bit '
+	movel	d2,d5		|
+	orl	d3,d5		|
+	beq	Ldivdf$b$0	| branch if b is zero
+	movel	d2,d5
+	cmpl	d7,d0		| is a big?
+	bhi	Ldivdf$inop	| if a is NaN return NaN
+	beq	Ldivdf$a$nf	| if d0 == 0x7ff00000 we check d1
+	cmpl	d7,d2		| now compare b with INFINITY 
+	bhi	Ldivdf$inop	| if b is NaN return NaN
+	beq	Ldivdf$b$nf	| if d2 == 0x7ff00000 we check d3
+| Here we have both numbers finite and nonzero (and with no sign bit).
+| Now we get the exponents into d4 and d5 and normalize the numbers to
+| ensure that the ratio of the fractions is around 1. We do this by
+| making sure that both numbers have bit #DBL_MANT_DIG-32-1 (hidden bit)
+| set, even if they were denormalized to start with.
+| Thus, the result will satisfy: 2 > result > 1/2.
+	andl	d7,d4		| and isolate exponent in d4
+	beq	Ldivdf$a$den	| if exponent is zero we have a denormalized
+	andl	d6,d0		| and isolate fraction
+	orl	IMM (0x00100000),d0 | and put hidden bit back
+	swap	d4		| I like exponents in the first byte
+#ifndef __mcoldfire__
+	lsrw	IMM (4),d4	| 
+#else
+	lsrl	IMM (4),d4	| 
+#endif
+Ldivdf$1:			| 
+	andl	d7,d5		|
+	beq	Ldivdf$b$den	|
+	andl	d6,d2		|
+	orl	IMM (0x00100000),d2
+	swap	d5		|
+#ifndef __mcoldfire__
+	lsrw	IMM (4),d5	|
+#else
+	lsrl	IMM (4),d5	|
+#endif
+Ldivdf$2:			|
+#ifndef __mcoldfire__
+	subw	d5,d4		| subtract exponents
+	addw	IMM (D_BIAS),d4	| and add bias
+#else
+	subl	d5,d4		| subtract exponents
+	addl	IMM (D_BIAS),d4	| and add bias
+#endif
+
+| We are now ready to do the division. We have prepared things in such a way
+| that the ratio of the fractions will be less than 2 but greater than 1/2.
+| At this point the registers in use are:
+| d0-d1	hold a (first operand, bit DBL_MANT_DIG-32=0, bit 
+| DBL_MANT_DIG-1-32=1)
+| d2-d3	hold b (second operand, bit DBL_MANT_DIG-32=1)
+| d4	holds the difference of the exponents, corrected by the bias
+| a0	holds the sign of the ratio
+
+| To do the rounding correctly we need to keep information about the
+| nonsignificant bits. One way to do this would be to do the division
+| using four registers; another is to use two registers (as originally
+| I did), but use a sticky bit to preserve information about the 
+| fractional part. Note that we can keep that info in a1, which is not
+| used.
+	movel	IMM (0),d6	| d6-d7 will hold the result
+	movel	d6,d7		| 
+	movel	IMM (0),a1	| and a1 will hold the sticky bit
+
+	movel	IMM (DBL_MANT_DIG-32+1),d5	
+	
+1:	cmpl	d0,d2		| is a < b?
+	bhi	3f		| if b > a skip the following
+	beq	4f		| if d0==d2 check d1 and d3
+2:	subl	d3,d1		| 
+	subxl	d2,d0		| a <-- a - b
+	bset	d5,d6		| set the corresponding bit in d6
+3:	addl	d1,d1		| shift a by 1
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d5,1b		| and branch back
+#else
+	subql	IMM (1), d5
+	bpl	1b
+#endif
+	bra	5f			
+4:	cmpl	d1,d3		| here d0==d2, so check d1 and d3
+	bhi	3b		| if d1 > d2 skip the subtraction
+	bra	2b		| else go do it
+5:
+| Here we have to start setting the bits in the second long.
+	movel	IMM (31),d5	| again d5 is counter
+
+1:	cmpl	d0,d2		| is a < b?
+	bhi	3f		| if b > a skip the following
+	beq	4f		| if d0==d2 check d1 and d3
+2:	subl	d3,d1		| 
+	subxl	d2,d0		| a <-- a - b
+	bset	d5,d7		| set the corresponding bit in d7
+3:	addl	d1,d1		| shift a by 1
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d5,1b		| and branch back
+#else
+	subql	IMM (1), d5
+	bpl	1b
+#endif
+	bra	5f			
+4:	cmpl	d1,d3		| here d0==d2, so check d1 and d3
+	bhi	3b		| if d1 > d2 skip the subtraction
+	bra	2b		| else go do it
+5:
+| Now go ahead checking until we hit a one, which we store in d2.
+	movel	IMM (DBL_MANT_DIG),d5
+1:	cmpl	d2,d0		| is a < b?
+	bhi	4f		| if b < a, exit
+	beq	3f		| if d0==d2 check d1 and d3
+2:	addl	d1,d1		| shift a by 1
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d5,1b		| and branch back
+#else
+	subql	IMM (1), d5
+	bpl	1b
+#endif
+	movel	IMM (0),d2	| here no sticky bit was found
+	movel	d2,d3
+	bra	5f			
+3:	cmpl	d1,d3		| here d0==d2, so check d1 and d3
+	bhi	2b		| if d1 > d2 go back
+4:
+| Here put the sticky bit in d2-d3 (in the position which actually corresponds
+| to it; if you don't do this the algorithm loses in some cases). '
+	movel	IMM (0),d2
+	movel	d2,d3
+#ifndef __mcoldfire__
+	subw	IMM (DBL_MANT_DIG),d5
+	addw	IMM (63),d5
+	cmpw	IMM (31),d5
+#else
+	subl	IMM (DBL_MANT_DIG),d5
+	addl	IMM (63),d5
+	cmpl	IMM (31),d5
+#endif
+	bhi	2f
+1:	bset	d5,d3
+	bra	5f
+#ifndef __mcoldfire__
+	subw	IMM (32),d5
+#else
+	subl	IMM (32),d5
+#endif
+2:	bset	d5,d2
+5:
+| Finally we are finished! Move the longs in the address registers to
+| their final destination:
+	movel	d6,d0
+	movel	d7,d1
+	movel	IMM (0),d3
+
+| Here we have finished the division, with the result in d0-d1-d2-d3, with
+| 2^21 <= d6 < 2^23. Thus bit 23 is not set, but bit 22 could be set.
+| If it is not, then definitely bit 21 is set. Normalize so bit 22 is
+| not set:
+	btst	IMM (DBL_MANT_DIG-32+1),d0
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	roxrl	IMM (1),d2
+	roxrl	IMM (1),d3
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+1:
+| Now round, check for over- and underflow, and exit.
+	movel	a0,d7		| restore sign bit to d7
+	moveq	IMM (DIVIDE),d5
+	bra	Lround$exit
+
+Ldivdf$inop:
+	moveq	IMM (DIVIDE),d5
+	bra	Ld$inop
+
+Ldivdf$a$0:
+| If a is zero check to see whether b is zero also. In that case return
+| NaN; then check if b is NaN, and return NaN also in that case. Else
+| return a properly signed zero.
+	moveq	IMM (DIVIDE),d5
+	bclr	IMM (31),d2	|
+	movel	d2,d4		| 
+	orl	d3,d4		| 
+	beq	Ld$inop		| if b is also zero return NaN
+	cmpl	IMM (0x7ff00000),d2 | check for NaN
+	bhi	Ld$inop		| 
+	blt	1f		|
+	tstl	d3		|
+	bne	Ld$inop		|
+1:	movel	a0,d0		| else return signed zero
+	moveq	IMM(0),d1	| 
+	PICLEA	SYM (_fpCCR),a0	| clear exception flags
+	movew	IMM (0),a0@	|
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7	| 
+#else
+	moveml	sp@,d2-d7	| 
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| 
+	rts			| 	
+
+Ldivdf$b$0:
+	moveq	IMM (DIVIDE),d5
+| If we got here a is not zero. Check if a is NaN; in that case return NaN,
+| else return +/-INFINITY. Remember that a is in d0 with the sign bit 
+| cleared already.
+	movel	a0,d7		| put a's sign bit back in d7 '
+	cmpl	IMM (0x7ff00000),d0 | compare d0 with INFINITY
+	bhi	Ld$inop		| if larger it is NaN
+	tstl	d1		| 
+	bne	Ld$inop		| 
+	bra	Ld$div$0	| else signal DIVIDE_BY_ZERO
+
+Ldivdf$b$nf:
+	moveq	IMM (DIVIDE),d5
+| If d2 == 0x7ff00000 we have to check d3.
+	tstl	d3		|
+	bne	Ld$inop		| if d3 <> 0, b is NaN
+	bra	Ld$underflow	| else b is +/-INFINITY, so signal underflow
+
+Ldivdf$a$nf:
+	moveq	IMM (DIVIDE),d5
+| If d0 == 0x7ff00000 we have to check d1.
+	tstl	d1		|
+	bne	Ld$inop		| if d1 <> 0, a is NaN
+| If a is INFINITY we have to check b
+	cmpl	d7,d2		| compare b with INFINITY 
+	bge	Ld$inop		| if b is NaN or INFINITY return NaN
+	tstl	d3		|
+	bne	Ld$inop		| 
+	bra	Ld$overflow	| else return overflow
+
+| If a number is denormalized we put an exponent of 1 but do not put the 
+| bit back into the fraction.
+Ldivdf$a$den:
+	movel	IMM (1),d4
+	andl	d6,d0
+1:	addl	d1,d1		| shift a left until bit 20 is set
+	addxl	d0,d0
+#ifndef __mcoldfire__
+	subw	IMM (1),d4	| and adjust exponent
+#else
+	subl	IMM (1),d4	| and adjust exponent
+#endif
+	btst	IMM (DBL_MANT_DIG-32-1),d0
+	bne	Ldivdf$1
+	bra	1b
+
+Ldivdf$b$den:
+	movel	IMM (1),d5
+	andl	d6,d2
+1:	addl	d3,d3		| shift b left until bit 20 is set
+	addxl	d2,d2
+#ifndef __mcoldfire__
+	subw	IMM (1),d5	| and adjust exponent
+#else
+	subql	IMM (1),d5	| and adjust exponent
+#endif
+	btst	IMM (DBL_MANT_DIG-32-1),d2
+	bne	Ldivdf$2
+	bra	1b
+
+Lround$exit:
+| This is a common exit point for __muldf3 and __divdf3. When they enter
+| this point the sign of the result is in d7, the result in d0-d1, normalized
+| so that 2^21 <= d0 < 2^22, and the exponent is in the lower byte of d4.
+
+| First check for underlow in the exponent:
+#ifndef __mcoldfire__
+	cmpw	IMM (-DBL_MANT_DIG-1),d4		
+#else
+	cmpl	IMM (-DBL_MANT_DIG-1),d4		
+#endif
+	blt	Ld$underflow	
+| It could happen that the exponent is less than 1, in which case the 
+| number is denormalized. In this case we shift right and adjust the 
+| exponent until it becomes 1 or the fraction is zero (in the latter case 
+| we signal underflow and return zero).
+	movel	d7,a0		|
+	movel	IMM (0),d6	| use d6-d7 to collect bits flushed right
+	movel	d6,d7		| use d6-d7 to collect bits flushed right
+#ifndef __mcoldfire__
+	cmpw	IMM (1),d4	| if the exponent is less than 1 we 
+#else
+	cmpl	IMM (1),d4	| if the exponent is less than 1 we 
+#endif
+	bge	2f		| have to shift right (denormalize)
+1:
+#ifndef __mcoldfire__
+	addw	IMM (1),d4	| adjust the exponent
+	lsrl	IMM (1),d0	| shift right once 
+	roxrl	IMM (1),d1	|
+	roxrl	IMM (1),d2	|
+	roxrl	IMM (1),d3	|
+	roxrl	IMM (1),d6	| 
+	roxrl	IMM (1),d7	|
+	cmpw	IMM (1),d4	| is the exponent 1 already?
+#else
+	addl	IMM (1),d4	| adjust the exponent
+	lsrl	IMM (1),d7
+	btst	IMM (0),d6
+	beq	13f
+	bset	IMM (31),d7
+13:	lsrl	IMM (1),d6
+	btst	IMM (0),d3
+	beq	14f
+	bset	IMM (31),d6
+14:	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d2
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	12f
+	bset	IMM (31),d1
+12:	lsrl	IMM (1),d0
+	cmpl	IMM (1),d4	| is the exponent 1 already?
+#endif
+	beq	2f		| if not loop back
+	bra	1b              |
+	bra	Ld$underflow	| safety check, shouldn't execute '
+2:	orl	d6,d2		| this is a trick so we don't lose  '
+	orl	d7,d3		| the bits which were flushed right
+	movel	a0,d7		| get back sign bit into d7
+| Now call the rounding routine (which takes care of denormalized numbers):
+	lea	pc@(Lround$0),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Lround$0:
+| Here we have a correctly rounded result (either normalized or denormalized).
+
+| Here we should have either a normalized number or a denormalized one, and
+| the exponent is necessarily larger or equal to 1 (so we don't have to  '
+| check again for underflow!). We have to check for overflow or for a 
+| denormalized number (which also signals underflow).
+| Check for overflow (i.e., exponent >= 0x7ff).
+#ifndef __mcoldfire__
+	cmpw	IMM (0x07ff),d4
+#else
+	cmpl	IMM (0x07ff),d4
+#endif
+	bge	Ld$overflow
+| Now check for a denormalized number (exponent==0):
+	movew	d4,d4
+	beq	Ld$den
+1:
+| Put back the exponents and sign and return.
+#ifndef __mcoldfire__
+	lslw	IMM (4),d4	| exponent back to fourth byte
+#else
+	lsll	IMM (4),d4	| exponent back to fourth byte
+#endif
+	bclr	IMM (DBL_MANT_DIG-32-1),d0
+	swap	d0		| and put back exponent
+#ifndef __mcoldfire__
+	orw	d4,d0		| 
+#else
+	orl	d4,d0		| 
+#endif
+	swap	d0		|
+	orl	d7,d0		| and sign also
+
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+|=============================================================================
+|                              __negdf2
+|=============================================================================
+
+| double __negdf2(double, double);
+	FUNC(__negdf2)
+SYM (__negdf2):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d2-d7,sp@-
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	moveq	IMM (NEGATE),d5
+	movel	a6@(8),d0	| get number to negate in d0-d1
+	movel	a6@(12),d1	|
+	bchg	IMM (31),d0	| negate
+	movel	d0,d2		| make a positive copy (for the tests)
+	bclr	IMM (31),d2	|
+	movel	d2,d4		| check for zero
+	orl	d1,d4		|
+	beq	2f		| if zero (either sign) return +zero
+	cmpl	IMM (0x7ff00000),d2 | compare to +INFINITY
+	blt	1f		| if finite, return
+	bhi	Ld$inop		| if larger (fraction not zero) is NaN
+	tstl	d1		| if d2 == 0x7ff00000 check d1
+	bne	Ld$inop		|
+	movel	d0,d7		| else get sign and return INFINITY
+	andl	IMM (0x80000000),d7
+	bra	Ld$infty		
+1:	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+2:	bclr	IMM (31),d0
+	bra	1b
+
+|=============================================================================
+|                              __cmpdf2
+|=============================================================================
+
+GREATER =  1
+LESS    = -1
+EQUAL   =  0
+
+| int __cmpdf2_internal(double, double, int);
+SYM (__cmpdf2_internal):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d2-d7,sp@- 	| save registers
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	moveq	IMM (COMPARE),d5
+	movel	a6@(8),d0	| get first operand
+	movel	a6@(12),d1	|
+	movel	a6@(16),d2	| get second operand
+	movel	a6@(20),d3	|
+| First check if a and/or b are (+/-) zero and in that case clear
+| the sign bit.
+	movel	d0,d6		| copy signs into d6 (a) and d7(b)
+	bclr	IMM (31),d0	| and clear signs in d0 and d2
+	movel	d2,d7		|
+	bclr	IMM (31),d2	|
+	cmpl	IMM (0x7ff00000),d0 | check for a == NaN
+	bhi	Lcmpd$inop		| if d0 > 0x7ff00000, a is NaN
+	beq	Lcmpdf$a$nf	| if equal can be INFINITY, so check d1
+	movel	d0,d4		| copy into d4 to test for zero
+	orl	d1,d4		|
+	beq	Lcmpdf$a$0	|
+Lcmpdf$0:
+	cmpl	IMM (0x7ff00000),d2 | check for b == NaN
+	bhi	Lcmpd$inop		| if d2 > 0x7ff00000, b is NaN
+	beq	Lcmpdf$b$nf	| if equal can be INFINITY, so check d3
+	movel	d2,d4		|
+	orl	d3,d4		|
+	beq	Lcmpdf$b$0	|
+Lcmpdf$1:
+| Check the signs
+	eorl	d6,d7
+	bpl	1f
+| If the signs are not equal check if a >= 0
+	tstl	d6
+	bpl	Lcmpdf$a$gt$b	| if (a >= 0 && b < 0) => a > b
+	bmi	Lcmpdf$b$gt$a	| if (a < 0 && b >= 0) => a < b
+1:
+| If the signs are equal check for < 0
+	tstl	d6
+	bpl	1f
+| If both are negative exchange them
+#ifndef __mcoldfire__
+	exg	d0,d2
+	exg	d1,d3
+#else
+	movel	d0,d7
+	movel	d2,d0
+	movel	d7,d2
+	movel	d1,d7
+	movel	d3,d1
+	movel	d7,d3
+#endif
+1:
+| Now that they are positive we just compare them as longs (does this also
+| work for denormalized numbers?).
+	cmpl	d0,d2
+	bhi	Lcmpdf$b$gt$a	| |b| > |a|
+	bne	Lcmpdf$a$gt$b	| |b| < |a|
+| If we got here d0 == d2, so we compare d1 and d3.
+	cmpl	d1,d3
+	bhi	Lcmpdf$b$gt$a	| |b| > |a|
+	bne	Lcmpdf$a$gt$b	| |b| < |a|
+| If we got here a == b.
+	movel	IMM (EQUAL),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7 	| put back the registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+Lcmpdf$a$gt$b:
+	movel	IMM (GREATER),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7 	| put back the registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+Lcmpdf$b$gt$a:
+	movel	IMM (LESS),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7 	| put back the registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+Lcmpdf$a$0:	
+	bclr	IMM (31),d6
+	bra	Lcmpdf$0
+Lcmpdf$b$0:
+	bclr	IMM (31),d7
+	bra	Lcmpdf$1
+
+Lcmpdf$a$nf:
+	tstl	d1
+	bne	Ld$inop
+	bra	Lcmpdf$0
+
+Lcmpdf$b$nf:
+	tstl	d3
+	bne	Ld$inop
+	bra	Lcmpdf$1
+
+Lcmpd$inop:
+	movl	a6@(24),d0
+	moveq	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
+	moveq	IMM (DOUBLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+| int __cmpdf2(double, double);
+	FUNC(__cmpdf2)
+SYM (__cmpdf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(20),sp@-
+	movl	a6@(16),sp@-
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpdf2_internal)
+	unlk	a6
+	rts
+
+|=============================================================================
+|                           rounding routines
+|=============================================================================
+
+| The rounding routines expect the number to be normalized in registers
+| d0-d1-d2-d3, with the exponent in register d4. They assume that the 
+| exponent is larger or equal to 1. They return a properly normalized number
+| if possible, and a denormalized number otherwise. The exponent is returned
+| in d4.
+
+Lround$to$nearest:
+| We now normalize as suggested by D. Knuth ("Seminumerical Algorithms"):
+| Here we assume that the exponent is not too small (this should be checked
+| before entering the rounding routine), but the number could be denormalized.
+
+| Check for denormalized numbers:
+1:	btst	IMM (DBL_MANT_DIG-32),d0
+	bne	2f		| if set the number is normalized
+| Normalize shifting left until bit #DBL_MANT_DIG-32 is set or the exponent 
+| is one (remember that a denormalized number corresponds to an 
+| exponent of -D_BIAS+1).
+#ifndef __mcoldfire__
+	cmpw	IMM (1),d4	| remember that the exponent is at least one
+#else
+	cmpl	IMM (1),d4	| remember that the exponent is at least one
+#endif
+ 	beq	2f		| an exponent of one means denormalized
+	addl	d3,d3		| else shift and adjust the exponent
+	addxl	d2,d2		|
+	addxl	d1,d1		|
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d4,1b		|
+#else
+	subql	IMM (1), d4
+	bpl	1b
+#endif
+2:
+| Now round: we do it as follows: after the shifting we can write the
+| fraction part as f + delta, where 1 < f < 2^25, and 0 <= delta <= 2.
+| If delta < 1, do nothing. If delta > 1, add 1 to f. 
+| If delta == 1, we make sure the rounded number will be even (odd?) 
+| (after shifting).
+	btst	IMM (0),d1	| is delta < 1?
+	beq	2f		| if so, do not do anything
+	orl	d2,d3		| is delta == 1?
+	bne	1f		| if so round to even
+	movel	d1,d3		| 
+	andl	IMM (2),d3	| bit 1 is the last significant bit
+	movel	IMM (0),d2	|
+	addl	d3,d1		|
+	addxl	d2,d0		|
+	bra	2f		| 
+1:	movel	IMM (1),d3	| else add 1 
+	movel	IMM (0),d2	|
+	addl	d3,d1		|
+	addxl	d2,d0
+| Shift right once (because we used bit #DBL_MANT_DIG-32!).
+2:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1		
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+#endif
+
+| Now check again bit #DBL_MANT_DIG-32 (rounding could have produced a
+| 'fraction overflow' ...).
+	btst	IMM (DBL_MANT_DIG-32),d0	
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	addw	IMM (1),d4
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	addl	IMM (1),d4
+#endif
+1:
+| If bit #DBL_MANT_DIG-32-1 is clear we have a denormalized number, so we 
+| have to put the exponent to zero and return a denormalized number.
+	btst	IMM (DBL_MANT_DIG-32-1),d0
+	beq	1f
+	jmp	a0@
+1:	movel	IMM (0),d4
+	jmp	a0@
+
+Lround$to$zero:
+Lround$to$plus:
+Lround$to$minus:
+	jmp	a0@
+#endif /* L_double */
+
+#ifdef  L_float
+
+	.globl	SYM (_fpCCR)
+	.globl  $_exception_handler
+
+QUIET_NaN    = 0xffffffff
+SIGNL_NaN    = 0x7f800001
+INFINITY     = 0x7f800000
+
+F_MAX_EXP      = 0xff
+F_BIAS         = 126
+FLT_MAX_EXP    = F_MAX_EXP - F_BIAS
+FLT_MIN_EXP    = 1 - F_BIAS
+FLT_MANT_DIG   = 24
+
+INEXACT_RESULT 		= 0x0001
+UNDERFLOW 		= 0x0002
+OVERFLOW 		= 0x0004
+DIVIDE_BY_ZERO 		= 0x0008
+INVALID_OPERATION 	= 0x0010
+
+SINGLE_FLOAT = 1
+
+NOOP         = 0
+ADD          = 1
+MULTIPLY     = 2
+DIVIDE       = 3
+NEGATE       = 4
+COMPARE      = 5
+EXTENDSFDF   = 6
+TRUNCDFSF    = 7
+
+UNKNOWN           = -1
+ROUND_TO_NEAREST  = 0 | round result to nearest representable value
+ROUND_TO_ZERO     = 1 | round result towards zero
+ROUND_TO_PLUS     = 2 | round result towards plus infinity
+ROUND_TO_MINUS    = 3 | round result towards minus infinity
+
+| Entry points:
+
+	.globl SYM (__addsf3)
+	.globl SYM (__subsf3)
+	.globl SYM (__mulsf3)
+	.globl SYM (__divsf3)
+	.globl SYM (__negsf2)
+	.globl SYM (__cmpsf2)
+	.globl SYM (__cmpsf2_internal)
+#ifdef __ELF__
+	.hidden SYM (__cmpsf2_internal)
+#endif
+
+| These are common routines to return and signal exceptions.	
+
+	.text
+	.even
+
+Lf$den:
+| Return and signal a denormalized number
+	orl	d7,d0
+	moveq	IMM (INEXACT_RESULT+UNDERFLOW),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Lf$infty:
+Lf$overflow:
+| Return a properly signed INFINITY and set the exception flags 
+	movel	IMM (INFINITY),d0
+	orl	d7,d0
+	moveq	IMM (INEXACT_RESULT+OVERFLOW),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Lf$underflow:
+| Return 0 and set the exception flags 
+	moveq	IMM (0),d0
+	moveq	IMM (INEXACT_RESULT+UNDERFLOW),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Lf$inop:
+| Return a quiet NaN and set the exception flags
+	movel	IMM (QUIET_NaN),d0
+	moveq	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+Lf$div$0:
+| Return a properly signed INFINITY and set the exception flags
+	movel	IMM (INFINITY),d0
+	orl	d7,d0
+	moveq	IMM (INEXACT_RESULT+DIVIDE_BY_ZERO),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+|=============================================================================
+|=============================================================================
+|                         single precision routines
+|=============================================================================
+|=============================================================================
+
+| A single precision floating point number (float) has the format:
+|
+| struct _float {
+|  unsigned int sign      : 1;  /* sign bit */ 
+|  unsigned int exponent  : 8;  /* exponent, shifted by 126 */
+|  unsigned int fraction  : 23; /* fraction */
+| } float;
+| 
+| Thus sizeof(float) = 4 (32 bits). 
+|
+| All the routines are callable from C programs, and return the result 
+| in the single register d0. They also preserve all registers except 
+| d0-d1 and a0-a1.
+
+|=============================================================================
+|                              __subsf3
+|=============================================================================
+
+| float __subsf3(float, float);
+	FUNC(__subsf3)
+SYM (__subsf3):
+	bchg	IMM (31),sp@(8)	| change sign of second operand
+				| and fall through
+|=============================================================================
+|                              __addsf3
+|=============================================================================
+
+| float __addsf3(float, float);
+	FUNC(__addsf3)
+SYM (__addsf3):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)	| everything will be done in registers
+	moveml	d2-d7,sp@-	| save all data registers but d0-d1
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	movel	a6@(8),d0	| get first operand
+	movel	a6@(12),d1	| get second operand
+	movel	d0,a0		| get d0's sign bit '
+	addl	d0,d0		| check and clear sign bit of a
+	beq	Laddsf$b	| if zero return second operand
+	movel	d1,a1		| save b's sign bit '
+	addl	d1,d1		| get rid of sign bit
+	beq	Laddsf$a	| if zero return first operand
+
+| Get the exponents and check for denormalized and/or infinity.
+
+	movel	IMM (0x00ffffff),d4	| mask to get fraction
+	movel	IMM (0x01000000),d5	| mask to put hidden bit back
+
+	movel	d0,d6		| save a to get exponent
+	andl	d4,d0		| get fraction in d0
+	notl 	d4		| make d4 into a mask for the exponent
+	andl	d4,d6		| get exponent in d6
+	beq	Laddsf$a$den	| branch if a is denormalized
+	cmpl	d4,d6		| check for INFINITY or NaN
+	beq	Laddsf$nf
+	swap	d6		| put exponent into first word
+	orl	d5,d0		| and put hidden bit back
+Laddsf$1:
+| Now we have a's exponent in d6 (second byte) and the mantissa in d0. '
+	movel	d1,d7		| get exponent in d7
+	andl	d4,d7		| 
+	beq	Laddsf$b$den	| branch if b is denormalized
+	cmpl	d4,d7		| check for INFINITY or NaN
+	beq	Laddsf$nf
+	swap	d7		| put exponent into first word
+	notl 	d4		| make d4 into a mask for the fraction
+	andl	d4,d1		| get fraction in d1
+	orl	d5,d1		| and put hidden bit back
+Laddsf$2:
+| Now we have b's exponent in d7 (second byte) and the mantissa in d1. '
+
+| Note that the hidden bit corresponds to bit #FLT_MANT_DIG-1, and we 
+| shifted right once, so bit #FLT_MANT_DIG is set (so we have one extra
+| bit).
+
+	movel	d1,d2		| move b to d2, since we want to use
+				| two registers to do the sum
+	movel	IMM (0),d1	| and clear the new ones
+	movel	d1,d3		|
+
+| Here we shift the numbers in registers d0 and d1 so the exponents are the
+| same, and put the largest exponent in d6. Note that we are using two
+| registers for each number (see the discussion by D. Knuth in "Seminumerical 
+| Algorithms").
+#ifndef __mcoldfire__
+	cmpw	d6,d7		| compare exponents
+#else
+	cmpl	d6,d7		| compare exponents
+#endif
+	beq	Laddsf$3	| if equal don't shift '
+	bhi	5f		| branch if second exponent largest
+1:
+	subl	d6,d7		| keep the largest exponent
+	negl	d7
+#ifndef __mcoldfire__
+	lsrw	IMM (8),d7	| put difference in lower byte
+#else
+	lsrl	IMM (8),d7	| put difference in lower byte
+#endif
+| if difference is too large we don't shift (actually, we can just exit) '
+#ifndef __mcoldfire__
+	cmpw	IMM (FLT_MANT_DIG+2),d7		
+#else
+	cmpl	IMM (FLT_MANT_DIG+2),d7		
+#endif
+	bge	Laddsf$b$small
+#ifndef __mcoldfire__
+	cmpw	IMM (16),d7	| if difference >= 16 swap
+#else
+	cmpl	IMM (16),d7	| if difference >= 16 swap
+#endif
+	bge	4f
+2:
+#ifndef __mcoldfire__
+	subw	IMM (1),d7
+#else
+	subql	IMM (1), d7
+#endif
+3:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d2	| shift right second operand
+	roxrl	IMM (1),d3
+	dbra	d7,3b
+#else
+	lsrl	IMM (1),d3
+	btst	IMM (0),d2
+	beq	10f
+	bset	IMM (31),d3
+10:	lsrl	IMM (1),d2
+	subql	IMM (1), d7
+	bpl	3b
+#endif
+	bra	Laddsf$3
+4:
+	movew	d2,d3
+	swap	d3
+	movew	d3,d2
+	swap	d2
+#ifndef __mcoldfire__
+	subw	IMM (16),d7
+#else
+	subl	IMM (16),d7
+#endif
+	bne	2b		| if still more bits, go back to normal case
+	bra	Laddsf$3
+5:
+#ifndef __mcoldfire__
+	exg	d6,d7		| exchange the exponents
+#else
+	eorl	d6,d7
+	eorl	d7,d6
+	eorl	d6,d7
+#endif
+	subl	d6,d7		| keep the largest exponent
+	negl	d7		|
+#ifndef __mcoldfire__
+	lsrw	IMM (8),d7	| put difference in lower byte
+#else
+	lsrl	IMM (8),d7	| put difference in lower byte
+#endif
+| if difference is too large we don't shift (and exit!) '
+#ifndef __mcoldfire__
+	cmpw	IMM (FLT_MANT_DIG+2),d7		
+#else
+	cmpl	IMM (FLT_MANT_DIG+2),d7		
+#endif
+	bge	Laddsf$a$small
+#ifndef __mcoldfire__
+	cmpw	IMM (16),d7	| if difference >= 16 swap
+#else
+	cmpl	IMM (16),d7	| if difference >= 16 swap
+#endif
+	bge	8f
+6:
+#ifndef __mcoldfire__
+	subw	IMM (1),d7
+#else
+	subl	IMM (1),d7
+#endif
+7:
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0	| shift right first operand
+	roxrl	IMM (1),d1
+	dbra	d7,7b
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	subql	IMM (1),d7
+	bpl	7b
+#endif
+	bra	Laddsf$3
+8:
+	movew	d0,d1
+	swap	d1
+	movew	d1,d0
+	swap	d0
+#ifndef __mcoldfire__
+	subw	IMM (16),d7
+#else
+	subl	IMM (16),d7
+#endif
+	bne	6b		| if still more bits, go back to normal case
+				| otherwise we fall through
+
+| Now we have a in d0-d1, b in d2-d3, and the largest exponent in d6 (the
+| signs are stored in a0 and a1).
+
+Laddsf$3:
+| Here we have to decide whether to add or subtract the numbers
+#ifndef __mcoldfire__
+	exg	d6,a0		| get signs back
+	exg	d7,a1		| and save the exponents
+#else
+	movel	d6,d4
+	movel	a0,d6
+	movel	d4,a0
+	movel	d7,d4
+	movel	a1,d7
+	movel	d4,a1
+#endif
+	eorl	d6,d7		| combine sign bits
+	bmi	Lsubsf$0	| if negative a and b have opposite 
+				| sign so we actually subtract the
+				| numbers
+
+| Here we have both positive or both negative
+#ifndef __mcoldfire__
+	exg	d6,a0		| now we have the exponent in d6
+#else
+	movel	d6,d4
+	movel	a0,d6
+	movel	d4,a0
+#endif
+	movel	a0,d7		| and sign in d7
+	andl	IMM (0x80000000),d7
+| Here we do the addition.
+	addl	d3,d1
+	addxl	d2,d0
+| Note: now we have d2, d3, d4 and d5 to play with! 
+
+| Put the exponent, in the first byte, in d2, to use the "standard" rounding
+| routines:
+	movel	d6,d2
+#ifndef __mcoldfire__
+	lsrw	IMM (8),d2
+#else
+	lsrl	IMM (8),d2
+#endif
+
+| Before rounding normalize so bit #FLT_MANT_DIG is set (we will consider
+| the case of denormalized numbers in the rounding routine itself).
+| As in the addition (not in the subtraction!) we could have set 
+| one more bit we check this:
+	btst	IMM (FLT_MANT_DIG+1),d0	
+	beq	1f
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+#endif
+	addl	IMM (1),d2
+1:
+	lea	pc@(Laddsf$4),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Laddsf$4:
+| Put back the exponent, but check for overflow.
+#ifndef __mcoldfire__
+	cmpw	IMM (0xff),d2
+#else
+	cmpl	IMM (0xff),d2
+#endif
+	bhi	1f
+	bclr	IMM (FLT_MANT_DIG-1),d0
+#ifndef __mcoldfire__
+	lslw	IMM (7),d2
+#else
+	lsll	IMM (7),d2
+#endif
+	swap	d2
+	orl	d2,d0
+	bra	Laddsf$ret
+1:
+	moveq	IMM (ADD),d5
+	bra	Lf$overflow
+
+Lsubsf$0:
+| We are here if a > 0 and b < 0 (sign bits cleared).
+| Here we do the subtraction.
+	movel	d6,d7		| put sign in d7
+	andl	IMM (0x80000000),d7
+
+	subl	d3,d1		| result in d0-d1
+	subxl	d2,d0		|
+	beq	Laddsf$ret	| if zero just exit
+	bpl	1f		| if positive skip the following
+	bchg	IMM (31),d7	| change sign bit in d7
+	negl	d1
+	negxl	d0
+1:
+#ifndef __mcoldfire__
+	exg	d2,a0		| now we have the exponent in d2
+	lsrw	IMM (8),d2	| put it in the first byte
+#else
+	movel	d2,d4
+	movel	a0,d2
+	movel	d4,a0
+	lsrl	IMM (8),d2	| put it in the first byte
+#endif
+
+| Now d0-d1 is positive and the sign bit is in d7.
+
+| Note that we do not have to normalize, since in the subtraction bit
+| #FLT_MANT_DIG+1 is never set, and denormalized numbers are handled by
+| the rounding routines themselves.
+	lea	pc@(Lsubsf$1),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Lsubsf$1:
+| Put back the exponent (we can't have overflow!). '
+	bclr	IMM (FLT_MANT_DIG-1),d0
+#ifndef __mcoldfire__
+	lslw	IMM (7),d2
+#else
+	lsll	IMM (7),d2
+#endif
+	swap	d2
+	orl	d2,d0
+	bra	Laddsf$ret
+
+| If one of the numbers was too small (difference of exponents >= 
+| FLT_MANT_DIG+2) we return the other (and now we don't have to '
+| check for finiteness or zero).
+Laddsf$a$small:
+	movel	a6@(12),d0
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7	| restore data registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| and return
+	rts
+
+Laddsf$b$small:
+	movel	a6@(8),d0
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7	| restore data registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| and return
+	rts
+
+| If the numbers are denormalized remember to put exponent equal to 1.
+
+Laddsf$a$den:
+	movel	d5,d6		| d5 contains 0x01000000
+	swap	d6
+	bra	Laddsf$1
+
+Laddsf$b$den:
+	movel	d5,d7
+	swap	d7
+	notl 	d4		| make d4 into a mask for the fraction
+				| (this was not executed after the jump)
+	bra	Laddsf$2
+
+| The rest is mainly code for the different results which can be 
+| returned (checking always for +/-INFINITY and NaN).
+
+Laddsf$b:
+| Return b (if a is zero).
+	movel	a6@(12),d0
+	cmpl	IMM (0x80000000),d0	| Check if b is -0
+	bne	1f
+	movel	a0,d7
+	andl	IMM (0x80000000),d7	| Use the sign of a
+	clrl	d0
+	bra	Laddsf$ret
+Laddsf$a:
+| Return a (if b is zero).
+	movel	a6@(8),d0
+1:
+	moveq	IMM (ADD),d5
+| We have to check for NaN and +/-infty.
+	movel	d0,d7
+	andl	IMM (0x80000000),d7	| put sign in d7
+	bclr	IMM (31),d0		| clear sign
+	cmpl	IMM (INFINITY),d0	| check for infty or NaN
+	bge	2f
+	movel	d0,d0		| check for zero (we do this because we don't '
+	bne	Laddsf$ret	| want to return -0 by mistake
+	bclr	IMM (31),d7	| if zero be sure to clear sign
+	bra	Laddsf$ret	| if everything OK just return
+2:
+| The value to be returned is either +/-infty or NaN
+	andl	IMM (0x007fffff),d0	| check for NaN
+	bne	Lf$inop			| if mantissa not zero is NaN
+	bra	Lf$infty
+
+Laddsf$ret:
+| Normal exit (a and b nonzero, result is not NaN nor +/-infty).
+| We have to clear the exception flags (just the exception type).
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+	orl	d7,d0		| put sign bit
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7	| restore data registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| and return
+	rts
+
+Laddsf$ret$den:
+| Return a denormalized number (for addition we don't signal underflow) '
+	lsrl	IMM (1),d0	| remember to shift right back once
+	bra	Laddsf$ret	| and return
+
+| Note: when adding two floats of the same sign if either one is 
+| NaN we return NaN without regard to whether the other is finite or 
+| not. When subtracting them (i.e., when adding two numbers of 
+| opposite signs) things are more complicated: if both are INFINITY 
+| we return NaN, if only one is INFINITY and the other is NaN we return
+| NaN, but if it is finite we return INFINITY with the corresponding sign.
+
+Laddsf$nf:
+	moveq	IMM (ADD),d5
+| This could be faster but it is not worth the effort, since it is not
+| executed very often. We sacrifice speed for clarity here.
+	movel	a6@(8),d0	| get the numbers back (remember that we
+	movel	a6@(12),d1	| did some processing already)
+	movel	IMM (INFINITY),d4 | useful constant (INFINITY)
+	movel	d0,d2		| save sign bits
+	movel	d1,d3
+	bclr	IMM (31),d0	| clear sign bits
+	bclr	IMM (31),d1
+| We know that one of them is either NaN of +/-INFINITY
+| Check for NaN (if either one is NaN return NaN)
+	cmpl	d4,d0		| check first a (d0)
+	bhi	Lf$inop		
+	cmpl	d4,d1		| check now b (d1)
+	bhi	Lf$inop		
+| Now comes the check for +/-INFINITY. We know that both are (maybe not
+| finite) numbers, but we have to check if both are infinite whether we
+| are adding or subtracting them.
+	eorl	d3,d2		| to check sign bits
+	bmi	1f
+	movel	d0,d7
+	andl	IMM (0x80000000),d7	| get (common) sign bit
+	bra	Lf$infty
+1:
+| We know one (or both) are infinite, so we test for equality between the
+| two numbers (if they are equal they have to be infinite both, so we
+| return NaN).
+	cmpl	d1,d0		| are both infinite?
+	beq	Lf$inop		| if so return NaN
+
+	movel	d0,d7
+	andl	IMM (0x80000000),d7 | get a's sign bit '
+	cmpl	d4,d0		| test now for infinity
+	beq	Lf$infty	| if a is INFINITY return with this sign
+	bchg	IMM (31),d7	| else we know b is INFINITY and has
+	bra	Lf$infty	| the opposite sign
+
+|=============================================================================
+|                             __mulsf3
+|=============================================================================
+
+| float __mulsf3(float, float);
+	FUNC(__mulsf3)
+SYM (__mulsf3):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d2-d7,sp@-
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	movel	a6@(8),d0	| get a into d0
+	movel	a6@(12),d1	| and b into d1
+	movel	d0,d7		| d7 will hold the sign of the product
+	eorl	d1,d7		|
+	andl	IMM (0x80000000),d7
+	movel	IMM (INFINITY),d6	| useful constant (+INFINITY)
+	movel	d6,d5			| another (mask for fraction)
+	notl	d5			|
+	movel	IMM (0x00800000),d4	| this is to put hidden bit back
+	bclr	IMM (31),d0		| get rid of a's sign bit '
+	movel	d0,d2			|
+	beq	Lmulsf$a$0		| branch if a is zero
+	bclr	IMM (31),d1		| get rid of b's sign bit '
+	movel	d1,d3		|
+	beq	Lmulsf$b$0	| branch if b is zero
+	cmpl	d6,d0		| is a big?
+	bhi	Lmulsf$inop	| if a is NaN return NaN
+	beq	Lmulsf$inf	| if a is INFINITY we have to check b
+	cmpl	d6,d1		| now compare b with INFINITY
+	bhi	Lmulsf$inop	| is b NaN?
+	beq	Lmulsf$overflow | is b INFINITY?
+| Here we have both numbers finite and nonzero (and with no sign bit).
+| Now we get the exponents into d2 and d3.
+	andl	d6,d2		| and isolate exponent in d2
+	beq	Lmulsf$a$den	| if exponent is zero we have a denormalized
+	andl	d5,d0		| and isolate fraction
+	orl	d4,d0		| and put hidden bit back
+	swap	d2		| I like exponents in the first byte
+#ifndef __mcoldfire__
+	lsrw	IMM (7),d2	| 
+#else
+	lsrl	IMM (7),d2	| 
+#endif
+Lmulsf$1:			| number
+	andl	d6,d3		|
+	beq	Lmulsf$b$den	|
+	andl	d5,d1		|
+	orl	d4,d1		|
+	swap	d3		|
+#ifndef __mcoldfire__
+	lsrw	IMM (7),d3	|
+#else
+	lsrl	IMM (7),d3	|
+#endif
+Lmulsf$2:			|
+#ifndef __mcoldfire__
+	addw	d3,d2		| add exponents
+	subw	IMM (F_BIAS+1),d2 | and subtract bias (plus one)
+#else
+	addl	d3,d2		| add exponents
+	subl	IMM (F_BIAS+1),d2 | and subtract bias (plus one)
+#endif
+
+| We are now ready to do the multiplication. The situation is as follows:
+| both a and b have bit FLT_MANT_DIG-1 set (even if they were 
+| denormalized to start with!), which means that in the product 
+| bit 2*(FLT_MANT_DIG-1) (that is, bit 2*FLT_MANT_DIG-2-32 of the 
+| high long) is set. 
+
+| To do the multiplication let us move the number a little bit around ...
+	movel	d1,d6		| second operand in d6
+	movel	d0,d5		| first operand in d4-d5
+	movel	IMM (0),d4
+	movel	d4,d1		| the sums will go in d0-d1
+	movel	d4,d0
+
+| now bit FLT_MANT_DIG-1 becomes bit 31:
+	lsll	IMM (31-FLT_MANT_DIG+1),d6		
+
+| Start the loop (we loop #FLT_MANT_DIG times):
+	moveq	IMM (FLT_MANT_DIG-1),d3	
+1:	addl	d1,d1		| shift sum 
+	addxl	d0,d0
+	lsll	IMM (1),d6	| get bit bn
+	bcc	2f		| if not set skip sum
+	addl	d5,d1		| add a
+	addxl	d4,d0
+2:
+#ifndef __mcoldfire__
+	dbf	d3,1b		| loop back
+#else
+	subql	IMM (1),d3
+	bpl	1b
+#endif
+
+| Now we have the product in d0-d1, with bit (FLT_MANT_DIG - 1) + FLT_MANT_DIG
+| (mod 32) of d0 set. The first thing to do now is to normalize it so bit 
+| FLT_MANT_DIG is set (to do the rounding).
+#ifndef __mcoldfire__
+	rorl	IMM (6),d1
+	swap	d1
+	movew	d1,d3
+	andw	IMM (0x03ff),d3
+	andw	IMM (0xfd00),d1
+#else
+	movel	d1,d3
+	lsll	IMM (8),d1
+	addl	d1,d1
+	addl	d1,d1
+	moveq	IMM (22),d5
+	lsrl	d5,d3
+	orl	d3,d1
+	andl	IMM (0xfffffd00),d1
+#endif
+	lsll	IMM (8),d0
+	addl	d0,d0
+	addl	d0,d0
+#ifndef __mcoldfire__
+	orw	d3,d0
+#else
+	orl	d3,d0
+#endif
+
+	moveq	IMM (MULTIPLY),d5
+	
+	btst	IMM (FLT_MANT_DIG+1),d0
+	beq	Lround$exit
+#ifndef __mcoldfire__
+	lsrl	IMM (1),d0
+	roxrl	IMM (1),d1
+	addw	IMM (1),d2
+#else
+	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	addql	IMM (1),d2
+#endif
+	bra	Lround$exit
+
+Lmulsf$inop:
+	moveq	IMM (MULTIPLY),d5
+	bra	Lf$inop
+
+Lmulsf$overflow:
+	moveq	IMM (MULTIPLY),d5
+	bra	Lf$overflow
+
+Lmulsf$inf:
+	moveq	IMM (MULTIPLY),d5
+| If either is NaN return NaN; else both are (maybe infinite) numbers, so
+| return INFINITY with the correct sign (which is in d7).
+	cmpl	d6,d1		| is b NaN?
+	bhi	Lf$inop		| if so return NaN
+	bra	Lf$overflow	| else return +/-INFINITY
+
+| If either number is zero return zero, unless the other is +/-INFINITY, 
+| or NaN, in which case we return NaN.
+Lmulsf$b$0:
+| Here d1 (==b) is zero.
+	movel	a6@(8),d1	| get a again to check for non-finiteness
+	bra	1f
+Lmulsf$a$0:
+	movel	a6@(12),d1	| get b again to check for non-finiteness
+1:	bclr	IMM (31),d1	| clear sign bit 
+	cmpl	IMM (INFINITY),d1 | and check for a large exponent
+	bge	Lf$inop		| if b is +/-INFINITY or NaN return NaN
+	movel	d7,d0		| else return signed zero
+	PICLEA	SYM (_fpCCR),a0	|
+	movew	IMM (0),a0@	| 
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7	| 
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6		| 
+	rts			| 
+
+| If a number is denormalized we put an exponent of 1 but do not put the 
+| hidden bit back into the fraction; instead we shift left until bit 23
+| (the hidden bit) is set, adjusting the exponent accordingly. We do this
+| to ensure that the product of the fractions is close to 1.
+Lmulsf$a$den:
+	movel	IMM (1),d2
+	andl	d5,d0
+1:	addl	d0,d0		| shift a left (until bit 23 is set)
+#ifndef __mcoldfire__
+	subw	IMM (1),d2	| and adjust exponent
+#else
+	subql	IMM (1),d2	| and adjust exponent
+#endif
+	btst	IMM (FLT_MANT_DIG-1),d0
+	bne	Lmulsf$1	|
+	bra	1b		| else loop back
+
+Lmulsf$b$den:
+	movel	IMM (1),d3
+	andl	d5,d1
+1:	addl	d1,d1		| shift b left until bit 23 is set
+#ifndef __mcoldfire__
+	subw	IMM (1),d3	| and adjust exponent
+#else
+	subql	IMM (1),d3	| and adjust exponent
+#endif
+	btst	IMM (FLT_MANT_DIG-1),d1
+	bne	Lmulsf$2	|
+	bra	1b		| else loop back
+
+|=============================================================================
+|                             __divsf3
+|=============================================================================
+
+| float __divsf3(float, float);
+	FUNC(__divsf3)
+SYM (__divsf3):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d2-d7,sp@-
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	movel	a6@(8),d0		| get a into d0
+	movel	a6@(12),d1		| and b into d1
+	movel	d0,d7			| d7 will hold the sign of the result
+	eorl	d1,d7			|
+	andl	IMM (0x80000000),d7	| 
+	movel	IMM (INFINITY),d6	| useful constant (+INFINITY)
+	movel	d6,d5			| another (mask for fraction)
+	notl	d5			|
+	movel	IMM (0x00800000),d4	| this is to put hidden bit back
+	bclr	IMM (31),d0		| get rid of a's sign bit '
+	movel	d0,d2			|
+	beq	Ldivsf$a$0		| branch if a is zero
+	bclr	IMM (31),d1		| get rid of b's sign bit '
+	movel	d1,d3			|
+	beq	Ldivsf$b$0		| branch if b is zero
+	cmpl	d6,d0			| is a big?
+	bhi	Ldivsf$inop		| if a is NaN return NaN
+	beq	Ldivsf$inf		| if a is INFINITY we have to check b
+	cmpl	d6,d1			| now compare b with INFINITY 
+	bhi	Ldivsf$inop		| if b is NaN return NaN
+	beq	Ldivsf$underflow
+| Here we have both numbers finite and nonzero (and with no sign bit).
+| Now we get the exponents into d2 and d3 and normalize the numbers to
+| ensure that the ratio of the fractions is close to 1. We do this by
+| making sure that bit #FLT_MANT_DIG-1 (hidden bit) is set.
+	andl	d6,d2		| and isolate exponent in d2
+	beq	Ldivsf$a$den	| if exponent is zero we have a denormalized
+	andl	d5,d0		| and isolate fraction
+	orl	d4,d0		| and put hidden bit back
+	swap	d2		| I like exponents in the first byte
+#ifndef __mcoldfire__
+	lsrw	IMM (7),d2	| 
+#else
+	lsrl	IMM (7),d2	| 
+#endif
+Ldivsf$1:			| 
+	andl	d6,d3		|
+	beq	Ldivsf$b$den	|
+	andl	d5,d1		|
+	orl	d4,d1		|
+	swap	d3		|
+#ifndef __mcoldfire__
+	lsrw	IMM (7),d3	|
+#else
+	lsrl	IMM (7),d3	|
+#endif
+Ldivsf$2:			|
+#ifndef __mcoldfire__
+	subw	d3,d2		| subtract exponents
+ 	addw	IMM (F_BIAS),d2	| and add bias
+#else
+	subl	d3,d2		| subtract exponents
+ 	addl	IMM (F_BIAS),d2	| and add bias
+#endif
+ 
+| We are now ready to do the division. We have prepared things in such a way
+| that the ratio of the fractions will be less than 2 but greater than 1/2.
+| At this point the registers in use are:
+| d0	holds a (first operand, bit FLT_MANT_DIG=0, bit FLT_MANT_DIG-1=1)
+| d1	holds b (second operand, bit FLT_MANT_DIG=1)
+| d2	holds the difference of the exponents, corrected by the bias
+| d7	holds the sign of the ratio
+| d4, d5, d6 hold some constants
+	movel	d7,a0		| d6-d7 will hold the ratio of the fractions
+	movel	IMM (0),d6	| 
+	movel	d6,d7
+
+	moveq	IMM (FLT_MANT_DIG+1),d3
+1:	cmpl	d0,d1		| is a < b?
+	bhi	2f		|
+	bset	d3,d6		| set a bit in d6
+	subl	d1,d0		| if a >= b  a <-- a-b
+	beq	3f		| if a is zero, exit
+2:	addl	d0,d0		| multiply a by 2
+#ifndef __mcoldfire__
+	dbra	d3,1b
+#else
+	subql	IMM (1),d3
+	bpl	1b
+#endif
+
+| Now we keep going to set the sticky bit ...
+	moveq	IMM (FLT_MANT_DIG),d3
+1:	cmpl	d0,d1
+	ble	2f
+	addl	d0,d0
+#ifndef __mcoldfire__
+	dbra	d3,1b
+#else
+	subql	IMM(1),d3
+	bpl	1b
+#endif
+	movel	IMM (0),d1
+	bra	3f
+2:	movel	IMM (0),d1
+#ifndef __mcoldfire__
+	subw	IMM (FLT_MANT_DIG),d3
+	addw	IMM (31),d3
+#else
+	subl	IMM (FLT_MANT_DIG),d3
+	addl	IMM (31),d3
+#endif
+	bset	d3,d1
+3:
+	movel	d6,d0		| put the ratio in d0-d1
+	movel	a0,d7		| get sign back
+
+| Because of the normalization we did before we are guaranteed that 
+| d0 is smaller than 2^26 but larger than 2^24. Thus bit 26 is not set,
+| bit 25 could be set, and if it is not set then bit 24 is necessarily set.
+	btst	IMM (FLT_MANT_DIG+1),d0		
+	beq	1f              | if it is not set, then bit 24 is set
+	lsrl	IMM (1),d0	|
+#ifndef __mcoldfire__
+	addw	IMM (1),d2	|
+#else
+	addl	IMM (1),d2	|
+#endif
+1:
+| Now round, check for over- and underflow, and exit.
+	moveq	IMM (DIVIDE),d5
+	bra	Lround$exit
+
+Ldivsf$inop:
+	moveq	IMM (DIVIDE),d5
+	bra	Lf$inop
+
+Ldivsf$overflow:
+	moveq	IMM (DIVIDE),d5
+	bra	Lf$overflow
+
+Ldivsf$underflow:
+	moveq	IMM (DIVIDE),d5
+	bra	Lf$underflow
+
+Ldivsf$a$0:
+	moveq	IMM (DIVIDE),d5
+| If a is zero check to see whether b is zero also. In that case return
+| NaN; then check if b is NaN, and return NaN also in that case. Else
+| return a properly signed zero.
+	andl	IMM (0x7fffffff),d1	| clear sign bit and test b
+	beq	Lf$inop			| if b is also zero return NaN
+	cmpl	IMM (INFINITY),d1	| check for NaN
+	bhi	Lf$inop			| 
+	movel	d7,d0			| else return signed zero
+	PICLEA	SYM (_fpCCR),a0		|
+	movew	IMM (0),a0@		|
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7		| 
+#else
+	moveml	sp@,d2-d7		| 
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6			| 
+	rts				| 
+	
+Ldivsf$b$0:
+	moveq	IMM (DIVIDE),d5
+| If we got here a is not zero. Check if a is NaN; in that case return NaN,
+| else return +/-INFINITY. Remember that a is in d0 with the sign bit 
+| cleared already.
+	cmpl	IMM (INFINITY),d0	| compare d0 with INFINITY
+	bhi	Lf$inop			| if larger it is NaN
+	bra	Lf$div$0		| else signal DIVIDE_BY_ZERO
+
+Ldivsf$inf:
+	moveq	IMM (DIVIDE),d5
+| If a is INFINITY we have to check b
+	cmpl	IMM (INFINITY),d1	| compare b with INFINITY 
+	bge	Lf$inop			| if b is NaN or INFINITY return NaN
+	bra	Lf$overflow		| else return overflow
+
+| If a number is denormalized we put an exponent of 1 but do not put the 
+| bit back into the fraction.
+Ldivsf$a$den:
+	movel	IMM (1),d2
+	andl	d5,d0
+1:	addl	d0,d0		| shift a left until bit FLT_MANT_DIG-1 is set
+#ifndef __mcoldfire__
+	subw	IMM (1),d2	| and adjust exponent
+#else
+	subl	IMM (1),d2	| and adjust exponent
+#endif
+	btst	IMM (FLT_MANT_DIG-1),d0
+	bne	Ldivsf$1
+	bra	1b
+
+Ldivsf$b$den:
+	movel	IMM (1),d3
+	andl	d5,d1
+1:	addl	d1,d1		| shift b left until bit FLT_MANT_DIG is set
+#ifndef __mcoldfire__
+	subw	IMM (1),d3	| and adjust exponent
+#else
+	subl	IMM (1),d3	| and adjust exponent
+#endif
+	btst	IMM (FLT_MANT_DIG-1),d1
+	bne	Ldivsf$2
+	bra	1b
+
+Lround$exit:
+| This is a common exit point for __mulsf3 and __divsf3. 
+
+| First check for underlow in the exponent:
+#ifndef __mcoldfire__
+	cmpw	IMM (-FLT_MANT_DIG-1),d2		
+#else
+	cmpl	IMM (-FLT_MANT_DIG-1),d2		
+#endif
+	blt	Lf$underflow	
+| It could happen that the exponent is less than 1, in which case the 
+| number is denormalized. In this case we shift right and adjust the 
+| exponent until it becomes 1 or the fraction is zero (in the latter case 
+| we signal underflow and return zero).
+	movel	IMM (0),d6	| d6 is used temporarily
+#ifndef __mcoldfire__
+	cmpw	IMM (1),d2	| if the exponent is less than 1 we 
+#else
+	cmpl	IMM (1),d2	| if the exponent is less than 1 we 
+#endif
+	bge	2f		| have to shift right (denormalize)
+1:
+#ifndef __mcoldfire__
+	addw	IMM (1),d2	| adjust the exponent
+	lsrl	IMM (1),d0	| shift right once 
+	roxrl	IMM (1),d1	|
+	roxrl	IMM (1),d6	| d6 collect bits we would lose otherwise
+	cmpw	IMM (1),d2	| is the exponent 1 already?
+#else
+	addql	IMM (1),d2	| adjust the exponent
+	lsrl	IMM (1),d6
+	btst	IMM (0),d1
+	beq	11f
+	bset	IMM (31),d6
+11:	lsrl	IMM (1),d1
+	btst	IMM (0),d0
+	beq	10f
+	bset	IMM (31),d1
+10:	lsrl	IMM (1),d0
+	cmpl	IMM (1),d2	| is the exponent 1 already?
+#endif
+	beq	2f		| if not loop back
+	bra	1b              |
+	bra	Lf$underflow	| safety check, shouldn't execute '
+2:	orl	d6,d1		| this is a trick so we don't lose  '
+				| the extra bits which were flushed right
+| Now call the rounding routine (which takes care of denormalized numbers):
+	lea	pc@(Lround$0),a0 | to return from rounding routine
+	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
+#ifdef __mcoldfire__
+	clrl	d6
+#endif
+	movew	a1@(6),d6	| rounding mode in d6
+	beq	Lround$to$nearest
+#ifndef __mcoldfire__
+	cmpw	IMM (ROUND_TO_PLUS),d6
+#else
+	cmpl	IMM (ROUND_TO_PLUS),d6
+#endif
+	bhi	Lround$to$minus
+	blt	Lround$to$zero
+	bra	Lround$to$plus
+Lround$0:
+| Here we have a correctly rounded result (either normalized or denormalized).
+
+| Here we should have either a normalized number or a denormalized one, and
+| the exponent is necessarily larger or equal to 1 (so we don't have to  '
+| check again for underflow!). We have to check for overflow or for a 
+| denormalized number (which also signals underflow).
+| Check for overflow (i.e., exponent >= 255).
+#ifndef __mcoldfire__
+	cmpw	IMM (0x00ff),d2
+#else
+	cmpl	IMM (0x00ff),d2
+#endif
+	bge	Lf$overflow
+| Now check for a denormalized number (exponent==0).
+	movew	d2,d2
+	beq	Lf$den
+1:
+| Put back the exponents and sign and return.
+#ifndef __mcoldfire__
+	lslw	IMM (7),d2	| exponent back to fourth byte
+#else
+	lsll	IMM (7),d2	| exponent back to fourth byte
+#endif
+	bclr	IMM (FLT_MANT_DIG-1),d0
+	swap	d0		| and put back exponent
+#ifndef __mcoldfire__
+	orw	d2,d0		| 
+#else
+	orl	d2,d0
+#endif
+	swap	d0		|
+	orl	d7,d0		| and sign also
+
+	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+|=============================================================================
+|                             __negsf2
+|=============================================================================
+
+| This is trivial and could be shorter if we didn't bother checking for NaN '
+| and +/-INFINITY.
+
+| float __negsf2(float);
+	FUNC(__negsf2)
+SYM (__negsf2):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d2-d7,sp@-
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	moveq	IMM (NEGATE),d5
+	movel	a6@(8),d0	| get number to negate in d0
+	bchg	IMM (31),d0	| negate
+	movel	d0,d1		| make a positive copy
+	bclr	IMM (31),d1	|
+	tstl	d1		| check for zero
+	beq	2f		| if zero (either sign) return +zero
+	cmpl	IMM (INFINITY),d1 | compare to +INFINITY
+	blt	1f		|
+	bhi	Lf$inop		| if larger (fraction not zero) is NaN
+	movel	d0,d7		| else get sign and return INFINITY
+	andl	IMM (0x80000000),d7
+	bra	Lf$infty		
+1:	PICLEA	SYM (_fpCCR),a0
+	movew	IMM (0),a0@
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+2:	bclr	IMM (31),d0
+	bra	1b
+
+|=============================================================================
+|                             __cmpsf2
+|=============================================================================
+
+GREATER =  1
+LESS    = -1
+EQUAL   =  0
+
+| int __cmpsf2_internal(float, float, int);
+SYM (__cmpsf2_internal):
+#ifndef __mcoldfire__
+	link	a6,IMM (0)
+	moveml	d2-d7,sp@- 	| save registers
+#else
+	link	a6,IMM (-24)
+	moveml	d2-d7,sp@
+#endif
+	moveq	IMM (COMPARE),d5
+	movel	a6@(8),d0	| get first operand
+	movel	a6@(12),d1	| get second operand
+| Check if either is NaN, and in that case return garbage and signal
+| INVALID_OPERATION. Check also if either is zero, and clear the signs
+| if necessary.
+	movel	d0,d6
+	andl	IMM (0x7fffffff),d0
+	beq	Lcmpsf$a$0
+	cmpl	IMM (0x7f800000),d0
+	bhi	Lcmpf$inop
+Lcmpsf$1:
+	movel	d1,d7
+	andl	IMM (0x7fffffff),d1
+	beq	Lcmpsf$b$0
+	cmpl	IMM (0x7f800000),d1
+	bhi	Lcmpf$inop
+Lcmpsf$2:
+| Check the signs
+	eorl	d6,d7
+	bpl	1f
+| If the signs are not equal check if a >= 0
+	tstl	d6
+	bpl	Lcmpsf$a$gt$b	| if (a >= 0 && b < 0) => a > b
+	bmi	Lcmpsf$b$gt$a	| if (a < 0 && b >= 0) => a < b
+1:
+| If the signs are equal check for < 0
+	tstl	d6
+	bpl	1f
+| If both are negative exchange them
+#ifndef __mcoldfire__
+	exg	d0,d1
+#else
+	movel	d0,d7
+	movel	d1,d0
+	movel	d7,d1
+#endif
+1:
+| Now that they are positive we just compare them as longs (does this also
+| work for denormalized numbers?).
+	cmpl	d0,d1
+	bhi	Lcmpsf$b$gt$a	| |b| > |a|
+	bne	Lcmpsf$a$gt$b	| |b| < |a|
+| If we got here a == b.
+	movel	IMM (EQUAL),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7 	| put back the registers
+#else
+	moveml	sp@,d2-d7
+#endif
+	unlk	a6
+	rts
+Lcmpsf$a$gt$b:
+	movel	IMM (GREATER),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7 	| put back the registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+Lcmpsf$b$gt$a:
+	movel	IMM (LESS),d0
+#ifndef __mcoldfire__
+	moveml	sp@+,d2-d7 	| put back the registers
+#else
+	moveml	sp@,d2-d7
+	| XXX if frame pointer is ever removed, stack pointer must
+	| be adjusted here.
+#endif
+	unlk	a6
+	rts
+
+Lcmpsf$a$0:	
+	bclr	IMM (31),d6
+	bra	Lcmpsf$1
+Lcmpsf$b$0:
+	bclr	IMM (31),d7
+	bra	Lcmpsf$2
+
+Lcmpf$inop:
+	movl	a6@(16),d0
+	moveq	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
+	moveq	IMM (SINGLE_FLOAT),d6
+	PICJUMP	$_exception_handler
+
+| int __cmpsf2(float, float);
+	FUNC(__cmpsf2)
+SYM (__cmpsf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL SYM (__cmpsf2_internal)
+	unlk	a6
+	rts
+
+|=============================================================================
+|                           rounding routines
+|=============================================================================
+
+| The rounding routines expect the number to be normalized in registers
+| d0-d1, with the exponent in register d2. They assume that the 
+| exponent is larger or equal to 1. They return a properly normalized number
+| if possible, and a denormalized number otherwise. The exponent is returned
+| in d2.
+
+Lround$to$nearest:
+| We now normalize as suggested by D. Knuth ("Seminumerical Algorithms"):
+| Here we assume that the exponent is not too small (this should be checked
+| before entering the rounding routine), but the number could be denormalized.
+
+| Check for denormalized numbers:
+1:	btst	IMM (FLT_MANT_DIG),d0
+	bne	2f		| if set the number is normalized
+| Normalize shifting left until bit #FLT_MANT_DIG is set or the exponent 
+| is one (remember that a denormalized number corresponds to an 
+| exponent of -F_BIAS+1).
+#ifndef __mcoldfire__
+	cmpw	IMM (1),d2	| remember that the exponent is at least one
+#else
+	cmpl	IMM (1),d2	| remember that the exponent is at least one
+#endif
+ 	beq	2f		| an exponent of one means denormalized
+	addl	d1,d1		| else shift and adjust the exponent
+	addxl	d0,d0		|
+#ifndef __mcoldfire__
+	dbra	d2,1b		|
+#else
+	subql	IMM (1),d2
+	bpl	1b
+#endif
+2:
+| Now round: we do it as follows: after the shifting we can write the
+| fraction part as f + delta, where 1 < f < 2^25, and 0 <= delta <= 2.
+| If delta < 1, do nothing. If delta > 1, add 1 to f. 
+| If delta == 1, we make sure the rounded number will be even (odd?) 
+| (after shifting).
+	btst	IMM (0),d0	| is delta < 1?
+	beq	2f		| if so, do not do anything
+	tstl	d1		| is delta == 1?
+	bne	1f		| if so round to even
+	movel	d0,d1		| 
+	andl	IMM (2),d1	| bit 1 is the last significant bit
+	addl	d1,d0		| 
+	bra	2f		| 
+1:	movel	IMM (1),d1	| else add 1 
+	addl	d1,d0		|
+| Shift right once (because we used bit #FLT_MANT_DIG!).
+2:	lsrl	IMM (1),d0		
+| Now check again bit #FLT_MANT_DIG (rounding could have produced a
+| 'fraction overflow' ...).
+	btst	IMM (FLT_MANT_DIG),d0	
+	beq	1f
+	lsrl	IMM (1),d0
+#ifndef __mcoldfire__
+	addw	IMM (1),d2
+#else
+	addql	IMM (1),d2
+#endif
+1:
+| If bit #FLT_MANT_DIG-1 is clear we have a denormalized number, so we 
+| have to put the exponent to zero and return a denormalized number.
+	btst	IMM (FLT_MANT_DIG-1),d0
+	beq	1f
+	jmp	a0@
+1:	movel	IMM (0),d2
+	jmp	a0@
+
+Lround$to$zero:
+Lround$to$plus:
+Lround$to$minus:
+	jmp	a0@
+#endif /* L_float */
+
+| gcc expects the routines __eqdf2, __nedf2, __gtdf2, __gedf2,
+| __ledf2, __ltdf2 to all return the same value as a direct call to
+| __cmpdf2 would.  In this implementation, each of these routines
+| simply calls __cmpdf2.  It would be more efficient to give the
+| __cmpdf2 routine several names, but separating them out will make it
+| easier to write efficient versions of these routines someday.
+| If the operands recompare unordered unordered __gtdf2 and __gedf2 return -1.
+| The other routines return 1.
+
+#ifdef  L_eqdf2
+	.text
+	FUNC(__eqdf2)
+	.globl	SYM (__eqdf2)
+SYM (__eqdf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(20),sp@-
+	movl	a6@(16),sp@-
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpdf2_internal)
+	unlk	a6
+	rts
+#endif /* L_eqdf2 */
+
+#ifdef  L_nedf2
+	.text
+	FUNC(__nedf2)
+	.globl	SYM (__nedf2)
+SYM (__nedf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(20),sp@-
+	movl	a6@(16),sp@-
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpdf2_internal)
+	unlk	a6
+	rts
+#endif /* L_nedf2 */
+
+#ifdef  L_gtdf2
+	.text
+	FUNC(__gtdf2)
+	.globl	SYM (__gtdf2)
+SYM (__gtdf2):
+	link	a6,IMM (0)
+	pea	-1
+	movl	a6@(20),sp@-
+	movl	a6@(16),sp@-
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpdf2_internal)
+	unlk	a6
+	rts
+#endif /* L_gtdf2 */
+
+#ifdef  L_gedf2
+	.text
+	FUNC(__gedf2)
+	.globl	SYM (__gedf2)
+SYM (__gedf2):
+	link	a6,IMM (0)
+	pea	-1
+	movl	a6@(20),sp@-
+	movl	a6@(16),sp@-
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpdf2_internal)
+	unlk	a6
+	rts
+#endif /* L_gedf2 */
+
+#ifdef  L_ltdf2
+	.text
+	FUNC(__ltdf2)
+	.globl	SYM (__ltdf2)
+SYM (__ltdf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(20),sp@-
+	movl	a6@(16),sp@-
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpdf2_internal)
+	unlk	a6
+	rts
+#endif /* L_ltdf2 */
+
+#ifdef  L_ledf2
+	.text
+	FUNC(__ledf2)
+	.globl	SYM (__ledf2)
+SYM (__ledf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(20),sp@-
+	movl	a6@(16),sp@-
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpdf2_internal)
+	unlk	a6
+	rts
+#endif /* L_ledf2 */
+
+| The comments above about __eqdf2, et. al., also apply to __eqsf2,
+| et. al., except that the latter call __cmpsf2 rather than __cmpdf2.
+
+#ifdef  L_eqsf2
+	.text
+	FUNC(__eqsf2)
+	.globl	SYM (__eqsf2)
+SYM (__eqsf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpsf2_internal)
+	unlk	a6
+	rts
+#endif /* L_eqsf2 */
+
+#ifdef  L_nesf2
+	.text
+	FUNC(__nesf2)
+	.globl	SYM (__nesf2)
+SYM (__nesf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpsf2_internal)
+	unlk	a6
+	rts
+#endif /* L_nesf2 */
+
+#ifdef  L_gtsf2
+	.text
+	FUNC(__gtsf2)
+	.globl	SYM (__gtsf2)
+SYM (__gtsf2):
+	link	a6,IMM (0)
+	pea	-1
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpsf2_internal)
+	unlk	a6
+	rts
+#endif /* L_gtsf2 */
+
+#ifdef  L_gesf2
+	.text
+	FUNC(__gesf2)
+	.globl	SYM (__gesf2)
+SYM (__gesf2):
+	link	a6,IMM (0)
+	pea	-1
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpsf2_internal)
+	unlk	a6
+	rts
+#endif /* L_gesf2 */
+
+#ifdef  L_ltsf2
+	.text
+	FUNC(__ltsf2)
+	.globl	SYM (__ltsf2)
+SYM (__ltsf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpsf2_internal)
+	unlk	a6
+	rts
+#endif /* L_ltsf2 */
+
+#ifdef  L_lesf2
+	.text
+	FUNC(__lesf2)
+	.globl	SYM (__lesf2)
+SYM (__lesf2):
+	link	a6,IMM (0)
+	pea	1
+	movl	a6@(12),sp@-
+	movl	a6@(8),sp@-
+	PICCALL	SYM (__cmpsf2_internal)
+	unlk	a6
+	rts
+#endif /* L_lesf2 */
+
diff -rupN gcc-4.6.4.test/gcc/config/m68k/lb1sf68.asm gcc-4.6.4-fastcall/gcc/config/m68k/lb1sf68.asm
--- gcc-4.6.4.test/gcc/config/m68k/lb1sf68.asm	2017-05-01 04:31:40.687947001 +0200
+++ gcc-4.6.4-fastcall/gcc/config/m68k/lb1sf68.asm	2017-04-30 19:10:52.555947001 +0200
@@ -222,3897 +222,12 @@ see the files COPYING3 and COPYING.RUNTI
 # endif
 #endif /* __PIC__ */
 
-
-#ifdef L_floatex
-
-| This is an attempt at a decent floating point (single, double and 
-| extended double) code for the GNU C compiler. It should be easy to
-| adapt to other compilers (but beware of the local labels!).
-
-| Starting date: 21 October, 1990
-
-| It is convenient to introduce the notation (s,e,f) for a floating point
-| number, where s=sign, e=exponent, f=fraction. We will call a floating
-| point number fpn to abbreviate, independently of the precision.
-| Let MAX_EXP be in each case the maximum exponent (255 for floats, 1023 
-| for doubles and 16383 for long doubles). We then have the following 
-| different cases:
-|  1. Normalized fpns have 0 < e < MAX_EXP. They correspond to 
-|     (-1)^s x 1.f x 2^(e-bias-1).
-|  2. Denormalized fpns have e=0. They correspond to numbers of the form
-|     (-1)^s x 0.f x 2^(-bias).
-|  3. +/-INFINITY have e=MAX_EXP, f=0.
-|  4. Quiet NaN (Not a Number) have all bits set.
-|  5. Signaling NaN (Not a Number) have s=0, e=MAX_EXP, f=1.
-
-|=============================================================================
-|                                  exceptions
-|=============================================================================
-
-| This is the floating point condition code register (_fpCCR):
-|
-| struct {
-|   short _exception_bits;	
-|   short _trap_enable_bits;	
-|   short _sticky_bits;
-|   short _rounding_mode;
-|   short _format;
-|   short _last_operation;
-|   union {
-|     float sf;
-|     double df;
-|   } _operand1;
-|   union {
-|     float sf;
-|     double df;
-|   } _operand2;
-| } _fpCCR;
-
-	.data
-	.even
-
-	.globl	SYM (_fpCCR)
-	
-SYM (_fpCCR):
-__exception_bits:
-	.word	0
-__trap_enable_bits:
-	.word	0
-__sticky_bits:
-	.word	0
-__rounding_mode:
-	.word	ROUND_TO_NEAREST
-__format:
-	.word	NIL
-__last_operation:
-	.word	NOOP
-__operand1:
-	.long	0
-	.long	0
-__operand2:
-	.long 	0
-	.long	0
-
-| Offsets:
-EBITS  = __exception_bits - SYM (_fpCCR)
-TRAPE  = __trap_enable_bits - SYM (_fpCCR)
-STICK  = __sticky_bits - SYM (_fpCCR)
-ROUND  = __rounding_mode - SYM (_fpCCR)
-FORMT  = __format - SYM (_fpCCR)
-LASTO  = __last_operation - SYM (_fpCCR)
-OPER1  = __operand1 - SYM (_fpCCR)
-OPER2  = __operand2 - SYM (_fpCCR)
-
-| The following exception types are supported:
-INEXACT_RESULT 		= 0x0001
-UNDERFLOW 		= 0x0002
-OVERFLOW 		= 0x0004
-DIVIDE_BY_ZERO 		= 0x0008
-INVALID_OPERATION 	= 0x0010
-
-| The allowed rounding modes are:
-UNKNOWN           = -1
-ROUND_TO_NEAREST  = 0 | round result to nearest representable value
-ROUND_TO_ZERO     = 1 | round result towards zero
-ROUND_TO_PLUS     = 2 | round result towards plus infinity
-ROUND_TO_MINUS    = 3 | round result towards minus infinity
-
-| The allowed values of format are:
-NIL          = 0
-SINGLE_FLOAT = 1
-DOUBLE_FLOAT = 2
-LONG_FLOAT   = 3
-
-| The allowed values for the last operation are:
-NOOP         = 0
-ADD          = 1
-MULTIPLY     = 2
-DIVIDE       = 3
-NEGATE       = 4
-COMPARE      = 5
-EXTENDSFDF   = 6
-TRUNCDFSF    = 7
-
-|=============================================================================
-|                           __clear_sticky_bits
-|=============================================================================
-
-| The sticky bits are normally not cleared (thus the name), whereas the 
-| exception type and exception value reflect the last computation. 
-| This routine is provided to clear them (you can also write to _fpCCR,
-| since it is globally visible).
-
-	.globl  SYM (__clear_sticky_bit)
-
-	.text
-	.even
-
-| void __clear_sticky_bits(void);
-SYM (__clear_sticky_bit):		
-	PICLEA	SYM (_fpCCR),a0
-#ifndef __mcoldfire__
-	movew	IMM (0),a0@(STICK)
-#else
-	clr.w	a0@(STICK)
-#endif
-	rts
-
-|=============================================================================
-|                           $_exception_handler
-|=============================================================================
-
-	.globl  $_exception_handler
-
-	.text
-	.even
-
-| This is the common exit point if an exception occurs.
-| NOTE: it is NOT callable from C!
-| It expects the exception type in d7, the format (SINGLE_FLOAT,
-| DOUBLE_FLOAT or LONG_FLOAT) in d6, and the last operation code in d5.
-| It sets the corresponding exception and sticky bits, and the format. 
-| Depending on the format if fills the corresponding slots for the 
-| operands which produced the exception (all this information is provided
-| so if you write your own exception handlers you have enough information
-| to deal with the problem).
-| Then checks to see if the corresponding exception is trap-enabled, 
-| in which case it pushes the address of _fpCCR and traps through 
-| trap FPTRAP (15 for the moment).
-
-FPTRAP = 15
-
-$_exception_handler:
-	PICLEA	SYM (_fpCCR),a0
-	movew	d7,a0@(EBITS)	| set __exception_bits
-#ifndef __mcoldfire__
-	orw	d7,a0@(STICK)	| and __sticky_bits
-#else
-	movew	a0@(STICK),d4
-	orl	d7,d4
-	movew	d4,a0@(STICK)
-#endif
-	movew	d6,a0@(FORMT)	| and __format
-	movew	d5,a0@(LASTO)	| and __last_operation
-
-| Now put the operands in place:
-#ifndef __mcoldfire__
-	cmpw	IMM (SINGLE_FLOAT),d6
-#else
-	cmpl	IMM (SINGLE_FLOAT),d6
-#endif
-	beq	1f
-	movel	a6@(8),a0@(OPER1)
-	movel	a6@(12),a0@(OPER1+4)
-	movel	a6@(16),a0@(OPER2)
-	movel	a6@(20),a0@(OPER2+4)
-	bra	2f
-1:	movel	a6@(8),a0@(OPER1)
-	movel	a6@(12),a0@(OPER2)
-2:
-| And check whether the exception is trap-enabled:
-#ifndef __mcoldfire__
-	andw	a0@(TRAPE),d7	| is exception trap-enabled?
-#else
-	clrl	d6
-	movew	a0@(TRAPE),d6
-	andl	d6,d7
-#endif
-	beq	1f		| no, exit
-	PICPEA	SYM (_fpCCR),a1	| yes, push address of _fpCCR
-	trap	IMM (FPTRAP)	| and trap
-#ifndef __mcoldfire__
-1:	moveml	sp@+,d2-d7	| restore data registers
-#else
-1:	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6		| and return
-	rts
-#endif /* L_floatex */
-
-#ifdef  L_mulsi3
-	.text
-	FUNC(__mulsi3)
-	.globl	SYM (__mulsi3)
-SYM (__mulsi3):
-	movew	sp@(4), d0	/* x0 -> d0 */
-	muluw	sp@(10), d0	/* x0*y1 */
-	movew	sp@(6), d1	/* x1 -> d1 */
-	muluw	sp@(8), d1	/* x1*y0 */
-#ifndef __mcoldfire__
-	addw	d1, d0
-#else
-	addl	d1, d0
-#endif
-	swap	d0
-	clrw	d0
-	movew	sp@(6), d1	/* x1 -> d1 */
-	muluw	sp@(10), d1	/* x1*y1 */
-	addl	d1, d0
-
-	rts
-#endif /* L_mulsi3 */
-
-#ifdef  L_udivsi3
-	.text
-	FUNC(__udivsi3)
-	.globl	SYM (__udivsi3)
-SYM (__udivsi3):
-#ifndef __mcoldfire__
-	movel	d2, sp@-
-	movel	sp@(12), d1	/* d1 = divisor */
-	movel	sp@(8), d0	/* d0 = dividend */
-
-	cmpl	IMM (0x10000), d1 /* divisor >= 2 ^ 16 ?   */
-	jcc	L3		/* then try next algorithm */
-	movel	d0, d2
-	clrw	d2
-	swap	d2
-	divu	d1, d2          /* high quotient in lower word */
-	movew	d2, d0		/* save high quotient */
-	swap	d0
-	movew	sp@(10), d2	/* get low dividend + high rest */
-	divu	d1, d2		/* low quotient */
-	movew	d2, d0
-	jra	L6
-
-L3:	movel	d1, d2		/* use d2 as divisor backup */
-L4:	lsrl	IMM (1), d1	/* shift divisor */
-	lsrl	IMM (1), d0	/* shift dividend */
-	cmpl	IMM (0x10000), d1 /* still divisor >= 2 ^ 16 ?  */
-	jcc	L4
-	divu	d1, d0		/* now we have 16-bit divisor */
-	andl	IMM (0xffff), d0 /* mask out divisor, ignore remainder */
-
-/* Multiply the 16-bit tentative quotient with the 32-bit divisor.  Because of
-   the operand ranges, this might give a 33-bit product.  If this product is
-   greater than the dividend, the tentative quotient was too large. */
-	movel	d2, d1
-	mulu	d0, d1		/* low part, 32 bits */
-	swap	d2
-	mulu	d0, d2		/* high part, at most 17 bits */
-	swap	d2		/* align high part with low part */
-	tstw	d2		/* high part 17 bits? */
-	jne	L5		/* if 17 bits, quotient was too large */
-	addl	d2, d1		/* add parts */
-	jcs	L5		/* if sum is 33 bits, quotient was too large */
-	cmpl	sp@(8), d1	/* compare the sum with the dividend */
-	jls	L6		/* if sum > dividend, quotient was too large */
-L5:	subql	IMM (1), d0	/* adjust quotient */
-
-L6:	movel	sp@+, d2
-	rts
-
-#else /* __mcoldfire__ */
-
-/* ColdFire implementation of non-restoring division algorithm from
-   Hennessy & Patterson, Appendix A. */
-	link	a6,IMM (-12)
-	moveml	d2-d4,sp@
-	movel	a6@(8),d0
-	movel	a6@(12),d1
-	clrl	d2		| clear p
-	moveq	IMM (31),d4
-L1:	addl	d0,d0		| shift reg pair (p,a) one bit left
-	addxl	d2,d2
-	movl	d2,d3		| subtract b from p, store in tmp.
-	subl	d1,d3
-	jcs	L2		| if no carry,
-	bset	IMM (0),d0	| set the low order bit of a to 1,
-	movl	d3,d2		| and store tmp in p.
-L2:	subql	IMM (1),d4
-	jcc	L1
-	moveml	sp@,d2-d4	| restore data registers
-	unlk	a6		| and return
-	rts
-#endif /* __mcoldfire__ */
-
-#endif /* L_udivsi3 */
-
-#ifdef  L_divsi3
-	.text
-	FUNC(__divsi3)
-	.globl	SYM (__divsi3)
-SYM (__divsi3):
-	movel	d2, sp@-
-
-	moveq	IMM (1), d2	/* sign of result stored in d2 (=1 or =-1) */
-	movel	sp@(12), d1	/* d1 = divisor */
-	jpl	L1
-	negl	d1
-#ifndef __mcoldfire__
-	negb	d2		/* change sign because divisor <0  */
-#else
-	negl	d2		/* change sign because divisor <0  */
-#endif
-L1:	movel	sp@(8), d0	/* d0 = dividend */
-	jpl	L2
-	negl	d0
-#ifndef __mcoldfire__
-	negb	d2
-#else
-	negl	d2
-#endif
-
-L2:	movel	d1, sp@-
-	movel	d0, sp@-
-	PICCALL	SYM (__udivsi3)	/* divide abs(dividend) by abs(divisor) */
-	addql	IMM (8), sp
-
-	tstb	d2
-	jpl	L3
-	negl	d0
-
-L3:	movel	sp@+, d2
-	rts
-#endif /* L_divsi3 */
-
-#ifdef  L_umodsi3
-	.text
-	FUNC(__umodsi3)
-	.globl	SYM (__umodsi3)
-SYM (__umodsi3):
-	movel	sp@(8), d1	/* d1 = divisor */
-	movel	sp@(4), d0	/* d0 = dividend */
-	movel	d1, sp@-
-	movel	d0, sp@-
-	PICCALL	SYM (__udivsi3)
-	addql	IMM (8), sp
-	movel	sp@(8), d1	/* d1 = divisor */
-#ifndef __mcoldfire__
-	movel	d1, sp@-
-	movel	d0, sp@-
-	PICCALL	SYM (__mulsi3)	/* d0 = (a/b)*b */
-	addql	IMM (8), sp
-#else
-	mulsl	d1,d0
-#endif
-	movel	sp@(4), d1	/* d1 = dividend */
-	subl	d0, d1		/* d1 = a - (a/b)*b */
-	movel	d1, d0
-	rts
-#endif /* L_umodsi3 */
-
-#ifdef  L_modsi3
-	.text
-	FUNC(__modsi3)
-	.globl	SYM (__modsi3)
-SYM (__modsi3):
-	movel	sp@(8), d1	/* d1 = divisor */
-	movel	sp@(4), d0	/* d0 = dividend */
-	movel	d1, sp@-
-	movel	d0, sp@-
-	PICCALL	SYM (__divsi3)
-	addql	IMM (8), sp
-	movel	sp@(8), d1	/* d1 = divisor */
-#ifndef __mcoldfire__
-	movel	d1, sp@-
-	movel	d0, sp@-
-	PICCALL	SYM (__mulsi3)	/* d0 = (a/b)*b */
-	addql	IMM (8), sp
-#else
-	mulsl	d1,d0
-#endif
-	movel	sp@(4), d1	/* d1 = dividend */
-	subl	d0, d1		/* d1 = a - (a/b)*b */
-	movel	d1, d0
-	rts
-#endif /* L_modsi3 */
-
-
-#ifdef  L_double
-
-	.globl	SYM (_fpCCR)
-	.globl  $_exception_handler
-
-QUIET_NaN      = 0xffffffff
-
-D_MAX_EXP      = 0x07ff
-D_BIAS         = 1022
-DBL_MAX_EXP    = D_MAX_EXP - D_BIAS
-DBL_MIN_EXP    = 1 - D_BIAS
-DBL_MANT_DIG   = 53
-
-INEXACT_RESULT 		= 0x0001
-UNDERFLOW 		= 0x0002
-OVERFLOW 		= 0x0004
-DIVIDE_BY_ZERO 		= 0x0008
-INVALID_OPERATION 	= 0x0010
-
-DOUBLE_FLOAT = 2
-
-NOOP         = 0
-ADD          = 1
-MULTIPLY     = 2
-DIVIDE       = 3
-NEGATE       = 4
-COMPARE      = 5
-EXTENDSFDF   = 6
-TRUNCDFSF    = 7
-
-UNKNOWN           = -1
-ROUND_TO_NEAREST  = 0 | round result to nearest representable value
-ROUND_TO_ZERO     = 1 | round result towards zero
-ROUND_TO_PLUS     = 2 | round result towards plus infinity
-ROUND_TO_MINUS    = 3 | round result towards minus infinity
-
-| Entry points:
-
-	.globl SYM (__adddf3)
-	.globl SYM (__subdf3)
-	.globl SYM (__muldf3)
-	.globl SYM (__divdf3)
-	.globl SYM (__negdf2)
-	.globl SYM (__cmpdf2)
-	.globl SYM (__cmpdf2_internal)
-#ifdef __ELF__
-	.hidden SYM (__cmpdf2_internal)
-#endif
-
-	.text
-	.even
-
-| These are common routines to return and signal exceptions.	
-
-Ld$den:
-| Return and signal a denormalized number
-	orl	d7,d0
-	movew	IMM (INEXACT_RESULT+UNDERFLOW),d7
-	moveq	IMM (DOUBLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-Ld$infty:
-Ld$overflow:
-| Return a properly signed INFINITY and set the exception flags 
-	movel	IMM (0x7ff00000),d0
-	movel	IMM (0),d1
-	orl	d7,d0
-	movew	IMM (INEXACT_RESULT+OVERFLOW),d7
-	moveq	IMM (DOUBLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-Ld$underflow:
-| Return 0 and set the exception flags 
-	movel	IMM (0),d0
-	movel	d0,d1
-	movew	IMM (INEXACT_RESULT+UNDERFLOW),d7
-	moveq	IMM (DOUBLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-Ld$inop:
-| Return a quiet NaN and set the exception flags
-	movel	IMM (QUIET_NaN),d0
-	movel	d0,d1
-	movew	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
-	moveq	IMM (DOUBLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-Ld$div$0:
-| Return a properly signed INFINITY and set the exception flags
-	movel	IMM (0x7ff00000),d0
-	movel	IMM (0),d1
-	orl	d7,d0
-	movew	IMM (INEXACT_RESULT+DIVIDE_BY_ZERO),d7
-	moveq	IMM (DOUBLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-|=============================================================================
-|=============================================================================
-|                         double precision routines
-|=============================================================================
-|=============================================================================
-
-| A double precision floating point number (double) has the format:
-|
-| struct _double {
-|  unsigned int sign      : 1;  /* sign bit */ 
-|  unsigned int exponent  : 11; /* exponent, shifted by 126 */
-|  unsigned int fraction  : 52; /* fraction */
-| } double;
-| 
-| Thus sizeof(double) = 8 (64 bits). 
-|
-| All the routines are callable from C programs, and return the result 
-| in the register pair d0-d1. They also preserve all registers except 
-| d0-d1 and a0-a1.
-
-|=============================================================================
-|                              __subdf3
-|=============================================================================
-
-| double __subdf3(double, double);
-	FUNC(__subdf3)
-SYM (__subdf3):
-	bchg	IMM (31),sp@(12) | change sign of second operand
-				| and fall through, so we always add
-|=============================================================================
-|                              __adddf3
-|=============================================================================
-
-| double __adddf3(double, double);
-	FUNC(__adddf3)
-SYM (__adddf3):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)	| everything will be done in registers
-	moveml	d2-d7,sp@-	| save all data registers and a2 (but d0-d1)
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	movel	a6@(8),d0	| get first operand
-	movel	a6@(12),d1	| 
-	movel	a6@(16),d2	| get second operand
-	movel	a6@(20),d3	| 
-
-	movel	d0,d7		| get d0's sign bit in d7 '
-	addl	d1,d1		| check and clear sign bit of a, and gain one
-	addxl	d0,d0		| bit of extra precision
-	beq	Ladddf$b	| if zero return second operand
-
-	movel	d2,d6		| save sign in d6 
-	addl	d3,d3		| get rid of sign bit and gain one bit of
-	addxl	d2,d2		| extra precision
-	beq	Ladddf$a	| if zero return first operand
-
-	andl	IMM (0x80000000),d7 | isolate a's sign bit '
-        swap	d6		| and also b's sign bit '
-#ifndef __mcoldfire__
-	andw	IMM (0x8000),d6	|
-	orw	d6,d7		| and combine them into d7, so that a's sign '
-				| bit is in the high word and b's is in the '
-				| low word, so d6 is free to be used
-#else
-	andl	IMM (0x8000),d6
-	orl	d6,d7
-#endif
-	movel	d7,a0		| now save d7 into a0, so d7 is free to
-                		| be used also
-
-| Get the exponents and check for denormalized and/or infinity.
-
-	movel	IMM (0x001fffff),d6 | mask for the fraction
-	movel	IMM (0x00200000),d7 | mask to put hidden bit back
-
-	movel	d0,d4		| 
-	andl	d6,d0		| get fraction in d0
-	notl	d6		| make d6 into mask for the exponent
-	andl	d6,d4		| get exponent in d4
-	beq	Ladddf$a$den	| branch if a is denormalized
-	cmpl	d6,d4		| check for INFINITY or NaN
-	beq	Ladddf$nf       | 
-	orl	d7,d0		| and put hidden bit back
-Ladddf$1:
-	swap	d4		| shift right exponent so that it starts
-#ifndef __mcoldfire__
-	lsrw	IMM (5),d4	| in bit 0 and not bit 20
-#else
-	lsrl	IMM (5),d4	| in bit 0 and not bit 20
-#endif
-| Now we have a's exponent in d4 and fraction in d0-d1 '
-	movel	d2,d5		| save b to get exponent
-	andl	d6,d5		| get exponent in d5
-	beq	Ladddf$b$den	| branch if b is denormalized
-	cmpl	d6,d5		| check for INFINITY or NaN
-	beq	Ladddf$nf
-	notl	d6		| make d6 into mask for the fraction again
-	andl	d6,d2		| and get fraction in d2
-	orl	d7,d2		| and put hidden bit back
-Ladddf$2:
-	swap	d5		| shift right exponent so that it starts
-#ifndef __mcoldfire__
-	lsrw	IMM (5),d5	| in bit 0 and not bit 20
-#else
-	lsrl	IMM (5),d5	| in bit 0 and not bit 20
-#endif
-
-| Now we have b's exponent in d5 and fraction in d2-d3. '
-
-| The situation now is as follows: the signs are combined in a0, the 
-| numbers are in d0-d1 (a) and d2-d3 (b), and the exponents in d4 (a)
-| and d5 (b). To do the rounding correctly we need to keep all the
-| bits until the end, so we need to use d0-d1-d2-d3 for the first number
-| and d4-d5-d6-d7 for the second. To do this we store (temporarily) the
-| exponents in a2-a3.
-
-#ifndef __mcoldfire__
-	moveml	a2-a3,sp@-	| save the address registers
-#else
-	movel	a2,sp@-	
-	movel	a3,sp@-	
-	movel	a4,sp@-	
-#endif
-
-	movel	d4,a2		| save the exponents
-	movel	d5,a3		| 
-
-	movel	IMM (0),d7	| and move the numbers around
-	movel	d7,d6		|
-	movel	d3,d5		|
-	movel	d2,d4		|
-	movel	d7,d3		|
-	movel	d7,d2		|
-
-| Here we shift the numbers until the exponents are the same, and put 
-| the largest exponent in a2.
-#ifndef __mcoldfire__
-	exg	d4,a2		| get exponents back
-	exg	d5,a3		|
-	cmpw	d4,d5		| compare the exponents
-#else
-	movel	d4,a4		| get exponents back
-	movel	a2,d4
-	movel	a4,a2
-	movel	d5,a4
-	movel	a3,d5
-	movel	a4,a3
-	cmpl	d4,d5		| compare the exponents
-#endif
-	beq	Ladddf$3	| if equal don't shift '
-	bhi	9f		| branch if second exponent is higher
-
-| Here we have a's exponent larger than b's, so we have to shift b. We do 
-| this by using as counter d2:
-1:	movew	d4,d2		| move largest exponent to d2
-#ifndef __mcoldfire__
-	subw	d5,d2		| and subtract second exponent
-	exg	d4,a2		| get back the longs we saved
-	exg	d5,a3		|
-#else
-	subl	d5,d2		| and subtract second exponent
-	movel	d4,a4		| get back the longs we saved
-	movel	a2,d4
-	movel	a4,a2
-	movel	d5,a4
-	movel	a3,d5
-	movel	a4,a3
-#endif
-| if difference is too large we don't shift (actually, we can just exit) '
-#ifndef __mcoldfire__
-	cmpw	IMM (DBL_MANT_DIG+2),d2
-#else
-	cmpl	IMM (DBL_MANT_DIG+2),d2
-#endif
-	bge	Ladddf$b$small
-#ifndef __mcoldfire__
-	cmpw	IMM (32),d2	| if difference >= 32, shift by longs
-#else
-	cmpl	IMM (32),d2	| if difference >= 32, shift by longs
-#endif
-	bge	5f
-2:
-#ifndef __mcoldfire__
-	cmpw	IMM (16),d2	| if difference >= 16, shift by words	
-#else
-	cmpl	IMM (16),d2	| if difference >= 16, shift by words	
-#endif
-	bge	6f
-	bra	3f		| enter dbra loop
-
-4:
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d4
-	roxrl	IMM (1),d5
-	roxrl	IMM (1),d6
-	roxrl	IMM (1),d7
-#else
-	lsrl	IMM (1),d7
-	btst	IMM (0),d6
-	beq	10f
-	bset	IMM (31),d7
-10:	lsrl	IMM (1),d6
-	btst	IMM (0),d5
-	beq	11f
-	bset	IMM (31),d6
-11:	lsrl	IMM (1),d5
-	btst	IMM (0),d4
-	beq	12f
-	bset	IMM (31),d5
-12:	lsrl	IMM (1),d4
-#endif
-3:
-#ifndef __mcoldfire__
-	dbra	d2,4b
-#else
-	subql	IMM (1),d2
-	bpl	4b	
-#endif
-	movel	IMM (0),d2
-	movel	d2,d3	
-	bra	Ladddf$4
-5:
-	movel	d6,d7
-	movel	d5,d6
-	movel	d4,d5
-	movel	IMM (0),d4
-#ifndef __mcoldfire__
-	subw	IMM (32),d2
-#else
-	subl	IMM (32),d2
-#endif
-	bra	2b
-6:
-	movew	d6,d7
-	swap	d7
-	movew	d5,d6
-	swap	d6
-	movew	d4,d5
-	swap	d5
-	movew	IMM (0),d4
-	swap	d4
-#ifndef __mcoldfire__
-	subw	IMM (16),d2
-#else
-	subl	IMM (16),d2
-#endif
-	bra	3b
-	
-9:
-#ifndef __mcoldfire__
-	exg	d4,d5
-	movew	d4,d6
-	subw	d5,d6		| keep d5 (largest exponent) in d4
-	exg	d4,a2
-	exg	d5,a3
-#else
-	movel	d5,d6
-	movel	d4,d5
-	movel	d6,d4
-	subl	d5,d6
-	movel	d4,a4
-	movel	a2,d4
-	movel	a4,a2
-	movel	d5,a4
-	movel	a3,d5
-	movel	a4,a3
-#endif
-| if difference is too large we don't shift (actually, we can just exit) '
-#ifndef __mcoldfire__
-	cmpw	IMM (DBL_MANT_DIG+2),d6
-#else
-	cmpl	IMM (DBL_MANT_DIG+2),d6
-#endif
-	bge	Ladddf$a$small
-#ifndef __mcoldfire__
-	cmpw	IMM (32),d6	| if difference >= 32, shift by longs
-#else
-	cmpl	IMM (32),d6	| if difference >= 32, shift by longs
-#endif
-	bge	5f
-2:
-#ifndef __mcoldfire__
-	cmpw	IMM (16),d6	| if difference >= 16, shift by words	
-#else
-	cmpl	IMM (16),d6	| if difference >= 16, shift by words	
-#endif
-	bge	6f
-	bra	3f		| enter dbra loop
-
-4:
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	roxrl	IMM (1),d2
-	roxrl	IMM (1),d3
-#else
-	lsrl	IMM (1),d3
-	btst	IMM (0),d2
-	beq	10f
-	bset	IMM (31),d3
-10:	lsrl	IMM (1),d2
-	btst	IMM (0),d1
-	beq	11f
-	bset	IMM (31),d2
-11:	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	12f
-	bset	IMM (31),d1
-12:	lsrl	IMM (1),d0
-#endif
-3:
-#ifndef __mcoldfire__
-	dbra	d6,4b
-#else
-	subql	IMM (1),d6
-	bpl	4b
-#endif
-	movel	IMM (0),d7
-	movel	d7,d6
-	bra	Ladddf$4
-5:
-	movel	d2,d3
-	movel	d1,d2
-	movel	d0,d1
-	movel	IMM (0),d0
-#ifndef __mcoldfire__
-	subw	IMM (32),d6
-#else
-	subl	IMM (32),d6
-#endif
-	bra	2b
-6:
-	movew	d2,d3
-	swap	d3
-	movew	d1,d2
-	swap	d2
-	movew	d0,d1
-	swap	d1
-	movew	IMM (0),d0
-	swap	d0
-#ifndef __mcoldfire__
-	subw	IMM (16),d6
-#else
-	subl	IMM (16),d6
-#endif
-	bra	3b
-Ladddf$3:
-#ifndef __mcoldfire__
-	exg	d4,a2	
-	exg	d5,a3
-#else
-	movel	d4,a4
-	movel	a2,d4
-	movel	a4,a2
-	movel	d5,a4
-	movel	a3,d5
-	movel	a4,a3
-#endif
-Ladddf$4:	
-| Now we have the numbers in d0--d3 and d4--d7, the exponent in a2, and
-| the signs in a4.
-
-| Here we have to decide whether to add or subtract the numbers:
-#ifndef __mcoldfire__
-	exg	d7,a0		| get the signs 
-	exg	d6,a3		| a3 is free to be used
-#else
-	movel	d7,a4
-	movel	a0,d7
-	movel	a4,a0
-	movel	d6,a4
-	movel	a3,d6
-	movel	a4,a3
-#endif
-	movel	d7,d6		|
-	movew	IMM (0),d7	| get a's sign in d7 '
-	swap	d6              |
-	movew	IMM (0),d6	| and b's sign in d6 '
-	eorl	d7,d6		| compare the signs
-	bmi	Lsubdf$0	| if the signs are different we have 
-				| to subtract
-#ifndef __mcoldfire__
-	exg	d7,a0		| else we add the numbers
-	exg	d6,a3		|
-#else
-	movel	d7,a4
-	movel	a0,d7
-	movel	a4,a0
-	movel	d6,a4
-	movel	a3,d6
-	movel	a4,a3
-#endif
-	addl	d7,d3		|
-	addxl	d6,d2		|
-	addxl	d5,d1		| 
-	addxl	d4,d0           |
-
-	movel	a2,d4		| return exponent to d4
-	movel	a0,d7		| 
-	andl	IMM (0x80000000),d7 | d7 now has the sign
-
-#ifndef __mcoldfire__
-	moveml	sp@+,a2-a3	
-#else
-	movel	sp@+,a4	
-	movel	sp@+,a3	
-	movel	sp@+,a2	
-#endif
-
-| Before rounding normalize so bit #DBL_MANT_DIG is set (we will consider
-| the case of denormalized numbers in the rounding routine itself).
-| As in the addition (not in the subtraction!) we could have set 
-| one more bit we check this:
-	btst	IMM (DBL_MANT_DIG+1),d0	
-	beq	1f
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	roxrl	IMM (1),d2
-	roxrl	IMM (1),d3
-	addw	IMM (1),d4
-#else
-	lsrl	IMM (1),d3
-	btst	IMM (0),d2
-	beq	10f
-	bset	IMM (31),d3
-10:	lsrl	IMM (1),d2
-	btst	IMM (0),d1
-	beq	11f
-	bset	IMM (31),d2
-11:	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	12f
-	bset	IMM (31),d1
-12:	lsrl	IMM (1),d0
-	addl	IMM (1),d4
-#endif
-1:
-	lea	pc@(Ladddf$5),a0 | to return from rounding routine
-	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
-#ifdef __mcoldfire__
-	clrl	d6
-#endif
-	movew	a1@(6),d6	| rounding mode in d6
-	beq	Lround$to$nearest
-#ifndef __mcoldfire__
-	cmpw	IMM (ROUND_TO_PLUS),d6
-#else
-	cmpl	IMM (ROUND_TO_PLUS),d6
-#endif
-	bhi	Lround$to$minus
-	blt	Lround$to$zero
-	bra	Lround$to$plus
-Ladddf$5:
-| Put back the exponent and check for overflow
-#ifndef __mcoldfire__
-	cmpw	IMM (0x7ff),d4	| is the exponent big?
-#else
-	cmpl	IMM (0x7ff),d4	| is the exponent big?
-#endif
-	bge	1f
-	bclr	IMM (DBL_MANT_DIG-1),d0
-#ifndef __mcoldfire__
-	lslw	IMM (4),d4	| put exponent back into position
-#else
-	lsll	IMM (4),d4	| put exponent back into position
-#endif
-	swap	d0		| 
-#ifndef __mcoldfire__
-	orw	d4,d0		|
-#else
-	orl	d4,d0		|
-#endif
-	swap	d0		|
-	bra	Ladddf$ret
-1:
-	moveq	IMM (ADD),d5
-	bra	Ld$overflow
-
-Lsubdf$0:
-| Here we do the subtraction.
-#ifndef __mcoldfire__
-	exg	d7,a0		| put sign back in a0
-	exg	d6,a3		|
-#else
-	movel	d7,a4
-	movel	a0,d7
-	movel	a4,a0
-	movel	d6,a4
-	movel	a3,d6
-	movel	a4,a3
-#endif
-	subl	d7,d3		|
-	subxl	d6,d2		|
-	subxl	d5,d1		|
-	subxl	d4,d0		|
-	beq	Ladddf$ret$1	| if zero just exit
-	bpl	1f		| if positive skip the following
-	movel	a0,d7		|
-	bchg	IMM (31),d7	| change sign bit in d7
-	movel	d7,a0		|
-	negl	d3		|
-	negxl	d2		|
-	negxl	d1              | and negate result
-	negxl	d0              |
-1:	
-	movel	a2,d4		| return exponent to d4
-	movel	a0,d7
-	andl	IMM (0x80000000),d7 | isolate sign bit
-#ifndef __mcoldfire__
-	moveml	sp@+,a2-a3	|
-#else
-	movel	sp@+,a4
-	movel	sp@+,a3
-	movel	sp@+,a2
-#endif
-
-| Before rounding normalize so bit #DBL_MANT_DIG is set (we will consider
-| the case of denormalized numbers in the rounding routine itself).
-| As in the addition (not in the subtraction!) we could have set 
-| one more bit we check this:
-	btst	IMM (DBL_MANT_DIG+1),d0	
-	beq	1f
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	roxrl	IMM (1),d2
-	roxrl	IMM (1),d3
-	addw	IMM (1),d4
-#else
-	lsrl	IMM (1),d3
-	btst	IMM (0),d2
-	beq	10f
-	bset	IMM (31),d3
-10:	lsrl	IMM (1),d2
-	btst	IMM (0),d1
-	beq	11f
-	bset	IMM (31),d2
-11:	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	12f
-	bset	IMM (31),d1
-12:	lsrl	IMM (1),d0
-	addl	IMM (1),d4
-#endif
-1:
-	lea	pc@(Lsubdf$1),a0 | to return from rounding routine
-	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
-#ifdef __mcoldfire__
-	clrl	d6
-#endif
-	movew	a1@(6),d6	| rounding mode in d6
-	beq	Lround$to$nearest
-#ifndef __mcoldfire__
-	cmpw	IMM (ROUND_TO_PLUS),d6
-#else
-	cmpl	IMM (ROUND_TO_PLUS),d6
-#endif
-	bhi	Lround$to$minus
-	blt	Lround$to$zero
-	bra	Lround$to$plus
-Lsubdf$1:
-| Put back the exponent and sign (we don't have overflow). '
-	bclr	IMM (DBL_MANT_DIG-1),d0	
-#ifndef __mcoldfire__
-	lslw	IMM (4),d4	| put exponent back into position
-#else
-	lsll	IMM (4),d4	| put exponent back into position
-#endif
-	swap	d0		| 
-#ifndef __mcoldfire__
-	orw	d4,d0		|
-#else
-	orl	d4,d0		|
-#endif
-	swap	d0		|
-	bra	Ladddf$ret
-
-| If one of the numbers was too small (difference of exponents >= 
-| DBL_MANT_DIG+1) we return the other (and now we don't have to '
-| check for finiteness or zero).
-Ladddf$a$small:
-#ifndef __mcoldfire__
-	moveml	sp@+,a2-a3	
-#else
-	movel	sp@+,a4
-	movel	sp@+,a3
-	movel	sp@+,a2
-#endif
-	movel	a6@(16),d0
-	movel	a6@(20),d1
-	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7	| restore data registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6		| and return
-	rts
-
-Ladddf$b$small:
-#ifndef __mcoldfire__
-	moveml	sp@+,a2-a3	
-#else
-	movel	sp@+,a4	
-	movel	sp@+,a3	
-	movel	sp@+,a2	
-#endif
-	movel	a6@(8),d0
-	movel	a6@(12),d1
-	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7	| restore data registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6		| and return
-	rts
-
-Ladddf$a$den:
-	movel	d7,d4		| d7 contains 0x00200000
-	bra	Ladddf$1
-
-Ladddf$b$den:
-	movel	d7,d5           | d7 contains 0x00200000
-	notl	d6
-	bra	Ladddf$2
-
-Ladddf$b:
-| Return b (if a is zero)
-	movel	d2,d0
-	movel	d3,d1
-	bne	1f			| Check if b is -0
-	cmpl	IMM (0x80000000),d0
-	bne	1f
-	andl	IMM (0x80000000),d7	| Use the sign of a
-	clrl	d0
-	bra	Ladddf$ret
-Ladddf$a:
-	movel	a6@(8),d0
-	movel	a6@(12),d1
-1:
-	moveq	IMM (ADD),d5
-| Check for NaN and +/-INFINITY.
-	movel	d0,d7         		|
-	andl	IMM (0x80000000),d7	|
-	bclr	IMM (31),d0		|
-	cmpl	IMM (0x7ff00000),d0	|
-	bge	2f			|
-	movel	d0,d0           	| check for zero, since we don't  '
-	bne	Ladddf$ret		| want to return -0 by mistake
-	bclr	IMM (31),d7		|
-	bra	Ladddf$ret		|
-2:
-	andl	IMM (0x000fffff),d0	| check for NaN (nonzero fraction)
-	orl	d1,d0			|
-	bne	Ld$inop         	|
-	bra	Ld$infty		|
-	
-Ladddf$ret$1:
-#ifndef __mcoldfire__
-	moveml	sp@+,a2-a3	| restore regs and exit
-#else
-	movel	sp@+,a4
-	movel	sp@+,a3
-	movel	sp@+,a2
-#endif
-
-Ladddf$ret:
-| Normal exit.
-	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-	orl	d7,d0		| put sign bit back
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-
-Ladddf$ret$den:
-| Return a denormalized number.
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0	| shift right once more
-	roxrl	IMM (1),d1	|
-#else
-	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	10f
-	bset	IMM (31),d1
-10:	lsrl	IMM (1),d0
-#endif
-	bra	Ladddf$ret
-
-Ladddf$nf:
-	moveq	IMM (ADD),d5
-| This could be faster but it is not worth the effort, since it is not
-| executed very often. We sacrifice speed for clarity here.
-	movel	a6@(8),d0	| get the numbers back (remember that we
-	movel	a6@(12),d1	| did some processing already)
-	movel	a6@(16),d2	| 
-	movel	a6@(20),d3	| 
-	movel	IMM (0x7ff00000),d4 | useful constant (INFINITY)
-	movel	d0,d7		| save sign bits
-	movel	d2,d6		| 
-	bclr	IMM (31),d0	| clear sign bits
-	bclr	IMM (31),d2	| 
-| We know that one of them is either NaN of +/-INFINITY
-| Check for NaN (if either one is NaN return NaN)
-	cmpl	d4,d0		| check first a (d0)
-	bhi	Ld$inop		| if d0 > 0x7ff00000 or equal and
-	bne	2f
-	tstl	d1		| d1 > 0, a is NaN
-	bne	Ld$inop		| 
-2:	cmpl	d4,d2		| check now b (d1)
-	bhi	Ld$inop		| 
-	bne	3f
-	tstl	d3		| 
-	bne	Ld$inop		| 
-3:
-| Now comes the check for +/-INFINITY. We know that both are (maybe not
-| finite) numbers, but we have to check if both are infinite whether we
-| are adding or subtracting them.
-	eorl	d7,d6		| to check sign bits
-	bmi	1f
-	andl	IMM (0x80000000),d7 | get (common) sign bit
-	bra	Ld$infty
-1:
-| We know one (or both) are infinite, so we test for equality between the
-| two numbers (if they are equal they have to be infinite both, so we
-| return NaN).
-	cmpl	d2,d0		| are both infinite?
-	bne	1f		| if d0 <> d2 they are not equal
-	cmpl	d3,d1		| if d0 == d2 test d3 and d1
-	beq	Ld$inop		| if equal return NaN
-1:	
-	andl	IMM (0x80000000),d7 | get a's sign bit '
-	cmpl	d4,d0		| test now for infinity
-	beq	Ld$infty	| if a is INFINITY return with this sign
-	bchg	IMM (31),d7	| else we know b is INFINITY and has
-	bra	Ld$infty	| the opposite sign
-
-|=============================================================================
-|                              __muldf3
-|=============================================================================
-
-| double __muldf3(double, double);
-	FUNC(__muldf3)
-SYM (__muldf3):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)
-	moveml	d2-d7,sp@-
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	movel	a6@(8),d0		| get a into d0-d1
-	movel	a6@(12),d1		| 
-	movel	a6@(16),d2		| and b into d2-d3
-	movel	a6@(20),d3		|
-	movel	d0,d7			| d7 will hold the sign of the product
-	eorl	d2,d7			|
-	andl	IMM (0x80000000),d7	|
-	movel	d7,a0			| save sign bit into a0 
-	movel	IMM (0x7ff00000),d7	| useful constant (+INFINITY)
-	movel	d7,d6			| another (mask for fraction)
-	notl	d6			|
-	bclr	IMM (31),d0		| get rid of a's sign bit '
-	movel	d0,d4			| 
-	orl	d1,d4			| 
-	beq	Lmuldf$a$0		| branch if a is zero
-	movel	d0,d4			|
-	bclr	IMM (31),d2		| get rid of b's sign bit '
-	movel	d2,d5			|
-	orl	d3,d5			| 
-	beq	Lmuldf$b$0		| branch if b is zero
-	movel	d2,d5			| 
-	cmpl	d7,d0			| is a big?
-	bhi	Lmuldf$inop		| if a is NaN return NaN
-	beq	Lmuldf$a$nf		| we still have to check d1 and b ...
-	cmpl	d7,d2			| now compare b with INFINITY
-	bhi	Lmuldf$inop		| is b NaN?
-	beq	Lmuldf$b$nf 		| we still have to check d3 ...
-| Here we have both numbers finite and nonzero (and with no sign bit).
-| Now we get the exponents into d4 and d5.
-	andl	d7,d4			| isolate exponent in d4
-	beq	Lmuldf$a$den		| if exponent zero, have denormalized
-	andl	d6,d0			| isolate fraction
-	orl	IMM (0x00100000),d0	| and put hidden bit back
-	swap	d4			| I like exponents in the first byte
-#ifndef __mcoldfire__
-	lsrw	IMM (4),d4		| 
-#else
-	lsrl	IMM (4),d4		| 
-#endif
-Lmuldf$1:			
-	andl	d7,d5			|
-	beq	Lmuldf$b$den		|
-	andl	d6,d2			|
-	orl	IMM (0x00100000),d2	| and put hidden bit back
-	swap	d5			|
-#ifndef __mcoldfire__
-	lsrw	IMM (4),d5		|
-#else
-	lsrl	IMM (4),d5		|
-#endif
-Lmuldf$2:				|
-#ifndef __mcoldfire__
-	addw	d5,d4			| add exponents
-	subw	IMM (D_BIAS+1),d4	| and subtract bias (plus one)
-#else
-	addl	d5,d4			| add exponents
-	subl	IMM (D_BIAS+1),d4	| and subtract bias (plus one)
-#endif
-
-| We are now ready to do the multiplication. The situation is as follows:
-| both a and b have bit 52 ( bit 20 of d0 and d2) set (even if they were 
-| denormalized to start with!), which means that in the product bit 104 
-| (which will correspond to bit 8 of the fourth long) is set.
-
-| Here we have to do the product.
-| To do it we have to juggle the registers back and forth, as there are not
-| enough to keep everything in them. So we use the address registers to keep
-| some intermediate data.
-
-#ifndef __mcoldfire__
-	moveml	a2-a3,sp@-	| save a2 and a3 for temporary use
-#else
-	movel	a2,sp@-
-	movel	a3,sp@-
-	movel	a4,sp@-
-#endif
-	movel	IMM (0),a2	| a2 is a null register
-	movel	d4,a3		| and a3 will preserve the exponent
-
-| First, shift d2-d3 so bit 20 becomes bit 31:
-#ifndef __mcoldfire__
-	rorl	IMM (5),d2	| rotate d2 5 places right
-	swap	d2		| and swap it
-	rorl	IMM (5),d3	| do the same thing with d3
-	swap	d3		|
-	movew	d3,d6		| get the rightmost 11 bits of d3
-	andw	IMM (0x07ff),d6	|
-	orw	d6,d2		| and put them into d2
-	andw	IMM (0xf800),d3	| clear those bits in d3
-#else
-	moveq	IMM (11),d7	| left shift d2 11 bits
-	lsll	d7,d2
-	movel	d3,d6		| get a copy of d3
-	lsll	d7,d3		| left shift d3 11 bits
-	andl	IMM (0xffe00000),d6 | get the top 11 bits of d3
-	moveq	IMM (21),d7	| right shift them 21 bits
-	lsrl	d7,d6
-	orl	d6,d2		| stick them at the end of d2
-#endif
-
-	movel	d2,d6		| move b into d6-d7
-	movel	d3,d7           | move a into d4-d5
-	movel	d0,d4           | and clear d0-d1-d2-d3 (to put result)
-	movel	d1,d5           |
-	movel	IMM (0),d3	|
-	movel	d3,d2           |
-	movel	d3,d1           |
-	movel	d3,d0	        |
-
-| We use a1 as counter:	
-	movel	IMM (DBL_MANT_DIG-1),a1		
-#ifndef __mcoldfire__
-	exg	d7,a1
-#else
-	movel	d7,a4
-	movel	a1,d7
-	movel	a4,a1
-#endif
-
-1:
-#ifndef __mcoldfire__
-	exg	d7,a1		| put counter back in a1
-#else
-	movel	d7,a4
-	movel	a1,d7
-	movel	a4,a1
-#endif
-	addl	d3,d3		| shift sum once left
-	addxl	d2,d2           |
-	addxl	d1,d1           |
-	addxl	d0,d0           |
-	addl	d7,d7		|
-	addxl	d6,d6		|
-	bcc	2f		| if bit clear skip the following
-#ifndef __mcoldfire__
-	exg	d7,a2		|
-#else
-	movel	d7,a4
-	movel	a2,d7
-	movel	a4,a2
-#endif
-	addl	d5,d3		| else add a to the sum
-	addxl	d4,d2		|
-	addxl	d7,d1		|
-	addxl	d7,d0		|
-#ifndef __mcoldfire__
-	exg	d7,a2		| 
-#else
-	movel	d7,a4
-	movel	a2,d7
-	movel	a4,a2
-#endif
-2:
-#ifndef __mcoldfire__
-	exg	d7,a1		| put counter in d7
-	dbf	d7,1b		| decrement and branch
-#else
-	movel	d7,a4
-	movel	a1,d7
-	movel	a4,a1
-	subql	IMM (1),d7
-	bpl	1b
-#endif
-
-	movel	a3,d4		| restore exponent
-#ifndef __mcoldfire__
-	moveml	sp@+,a2-a3
-#else
-	movel	sp@+,a4
-	movel	sp@+,a3
-	movel	sp@+,a2
-#endif
-
-| Now we have the product in d0-d1-d2-d3, with bit 8 of d0 set. The 
-| first thing to do now is to normalize it so bit 8 becomes bit 
-| DBL_MANT_DIG-32 (to do the rounding); later we will shift right.
-	swap	d0
-	swap	d1
-	movew	d1,d0
-	swap	d2
-	movew	d2,d1
-	swap	d3
-	movew	d3,d2
-	movew	IMM (0),d3
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	roxrl	IMM (1),d2
-	roxrl	IMM (1),d3
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	roxrl	IMM (1),d2
-	roxrl	IMM (1),d3
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	roxrl	IMM (1),d2
-	roxrl	IMM (1),d3
-#else
-	moveq	IMM (29),d6
-	lsrl	IMM (3),d3
-	movel	d2,d7
-	lsll	d6,d7
-	orl	d7,d3
-	lsrl	IMM (3),d2
-	movel	d1,d7
-	lsll	d6,d7
-	orl	d7,d2
-	lsrl	IMM (3),d1
-	movel	d0,d7
-	lsll	d6,d7
-	orl	d7,d1
-	lsrl	IMM (3),d0
-#endif
-	
-| Now round, check for over- and underflow, and exit.
-	movel	a0,d7		| get sign bit back into d7
-	moveq	IMM (MULTIPLY),d5
-
-	btst	IMM (DBL_MANT_DIG+1-32),d0
-	beq	Lround$exit
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	addw	IMM (1),d4
-#else
-	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	10f
-	bset	IMM (31),d1
-10:	lsrl	IMM (1),d0
-	addl	IMM (1),d4
-#endif
-	bra	Lround$exit
-
-Lmuldf$inop:
-	moveq	IMM (MULTIPLY),d5
-	bra	Ld$inop
-
-Lmuldf$b$nf:
-	moveq	IMM (MULTIPLY),d5
-	movel	a0,d7		| get sign bit back into d7
-	tstl	d3		| we know d2 == 0x7ff00000, so check d3
-	bne	Ld$inop		| if d3 <> 0 b is NaN
-	bra	Ld$overflow	| else we have overflow (since a is finite)
-
-Lmuldf$a$nf:
-	moveq	IMM (MULTIPLY),d5
-	movel	a0,d7		| get sign bit back into d7
-	tstl	d1		| we know d0 == 0x7ff00000, so check d1
-	bne	Ld$inop		| if d1 <> 0 a is NaN
-	bra	Ld$overflow	| else signal overflow
-
-| If either number is zero return zero, unless the other is +/-INFINITY or
-| NaN, in which case we return NaN.
-Lmuldf$b$0:
-	moveq	IMM (MULTIPLY),d5
-#ifndef __mcoldfire__
-	exg	d2,d0		| put b (==0) into d0-d1
-	exg	d3,d1		| and a (with sign bit cleared) into d2-d3
-	movel	a0,d0		| set result sign
-#else
-	movel	d0,d2		| put a into d2-d3
-	movel	d1,d3
-	movel	a0,d0		| put result zero into d0-d1
-	movq	IMM(0),d1
-#endif
-	bra	1f
-Lmuldf$a$0:
-	movel	a0,d0		| set result sign
-	movel	a6@(16),d2	| put b into d2-d3 again
-	movel	a6@(20),d3	|
-	bclr	IMM (31),d2	| clear sign bit
-1:	cmpl	IMM (0x7ff00000),d2 | check for non-finiteness
-	bge	Ld$inop		| in case NaN or +/-INFINITY return NaN
-	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-
-| If a number is denormalized we put an exponent of 1 but do not put the 
-| hidden bit back into the fraction; instead we shift left until bit 21
-| (the hidden bit) is set, adjusting the exponent accordingly. We do this
-| to ensure that the product of the fractions is close to 1.
-Lmuldf$a$den:
-	movel	IMM (1),d4
-	andl	d6,d0
-1:	addl	d1,d1           | shift a left until bit 20 is set
-	addxl	d0,d0		|
-#ifndef __mcoldfire__
-	subw	IMM (1),d4	| and adjust exponent
-#else
-	subl	IMM (1),d4	| and adjust exponent
-#endif
-	btst	IMM (20),d0	|
-	bne	Lmuldf$1        |
-	bra	1b
-
-Lmuldf$b$den:
-	movel	IMM (1),d5
-	andl	d6,d2
-1:	addl	d3,d3		| shift b left until bit 20 is set
-	addxl	d2,d2		|
-#ifndef __mcoldfire__
-	subw	IMM (1),d5	| and adjust exponent
-#else
-	subql	IMM (1),d5	| and adjust exponent
-#endif
-	btst	IMM (20),d2	|
-	bne	Lmuldf$2	|
-	bra	1b
-
-
-|=============================================================================
-|                              __divdf3
-|=============================================================================
-
-| double __divdf3(double, double);
-	FUNC(__divdf3)
-SYM (__divdf3):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)
-	moveml	d2-d7,sp@-
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	movel	a6@(8),d0	| get a into d0-d1
-	movel	a6@(12),d1	| 
-	movel	a6@(16),d2	| and b into d2-d3
-	movel	a6@(20),d3	|
-	movel	d0,d7		| d7 will hold the sign of the result
-	eorl	d2,d7		|
-	andl	IMM (0x80000000),d7
-	movel	d7,a0		| save sign into a0
-	movel	IMM (0x7ff00000),d7 | useful constant (+INFINITY)
-	movel	d7,d6		| another (mask for fraction)
-	notl	d6		|
-	bclr	IMM (31),d0	| get rid of a's sign bit '
-	movel	d0,d4		|
-	orl	d1,d4		|
-	beq	Ldivdf$a$0	| branch if a is zero
-	movel	d0,d4		|
-	bclr	IMM (31),d2	| get rid of b's sign bit '
-	movel	d2,d5		|
-	orl	d3,d5		|
-	beq	Ldivdf$b$0	| branch if b is zero
-	movel	d2,d5
-	cmpl	d7,d0		| is a big?
-	bhi	Ldivdf$inop	| if a is NaN return NaN
-	beq	Ldivdf$a$nf	| if d0 == 0x7ff00000 we check d1
-	cmpl	d7,d2		| now compare b with INFINITY 
-	bhi	Ldivdf$inop	| if b is NaN return NaN
-	beq	Ldivdf$b$nf	| if d2 == 0x7ff00000 we check d3
-| Here we have both numbers finite and nonzero (and with no sign bit).
-| Now we get the exponents into d4 and d5 and normalize the numbers to
-| ensure that the ratio of the fractions is around 1. We do this by
-| making sure that both numbers have bit #DBL_MANT_DIG-32-1 (hidden bit)
-| set, even if they were denormalized to start with.
-| Thus, the result will satisfy: 2 > result > 1/2.
-	andl	d7,d4		| and isolate exponent in d4
-	beq	Ldivdf$a$den	| if exponent is zero we have a denormalized
-	andl	d6,d0		| and isolate fraction
-	orl	IMM (0x00100000),d0 | and put hidden bit back
-	swap	d4		| I like exponents in the first byte
-#ifndef __mcoldfire__
-	lsrw	IMM (4),d4	| 
-#else
-	lsrl	IMM (4),d4	| 
-#endif
-Ldivdf$1:			| 
-	andl	d7,d5		|
-	beq	Ldivdf$b$den	|
-	andl	d6,d2		|
-	orl	IMM (0x00100000),d2
-	swap	d5		|
-#ifndef __mcoldfire__
-	lsrw	IMM (4),d5	|
-#else
-	lsrl	IMM (4),d5	|
-#endif
-Ldivdf$2:			|
-#ifndef __mcoldfire__
-	subw	d5,d4		| subtract exponents
-	addw	IMM (D_BIAS),d4	| and add bias
-#else
-	subl	d5,d4		| subtract exponents
-	addl	IMM (D_BIAS),d4	| and add bias
-#endif
-
-| We are now ready to do the division. We have prepared things in such a way
-| that the ratio of the fractions will be less than 2 but greater than 1/2.
-| At this point the registers in use are:
-| d0-d1	hold a (first operand, bit DBL_MANT_DIG-32=0, bit 
-| DBL_MANT_DIG-1-32=1)
-| d2-d3	hold b (second operand, bit DBL_MANT_DIG-32=1)
-| d4	holds the difference of the exponents, corrected by the bias
-| a0	holds the sign of the ratio
-
-| To do the rounding correctly we need to keep information about the
-| nonsignificant bits. One way to do this would be to do the division
-| using four registers; another is to use two registers (as originally
-| I did), but use a sticky bit to preserve information about the 
-| fractional part. Note that we can keep that info in a1, which is not
-| used.
-	movel	IMM (0),d6	| d6-d7 will hold the result
-	movel	d6,d7		| 
-	movel	IMM (0),a1	| and a1 will hold the sticky bit
-
-	movel	IMM (DBL_MANT_DIG-32+1),d5	
-	
-1:	cmpl	d0,d2		| is a < b?
-	bhi	3f		| if b > a skip the following
-	beq	4f		| if d0==d2 check d1 and d3
-2:	subl	d3,d1		| 
-	subxl	d2,d0		| a <-- a - b
-	bset	d5,d6		| set the corresponding bit in d6
-3:	addl	d1,d1		| shift a by 1
-	addxl	d0,d0		|
-#ifndef __mcoldfire__
-	dbra	d5,1b		| and branch back
-#else
-	subql	IMM (1), d5
-	bpl	1b
-#endif
-	bra	5f			
-4:	cmpl	d1,d3		| here d0==d2, so check d1 and d3
-	bhi	3b		| if d1 > d2 skip the subtraction
-	bra	2b		| else go do it
-5:
-| Here we have to start setting the bits in the second long.
-	movel	IMM (31),d5	| again d5 is counter
-
-1:	cmpl	d0,d2		| is a < b?
-	bhi	3f		| if b > a skip the following
-	beq	4f		| if d0==d2 check d1 and d3
-2:	subl	d3,d1		| 
-	subxl	d2,d0		| a <-- a - b
-	bset	d5,d7		| set the corresponding bit in d7
-3:	addl	d1,d1		| shift a by 1
-	addxl	d0,d0		|
-#ifndef __mcoldfire__
-	dbra	d5,1b		| and branch back
-#else
-	subql	IMM (1), d5
-	bpl	1b
-#endif
-	bra	5f			
-4:	cmpl	d1,d3		| here d0==d2, so check d1 and d3
-	bhi	3b		| if d1 > d2 skip the subtraction
-	bra	2b		| else go do it
-5:
-| Now go ahead checking until we hit a one, which we store in d2.
-	movel	IMM (DBL_MANT_DIG),d5
-1:	cmpl	d2,d0		| is a < b?
-	bhi	4f		| if b < a, exit
-	beq	3f		| if d0==d2 check d1 and d3
-2:	addl	d1,d1		| shift a by 1
-	addxl	d0,d0		|
-#ifndef __mcoldfire__
-	dbra	d5,1b		| and branch back
-#else
-	subql	IMM (1), d5
-	bpl	1b
-#endif
-	movel	IMM (0),d2	| here no sticky bit was found
-	movel	d2,d3
-	bra	5f			
-3:	cmpl	d1,d3		| here d0==d2, so check d1 and d3
-	bhi	2b		| if d1 > d2 go back
-4:
-| Here put the sticky bit in d2-d3 (in the position which actually corresponds
-| to it; if you don't do this the algorithm loses in some cases). '
-	movel	IMM (0),d2
-	movel	d2,d3
-#ifndef __mcoldfire__
-	subw	IMM (DBL_MANT_DIG),d5
-	addw	IMM (63),d5
-	cmpw	IMM (31),d5
-#else
-	subl	IMM (DBL_MANT_DIG),d5
-	addl	IMM (63),d5
-	cmpl	IMM (31),d5
-#endif
-	bhi	2f
-1:	bset	d5,d3
-	bra	5f
-#ifndef __mcoldfire__
-	subw	IMM (32),d5
-#else
-	subl	IMM (32),d5
-#endif
-2:	bset	d5,d2
-5:
-| Finally we are finished! Move the longs in the address registers to
-| their final destination:
-	movel	d6,d0
-	movel	d7,d1
-	movel	IMM (0),d3
-
-| Here we have finished the division, with the result in d0-d1-d2-d3, with
-| 2^21 <= d6 < 2^23. Thus bit 23 is not set, but bit 22 could be set.
-| If it is not, then definitely bit 21 is set. Normalize so bit 22 is
-| not set:
-	btst	IMM (DBL_MANT_DIG-32+1),d0
-	beq	1f
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	roxrl	IMM (1),d2
-	roxrl	IMM (1),d3
-	addw	IMM (1),d4
-#else
-	lsrl	IMM (1),d3
-	btst	IMM (0),d2
-	beq	10f
-	bset	IMM (31),d3
-10:	lsrl	IMM (1),d2
-	btst	IMM (0),d1
-	beq	11f
-	bset	IMM (31),d2
-11:	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	12f
-	bset	IMM (31),d1
-12:	lsrl	IMM (1),d0
-	addl	IMM (1),d4
-#endif
-1:
-| Now round, check for over- and underflow, and exit.
-	movel	a0,d7		| restore sign bit to d7
-	moveq	IMM (DIVIDE),d5
-	bra	Lround$exit
-
-Ldivdf$inop:
-	moveq	IMM (DIVIDE),d5
-	bra	Ld$inop
-
-Ldivdf$a$0:
-| If a is zero check to see whether b is zero also. In that case return
-| NaN; then check if b is NaN, and return NaN also in that case. Else
-| return a properly signed zero.
-	moveq	IMM (DIVIDE),d5
-	bclr	IMM (31),d2	|
-	movel	d2,d4		| 
-	orl	d3,d4		| 
-	beq	Ld$inop		| if b is also zero return NaN
-	cmpl	IMM (0x7ff00000),d2 | check for NaN
-	bhi	Ld$inop		| 
-	blt	1f		|
-	tstl	d3		|
-	bne	Ld$inop		|
-1:	movel	a0,d0		| else return signed zero
-	moveq	IMM(0),d1	| 
-	PICLEA	SYM (_fpCCR),a0	| clear exception flags
-	movew	IMM (0),a0@	|
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7	| 
-#else
-	moveml	sp@,d2-d7	| 
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6		| 
-	rts			| 	
-
-Ldivdf$b$0:
-	moveq	IMM (DIVIDE),d5
-| If we got here a is not zero. Check if a is NaN; in that case return NaN,
-| else return +/-INFINITY. Remember that a is in d0 with the sign bit 
-| cleared already.
-	movel	a0,d7		| put a's sign bit back in d7 '
-	cmpl	IMM (0x7ff00000),d0 | compare d0 with INFINITY
-	bhi	Ld$inop		| if larger it is NaN
-	tstl	d1		| 
-	bne	Ld$inop		| 
-	bra	Ld$div$0	| else signal DIVIDE_BY_ZERO
-
-Ldivdf$b$nf:
-	moveq	IMM (DIVIDE),d5
-| If d2 == 0x7ff00000 we have to check d3.
-	tstl	d3		|
-	bne	Ld$inop		| if d3 <> 0, b is NaN
-	bra	Ld$underflow	| else b is +/-INFINITY, so signal underflow
-
-Ldivdf$a$nf:
-	moveq	IMM (DIVIDE),d5
-| If d0 == 0x7ff00000 we have to check d1.
-	tstl	d1		|
-	bne	Ld$inop		| if d1 <> 0, a is NaN
-| If a is INFINITY we have to check b
-	cmpl	d7,d2		| compare b with INFINITY 
-	bge	Ld$inop		| if b is NaN or INFINITY return NaN
-	tstl	d3		|
-	bne	Ld$inop		| 
-	bra	Ld$overflow	| else return overflow
-
-| If a number is denormalized we put an exponent of 1 but do not put the 
-| bit back into the fraction.
-Ldivdf$a$den:
-	movel	IMM (1),d4
-	andl	d6,d0
-1:	addl	d1,d1		| shift a left until bit 20 is set
-	addxl	d0,d0
-#ifndef __mcoldfire__
-	subw	IMM (1),d4	| and adjust exponent
-#else
-	subl	IMM (1),d4	| and adjust exponent
-#endif
-	btst	IMM (DBL_MANT_DIG-32-1),d0
-	bne	Ldivdf$1
-	bra	1b
-
-Ldivdf$b$den:
-	movel	IMM (1),d5
-	andl	d6,d2
-1:	addl	d3,d3		| shift b left until bit 20 is set
-	addxl	d2,d2
-#ifndef __mcoldfire__
-	subw	IMM (1),d5	| and adjust exponent
-#else
-	subql	IMM (1),d5	| and adjust exponent
-#endif
-	btst	IMM (DBL_MANT_DIG-32-1),d2
-	bne	Ldivdf$2
-	bra	1b
-
-Lround$exit:
-| This is a common exit point for __muldf3 and __divdf3. When they enter
-| this point the sign of the result is in d7, the result in d0-d1, normalized
-| so that 2^21 <= d0 < 2^22, and the exponent is in the lower byte of d4.
-
-| First check for underlow in the exponent:
-#ifndef __mcoldfire__
-	cmpw	IMM (-DBL_MANT_DIG-1),d4		
-#else
-	cmpl	IMM (-DBL_MANT_DIG-1),d4		
-#endif
-	blt	Ld$underflow	
-| It could happen that the exponent is less than 1, in which case the 
-| number is denormalized. In this case we shift right and adjust the 
-| exponent until it becomes 1 or the fraction is zero (in the latter case 
-| we signal underflow and return zero).
-	movel	d7,a0		|
-	movel	IMM (0),d6	| use d6-d7 to collect bits flushed right
-	movel	d6,d7		| use d6-d7 to collect bits flushed right
-#ifndef __mcoldfire__
-	cmpw	IMM (1),d4	| if the exponent is less than 1 we 
-#else
-	cmpl	IMM (1),d4	| if the exponent is less than 1 we 
-#endif
-	bge	2f		| have to shift right (denormalize)
-1:
-#ifndef __mcoldfire__
-	addw	IMM (1),d4	| adjust the exponent
-	lsrl	IMM (1),d0	| shift right once 
-	roxrl	IMM (1),d1	|
-	roxrl	IMM (1),d2	|
-	roxrl	IMM (1),d3	|
-	roxrl	IMM (1),d6	| 
-	roxrl	IMM (1),d7	|
-	cmpw	IMM (1),d4	| is the exponent 1 already?
-#else
-	addl	IMM (1),d4	| adjust the exponent
-	lsrl	IMM (1),d7
-	btst	IMM (0),d6
-	beq	13f
-	bset	IMM (31),d7
-13:	lsrl	IMM (1),d6
-	btst	IMM (0),d3
-	beq	14f
-	bset	IMM (31),d6
-14:	lsrl	IMM (1),d3
-	btst	IMM (0),d2
-	beq	10f
-	bset	IMM (31),d3
-10:	lsrl	IMM (1),d2
-	btst	IMM (0),d1
-	beq	11f
-	bset	IMM (31),d2
-11:	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	12f
-	bset	IMM (31),d1
-12:	lsrl	IMM (1),d0
-	cmpl	IMM (1),d4	| is the exponent 1 already?
-#endif
-	beq	2f		| if not loop back
-	bra	1b              |
-	bra	Ld$underflow	| safety check, shouldn't execute '
-2:	orl	d6,d2		| this is a trick so we don't lose  '
-	orl	d7,d3		| the bits which were flushed right
-	movel	a0,d7		| get back sign bit into d7
-| Now call the rounding routine (which takes care of denormalized numbers):
-	lea	pc@(Lround$0),a0 | to return from rounding routine
-	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
-#ifdef __mcoldfire__
-	clrl	d6
-#endif
-	movew	a1@(6),d6	| rounding mode in d6
-	beq	Lround$to$nearest
-#ifndef __mcoldfire__
-	cmpw	IMM (ROUND_TO_PLUS),d6
-#else
-	cmpl	IMM (ROUND_TO_PLUS),d6
-#endif
-	bhi	Lround$to$minus
-	blt	Lround$to$zero
-	bra	Lround$to$plus
-Lround$0:
-| Here we have a correctly rounded result (either normalized or denormalized).
-
-| Here we should have either a normalized number or a denormalized one, and
-| the exponent is necessarily larger or equal to 1 (so we don't have to  '
-| check again for underflow!). We have to check for overflow or for a 
-| denormalized number (which also signals underflow).
-| Check for overflow (i.e., exponent >= 0x7ff).
-#ifndef __mcoldfire__
-	cmpw	IMM (0x07ff),d4
-#else
-	cmpl	IMM (0x07ff),d4
-#endif
-	bge	Ld$overflow
-| Now check for a denormalized number (exponent==0):
-	movew	d4,d4
-	beq	Ld$den
-1:
-| Put back the exponents and sign and return.
-#ifndef __mcoldfire__
-	lslw	IMM (4),d4	| exponent back to fourth byte
-#else
-	lsll	IMM (4),d4	| exponent back to fourth byte
-#endif
-	bclr	IMM (DBL_MANT_DIG-32-1),d0
-	swap	d0		| and put back exponent
-#ifndef __mcoldfire__
-	orw	d4,d0		| 
-#else
-	orl	d4,d0		| 
-#endif
-	swap	d0		|
-	orl	d7,d0		| and sign also
-
-	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-
-|=============================================================================
-|                              __negdf2
-|=============================================================================
-
-| double __negdf2(double, double);
-	FUNC(__negdf2)
-SYM (__negdf2):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)
-	moveml	d2-d7,sp@-
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	moveq	IMM (NEGATE),d5
-	movel	a6@(8),d0	| get number to negate in d0-d1
-	movel	a6@(12),d1	|
-	bchg	IMM (31),d0	| negate
-	movel	d0,d2		| make a positive copy (for the tests)
-	bclr	IMM (31),d2	|
-	movel	d2,d4		| check for zero
-	orl	d1,d4		|
-	beq	2f		| if zero (either sign) return +zero
-	cmpl	IMM (0x7ff00000),d2 | compare to +INFINITY
-	blt	1f		| if finite, return
-	bhi	Ld$inop		| if larger (fraction not zero) is NaN
-	tstl	d1		| if d2 == 0x7ff00000 check d1
-	bne	Ld$inop		|
-	movel	d0,d7		| else get sign and return INFINITY
-	andl	IMM (0x80000000),d7
-	bra	Ld$infty		
-1:	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-2:	bclr	IMM (31),d0
-	bra	1b
-
-|=============================================================================
-|                              __cmpdf2
-|=============================================================================
-
-GREATER =  1
-LESS    = -1
-EQUAL   =  0
-
-| int __cmpdf2_internal(double, double, int);
-SYM (__cmpdf2_internal):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)
-	moveml	d2-d7,sp@- 	| save registers
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	moveq	IMM (COMPARE),d5
-	movel	a6@(8),d0	| get first operand
-	movel	a6@(12),d1	|
-	movel	a6@(16),d2	| get second operand
-	movel	a6@(20),d3	|
-| First check if a and/or b are (+/-) zero and in that case clear
-| the sign bit.
-	movel	d0,d6		| copy signs into d6 (a) and d7(b)
-	bclr	IMM (31),d0	| and clear signs in d0 and d2
-	movel	d2,d7		|
-	bclr	IMM (31),d2	|
-	cmpl	IMM (0x7ff00000),d0 | check for a == NaN
-	bhi	Lcmpd$inop		| if d0 > 0x7ff00000, a is NaN
-	beq	Lcmpdf$a$nf	| if equal can be INFINITY, so check d1
-	movel	d0,d4		| copy into d4 to test for zero
-	orl	d1,d4		|
-	beq	Lcmpdf$a$0	|
-Lcmpdf$0:
-	cmpl	IMM (0x7ff00000),d2 | check for b == NaN
-	bhi	Lcmpd$inop		| if d2 > 0x7ff00000, b is NaN
-	beq	Lcmpdf$b$nf	| if equal can be INFINITY, so check d3
-	movel	d2,d4		|
-	orl	d3,d4		|
-	beq	Lcmpdf$b$0	|
-Lcmpdf$1:
-| Check the signs
-	eorl	d6,d7
-	bpl	1f
-| If the signs are not equal check if a >= 0
-	tstl	d6
-	bpl	Lcmpdf$a$gt$b	| if (a >= 0 && b < 0) => a > b
-	bmi	Lcmpdf$b$gt$a	| if (a < 0 && b >= 0) => a < b
-1:
-| If the signs are equal check for < 0
-	tstl	d6
-	bpl	1f
-| If both are negative exchange them
-#ifndef __mcoldfire__
-	exg	d0,d2
-	exg	d1,d3
-#else
-	movel	d0,d7
-	movel	d2,d0
-	movel	d7,d2
-	movel	d1,d7
-	movel	d3,d1
-	movel	d7,d3
-#endif
-1:
-| Now that they are positive we just compare them as longs (does this also
-| work for denormalized numbers?).
-	cmpl	d0,d2
-	bhi	Lcmpdf$b$gt$a	| |b| > |a|
-	bne	Lcmpdf$a$gt$b	| |b| < |a|
-| If we got here d0 == d2, so we compare d1 and d3.
-	cmpl	d1,d3
-	bhi	Lcmpdf$b$gt$a	| |b| > |a|
-	bne	Lcmpdf$a$gt$b	| |b| < |a|
-| If we got here a == b.
-	movel	IMM (EQUAL),d0
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7 	| put back the registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-Lcmpdf$a$gt$b:
-	movel	IMM (GREATER),d0
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7 	| put back the registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-Lcmpdf$b$gt$a:
-	movel	IMM (LESS),d0
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7 	| put back the registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-
-Lcmpdf$a$0:	
-	bclr	IMM (31),d6
-	bra	Lcmpdf$0
-Lcmpdf$b$0:
-	bclr	IMM (31),d7
-	bra	Lcmpdf$1
-
-Lcmpdf$a$nf:
-	tstl	d1
-	bne	Ld$inop
-	bra	Lcmpdf$0
-
-Lcmpdf$b$nf:
-	tstl	d3
-	bne	Ld$inop
-	bra	Lcmpdf$1
-
-Lcmpd$inop:
-	movl	a6@(24),d0
-	moveq	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
-	moveq	IMM (DOUBLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-| int __cmpdf2(double, double);
-	FUNC(__cmpdf2)
-SYM (__cmpdf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(20),sp@-
-	movl	a6@(16),sp@-
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpdf2_internal)
-	unlk	a6
-	rts
-
-|=============================================================================
-|                           rounding routines
-|=============================================================================
-
-| The rounding routines expect the number to be normalized in registers
-| d0-d1-d2-d3, with the exponent in register d4. They assume that the 
-| exponent is larger or equal to 1. They return a properly normalized number
-| if possible, and a denormalized number otherwise. The exponent is returned
-| in d4.
-
-Lround$to$nearest:
-| We now normalize as suggested by D. Knuth ("Seminumerical Algorithms"):
-| Here we assume that the exponent is not too small (this should be checked
-| before entering the rounding routine), but the number could be denormalized.
-
-| Check for denormalized numbers:
-1:	btst	IMM (DBL_MANT_DIG-32),d0
-	bne	2f		| if set the number is normalized
-| Normalize shifting left until bit #DBL_MANT_DIG-32 is set or the exponent 
-| is one (remember that a denormalized number corresponds to an 
-| exponent of -D_BIAS+1).
-#ifndef __mcoldfire__
-	cmpw	IMM (1),d4	| remember that the exponent is at least one
-#else
-	cmpl	IMM (1),d4	| remember that the exponent is at least one
-#endif
- 	beq	2f		| an exponent of one means denormalized
-	addl	d3,d3		| else shift and adjust the exponent
-	addxl	d2,d2		|
-	addxl	d1,d1		|
-	addxl	d0,d0		|
-#ifndef __mcoldfire__
-	dbra	d4,1b		|
-#else
-	subql	IMM (1), d4
-	bpl	1b
-#endif
-2:
-| Now round: we do it as follows: after the shifting we can write the
-| fraction part as f + delta, where 1 < f < 2^25, and 0 <= delta <= 2.
-| If delta < 1, do nothing. If delta > 1, add 1 to f. 
-| If delta == 1, we make sure the rounded number will be even (odd?) 
-| (after shifting).
-	btst	IMM (0),d1	| is delta < 1?
-	beq	2f		| if so, do not do anything
-	orl	d2,d3		| is delta == 1?
-	bne	1f		| if so round to even
-	movel	d1,d3		| 
-	andl	IMM (2),d3	| bit 1 is the last significant bit
-	movel	IMM (0),d2	|
-	addl	d3,d1		|
-	addxl	d2,d0		|
-	bra	2f		| 
-1:	movel	IMM (1),d3	| else add 1 
-	movel	IMM (0),d2	|
-	addl	d3,d1		|
-	addxl	d2,d0
-| Shift right once (because we used bit #DBL_MANT_DIG-32!).
-2:
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1		
-#else
-	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	10f
-	bset	IMM (31),d1
-10:	lsrl	IMM (1),d0
-#endif
-
-| Now check again bit #DBL_MANT_DIG-32 (rounding could have produced a
-| 'fraction overflow' ...).
-	btst	IMM (DBL_MANT_DIG-32),d0	
-	beq	1f
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	addw	IMM (1),d4
-#else
-	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	10f
-	bset	IMM (31),d1
-10:	lsrl	IMM (1),d0
-	addl	IMM (1),d4
-#endif
-1:
-| If bit #DBL_MANT_DIG-32-1 is clear we have a denormalized number, so we 
-| have to put the exponent to zero and return a denormalized number.
-	btst	IMM (DBL_MANT_DIG-32-1),d0
-	beq	1f
-	jmp	a0@
-1:	movel	IMM (0),d4
-	jmp	a0@
-
-Lround$to$zero:
-Lround$to$plus:
-Lround$to$minus:
-	jmp	a0@
-#endif /* L_double */
-
-#ifdef  L_float
-
-	.globl	SYM (_fpCCR)
-	.globl  $_exception_handler
-
-QUIET_NaN    = 0xffffffff
-SIGNL_NaN    = 0x7f800001
-INFINITY     = 0x7f800000
-
-F_MAX_EXP      = 0xff
-F_BIAS         = 126
-FLT_MAX_EXP    = F_MAX_EXP - F_BIAS
-FLT_MIN_EXP    = 1 - F_BIAS
-FLT_MANT_DIG   = 24
-
-INEXACT_RESULT 		= 0x0001
-UNDERFLOW 		= 0x0002
-OVERFLOW 		= 0x0004
-DIVIDE_BY_ZERO 		= 0x0008
-INVALID_OPERATION 	= 0x0010
-
-SINGLE_FLOAT = 1
-
-NOOP         = 0
-ADD          = 1
-MULTIPLY     = 2
-DIVIDE       = 3
-NEGATE       = 4
-COMPARE      = 5
-EXTENDSFDF   = 6
-TRUNCDFSF    = 7
-
-UNKNOWN           = -1
-ROUND_TO_NEAREST  = 0 | round result to nearest representable value
-ROUND_TO_ZERO     = 1 | round result towards zero
-ROUND_TO_PLUS     = 2 | round result towards plus infinity
-ROUND_TO_MINUS    = 3 | round result towards minus infinity
-
-| Entry points:
-
-	.globl SYM (__addsf3)
-	.globl SYM (__subsf3)
-	.globl SYM (__mulsf3)
-	.globl SYM (__divsf3)
-	.globl SYM (__negsf2)
-	.globl SYM (__cmpsf2)
-	.globl SYM (__cmpsf2_internal)
-#ifdef __ELF__
-	.hidden SYM (__cmpsf2_internal)
-#endif
-
-| These are common routines to return and signal exceptions.	
-
-	.text
-	.even
-
-Lf$den:
-| Return and signal a denormalized number
-	orl	d7,d0
-	moveq	IMM (INEXACT_RESULT+UNDERFLOW),d7
-	moveq	IMM (SINGLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-Lf$infty:
-Lf$overflow:
-| Return a properly signed INFINITY and set the exception flags 
-	movel	IMM (INFINITY),d0
-	orl	d7,d0
-	moveq	IMM (INEXACT_RESULT+OVERFLOW),d7
-	moveq	IMM (SINGLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-Lf$underflow:
-| Return 0 and set the exception flags 
-	moveq	IMM (0),d0
-	moveq	IMM (INEXACT_RESULT+UNDERFLOW),d7
-	moveq	IMM (SINGLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-Lf$inop:
-| Return a quiet NaN and set the exception flags
-	movel	IMM (QUIET_NaN),d0
-	moveq	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
-	moveq	IMM (SINGLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-Lf$div$0:
-| Return a properly signed INFINITY and set the exception flags
-	movel	IMM (INFINITY),d0
-	orl	d7,d0
-	moveq	IMM (INEXACT_RESULT+DIVIDE_BY_ZERO),d7
-	moveq	IMM (SINGLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-|=============================================================================
-|=============================================================================
-|                         single precision routines
-|=============================================================================
-|=============================================================================
-
-| A single precision floating point number (float) has the format:
-|
-| struct _float {
-|  unsigned int sign      : 1;  /* sign bit */ 
-|  unsigned int exponent  : 8;  /* exponent, shifted by 126 */
-|  unsigned int fraction  : 23; /* fraction */
-| } float;
-| 
-| Thus sizeof(float) = 4 (32 bits). 
-|
-| All the routines are callable from C programs, and return the result 
-| in the single register d0. They also preserve all registers except 
-| d0-d1 and a0-a1.
-
-|=============================================================================
-|                              __subsf3
-|=============================================================================
-
-| float __subsf3(float, float);
-	FUNC(__subsf3)
-SYM (__subsf3):
-	bchg	IMM (31),sp@(8)	| change sign of second operand
-				| and fall through
-|=============================================================================
-|                              __addsf3
-|=============================================================================
-
-| float __addsf3(float, float);
-	FUNC(__addsf3)
-SYM (__addsf3):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)	| everything will be done in registers
-	moveml	d2-d7,sp@-	| save all data registers but d0-d1
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	movel	a6@(8),d0	| get first operand
-	movel	a6@(12),d1	| get second operand
-	movel	d0,a0		| get d0's sign bit '
-	addl	d0,d0		| check and clear sign bit of a
-	beq	Laddsf$b	| if zero return second operand
-	movel	d1,a1		| save b's sign bit '
-	addl	d1,d1		| get rid of sign bit
-	beq	Laddsf$a	| if zero return first operand
-
-| Get the exponents and check for denormalized and/or infinity.
-
-	movel	IMM (0x00ffffff),d4	| mask to get fraction
-	movel	IMM (0x01000000),d5	| mask to put hidden bit back
-
-	movel	d0,d6		| save a to get exponent
-	andl	d4,d0		| get fraction in d0
-	notl 	d4		| make d4 into a mask for the exponent
-	andl	d4,d6		| get exponent in d6
-	beq	Laddsf$a$den	| branch if a is denormalized
-	cmpl	d4,d6		| check for INFINITY or NaN
-	beq	Laddsf$nf
-	swap	d6		| put exponent into first word
-	orl	d5,d0		| and put hidden bit back
-Laddsf$1:
-| Now we have a's exponent in d6 (second byte) and the mantissa in d0. '
-	movel	d1,d7		| get exponent in d7
-	andl	d4,d7		| 
-	beq	Laddsf$b$den	| branch if b is denormalized
-	cmpl	d4,d7		| check for INFINITY or NaN
-	beq	Laddsf$nf
-	swap	d7		| put exponent into first word
-	notl 	d4		| make d4 into a mask for the fraction
-	andl	d4,d1		| get fraction in d1
-	orl	d5,d1		| and put hidden bit back
-Laddsf$2:
-| Now we have b's exponent in d7 (second byte) and the mantissa in d1. '
-
-| Note that the hidden bit corresponds to bit #FLT_MANT_DIG-1, and we 
-| shifted right once, so bit #FLT_MANT_DIG is set (so we have one extra
-| bit).
-
-	movel	d1,d2		| move b to d2, since we want to use
-				| two registers to do the sum
-	movel	IMM (0),d1	| and clear the new ones
-	movel	d1,d3		|
-
-| Here we shift the numbers in registers d0 and d1 so the exponents are the
-| same, and put the largest exponent in d6. Note that we are using two
-| registers for each number (see the discussion by D. Knuth in "Seminumerical 
-| Algorithms").
-#ifndef __mcoldfire__
-	cmpw	d6,d7		| compare exponents
-#else
-	cmpl	d6,d7		| compare exponents
-#endif
-	beq	Laddsf$3	| if equal don't shift '
-	bhi	5f		| branch if second exponent largest
-1:
-	subl	d6,d7		| keep the largest exponent
-	negl	d7
-#ifndef __mcoldfire__
-	lsrw	IMM (8),d7	| put difference in lower byte
-#else
-	lsrl	IMM (8),d7	| put difference in lower byte
-#endif
-| if difference is too large we don't shift (actually, we can just exit) '
-#ifndef __mcoldfire__
-	cmpw	IMM (FLT_MANT_DIG+2),d7		
-#else
-	cmpl	IMM (FLT_MANT_DIG+2),d7		
-#endif
-	bge	Laddsf$b$small
-#ifndef __mcoldfire__
-	cmpw	IMM (16),d7	| if difference >= 16 swap
-#else
-	cmpl	IMM (16),d7	| if difference >= 16 swap
-#endif
-	bge	4f
-2:
-#ifndef __mcoldfire__
-	subw	IMM (1),d7
-#else
-	subql	IMM (1), d7
-#endif
-3:
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d2	| shift right second operand
-	roxrl	IMM (1),d3
-	dbra	d7,3b
-#else
-	lsrl	IMM (1),d3
-	btst	IMM (0),d2
-	beq	10f
-	bset	IMM (31),d3
-10:	lsrl	IMM (1),d2
-	subql	IMM (1), d7
-	bpl	3b
-#endif
-	bra	Laddsf$3
-4:
-	movew	d2,d3
-	swap	d3
-	movew	d3,d2
-	swap	d2
-#ifndef __mcoldfire__
-	subw	IMM (16),d7
-#else
-	subl	IMM (16),d7
-#endif
-	bne	2b		| if still more bits, go back to normal case
-	bra	Laddsf$3
-5:
-#ifndef __mcoldfire__
-	exg	d6,d7		| exchange the exponents
-#else
-	eorl	d6,d7
-	eorl	d7,d6
-	eorl	d6,d7
-#endif
-	subl	d6,d7		| keep the largest exponent
-	negl	d7		|
-#ifndef __mcoldfire__
-	lsrw	IMM (8),d7	| put difference in lower byte
-#else
-	lsrl	IMM (8),d7	| put difference in lower byte
-#endif
-| if difference is too large we don't shift (and exit!) '
-#ifndef __mcoldfire__
-	cmpw	IMM (FLT_MANT_DIG+2),d7		
-#else
-	cmpl	IMM (FLT_MANT_DIG+2),d7		
-#endif
-	bge	Laddsf$a$small
-#ifndef __mcoldfire__
-	cmpw	IMM (16),d7	| if difference >= 16 swap
-#else
-	cmpl	IMM (16),d7	| if difference >= 16 swap
-#endif
-	bge	8f
-6:
-#ifndef __mcoldfire__
-	subw	IMM (1),d7
-#else
-	subl	IMM (1),d7
-#endif
-7:
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0	| shift right first operand
-	roxrl	IMM (1),d1
-	dbra	d7,7b
-#else
-	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	10f
-	bset	IMM (31),d1
-10:	lsrl	IMM (1),d0
-	subql	IMM (1),d7
-	bpl	7b
-#endif
-	bra	Laddsf$3
-8:
-	movew	d0,d1
-	swap	d1
-	movew	d1,d0
-	swap	d0
-#ifndef __mcoldfire__
-	subw	IMM (16),d7
-#else
-	subl	IMM (16),d7
-#endif
-	bne	6b		| if still more bits, go back to normal case
-				| otherwise we fall through
-
-| Now we have a in d0-d1, b in d2-d3, and the largest exponent in d6 (the
-| signs are stored in a0 and a1).
-
-Laddsf$3:
-| Here we have to decide whether to add or subtract the numbers
-#ifndef __mcoldfire__
-	exg	d6,a0		| get signs back
-	exg	d7,a1		| and save the exponents
-#else
-	movel	d6,d4
-	movel	a0,d6
-	movel	d4,a0
-	movel	d7,d4
-	movel	a1,d7
-	movel	d4,a1
-#endif
-	eorl	d6,d7		| combine sign bits
-	bmi	Lsubsf$0	| if negative a and b have opposite 
-				| sign so we actually subtract the
-				| numbers
-
-| Here we have both positive or both negative
-#ifndef __mcoldfire__
-	exg	d6,a0		| now we have the exponent in d6
-#else
-	movel	d6,d4
-	movel	a0,d6
-	movel	d4,a0
-#endif
-	movel	a0,d7		| and sign in d7
-	andl	IMM (0x80000000),d7
-| Here we do the addition.
-	addl	d3,d1
-	addxl	d2,d0
-| Note: now we have d2, d3, d4 and d5 to play with! 
-
-| Put the exponent, in the first byte, in d2, to use the "standard" rounding
-| routines:
-	movel	d6,d2
-#ifndef __mcoldfire__
-	lsrw	IMM (8),d2
-#else
-	lsrl	IMM (8),d2
-#endif
-
-| Before rounding normalize so bit #FLT_MANT_DIG is set (we will consider
-| the case of denormalized numbers in the rounding routine itself).
-| As in the addition (not in the subtraction!) we could have set 
-| one more bit we check this:
-	btst	IMM (FLT_MANT_DIG+1),d0	
-	beq	1f
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-#else
-	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	10f
-	bset	IMM (31),d1
-10:	lsrl	IMM (1),d0
-#endif
-	addl	IMM (1),d2
-1:
-	lea	pc@(Laddsf$4),a0 | to return from rounding routine
-	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
-#ifdef __mcoldfire__
-	clrl	d6
-#endif
-	movew	a1@(6),d6	| rounding mode in d6
-	beq	Lround$to$nearest
-#ifndef __mcoldfire__
-	cmpw	IMM (ROUND_TO_PLUS),d6
-#else
-	cmpl	IMM (ROUND_TO_PLUS),d6
-#endif
-	bhi	Lround$to$minus
-	blt	Lround$to$zero
-	bra	Lround$to$plus
-Laddsf$4:
-| Put back the exponent, but check for overflow.
-#ifndef __mcoldfire__
-	cmpw	IMM (0xff),d2
-#else
-	cmpl	IMM (0xff),d2
-#endif
-	bhi	1f
-	bclr	IMM (FLT_MANT_DIG-1),d0
-#ifndef __mcoldfire__
-	lslw	IMM (7),d2
-#else
-	lsll	IMM (7),d2
-#endif
-	swap	d2
-	orl	d2,d0
-	bra	Laddsf$ret
-1:
-	moveq	IMM (ADD),d5
-	bra	Lf$overflow
-
-Lsubsf$0:
-| We are here if a > 0 and b < 0 (sign bits cleared).
-| Here we do the subtraction.
-	movel	d6,d7		| put sign in d7
-	andl	IMM (0x80000000),d7
-
-	subl	d3,d1		| result in d0-d1
-	subxl	d2,d0		|
-	beq	Laddsf$ret	| if zero just exit
-	bpl	1f		| if positive skip the following
-	bchg	IMM (31),d7	| change sign bit in d7
-	negl	d1
-	negxl	d0
-1:
-#ifndef __mcoldfire__
-	exg	d2,a0		| now we have the exponent in d2
-	lsrw	IMM (8),d2	| put it in the first byte
-#else
-	movel	d2,d4
-	movel	a0,d2
-	movel	d4,a0
-	lsrl	IMM (8),d2	| put it in the first byte
-#endif
-
-| Now d0-d1 is positive and the sign bit is in d7.
-
-| Note that we do not have to normalize, since in the subtraction bit
-| #FLT_MANT_DIG+1 is never set, and denormalized numbers are handled by
-| the rounding routines themselves.
-	lea	pc@(Lsubsf$1),a0 | to return from rounding routine
-	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
-#ifdef __mcoldfire__
-	clrl	d6
-#endif
-	movew	a1@(6),d6	| rounding mode in d6
-	beq	Lround$to$nearest
-#ifndef __mcoldfire__
-	cmpw	IMM (ROUND_TO_PLUS),d6
-#else
-	cmpl	IMM (ROUND_TO_PLUS),d6
-#endif
-	bhi	Lround$to$minus
-	blt	Lround$to$zero
-	bra	Lround$to$plus
-Lsubsf$1:
-| Put back the exponent (we can't have overflow!). '
-	bclr	IMM (FLT_MANT_DIG-1),d0
-#ifndef __mcoldfire__
-	lslw	IMM (7),d2
-#else
-	lsll	IMM (7),d2
-#endif
-	swap	d2
-	orl	d2,d0
-	bra	Laddsf$ret
-
-| If one of the numbers was too small (difference of exponents >= 
-| FLT_MANT_DIG+2) we return the other (and now we don't have to '
-| check for finiteness or zero).
-Laddsf$a$small:
-	movel	a6@(12),d0
-	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7	| restore data registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6		| and return
-	rts
-
-Laddsf$b$small:
-	movel	a6@(8),d0
-	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7	| restore data registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6		| and return
-	rts
-
-| If the numbers are denormalized remember to put exponent equal to 1.
-
-Laddsf$a$den:
-	movel	d5,d6		| d5 contains 0x01000000
-	swap	d6
-	bra	Laddsf$1
-
-Laddsf$b$den:
-	movel	d5,d7
-	swap	d7
-	notl 	d4		| make d4 into a mask for the fraction
-				| (this was not executed after the jump)
-	bra	Laddsf$2
-
-| The rest is mainly code for the different results which can be 
-| returned (checking always for +/-INFINITY and NaN).
-
-Laddsf$b:
-| Return b (if a is zero).
-	movel	a6@(12),d0
-	cmpl	IMM (0x80000000),d0	| Check if b is -0
-	bne	1f
-	movel	a0,d7
-	andl	IMM (0x80000000),d7	| Use the sign of a
-	clrl	d0
-	bra	Laddsf$ret
-Laddsf$a:
-| Return a (if b is zero).
-	movel	a6@(8),d0
-1:
-	moveq	IMM (ADD),d5
-| We have to check for NaN and +/-infty.
-	movel	d0,d7
-	andl	IMM (0x80000000),d7	| put sign in d7
-	bclr	IMM (31),d0		| clear sign
-	cmpl	IMM (INFINITY),d0	| check for infty or NaN
-	bge	2f
-	movel	d0,d0		| check for zero (we do this because we don't '
-	bne	Laddsf$ret	| want to return -0 by mistake
-	bclr	IMM (31),d7	| if zero be sure to clear sign
-	bra	Laddsf$ret	| if everything OK just return
-2:
-| The value to be returned is either +/-infty or NaN
-	andl	IMM (0x007fffff),d0	| check for NaN
-	bne	Lf$inop			| if mantissa not zero is NaN
-	bra	Lf$infty
-
-Laddsf$ret:
-| Normal exit (a and b nonzero, result is not NaN nor +/-infty).
-| We have to clear the exception flags (just the exception type).
-	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-	orl	d7,d0		| put sign bit
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7	| restore data registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6		| and return
-	rts
-
-Laddsf$ret$den:
-| Return a denormalized number (for addition we don't signal underflow) '
-	lsrl	IMM (1),d0	| remember to shift right back once
-	bra	Laddsf$ret	| and return
-
-| Note: when adding two floats of the same sign if either one is 
-| NaN we return NaN without regard to whether the other is finite or 
-| not. When subtracting them (i.e., when adding two numbers of 
-| opposite signs) things are more complicated: if both are INFINITY 
-| we return NaN, if only one is INFINITY and the other is NaN we return
-| NaN, but if it is finite we return INFINITY with the corresponding sign.
-
-Laddsf$nf:
-	moveq	IMM (ADD),d5
-| This could be faster but it is not worth the effort, since it is not
-| executed very often. We sacrifice speed for clarity here.
-	movel	a6@(8),d0	| get the numbers back (remember that we
-	movel	a6@(12),d1	| did some processing already)
-	movel	IMM (INFINITY),d4 | useful constant (INFINITY)
-	movel	d0,d2		| save sign bits
-	movel	d1,d3
-	bclr	IMM (31),d0	| clear sign bits
-	bclr	IMM (31),d1
-| We know that one of them is either NaN of +/-INFINITY
-| Check for NaN (if either one is NaN return NaN)
-	cmpl	d4,d0		| check first a (d0)
-	bhi	Lf$inop		
-	cmpl	d4,d1		| check now b (d1)
-	bhi	Lf$inop		
-| Now comes the check for +/-INFINITY. We know that both are (maybe not
-| finite) numbers, but we have to check if both are infinite whether we
-| are adding or subtracting them.
-	eorl	d3,d2		| to check sign bits
-	bmi	1f
-	movel	d0,d7
-	andl	IMM (0x80000000),d7	| get (common) sign bit
-	bra	Lf$infty
-1:
-| We know one (or both) are infinite, so we test for equality between the
-| two numbers (if they are equal they have to be infinite both, so we
-| return NaN).
-	cmpl	d1,d0		| are both infinite?
-	beq	Lf$inop		| if so return NaN
-
-	movel	d0,d7
-	andl	IMM (0x80000000),d7 | get a's sign bit '
-	cmpl	d4,d0		| test now for infinity
-	beq	Lf$infty	| if a is INFINITY return with this sign
-	bchg	IMM (31),d7	| else we know b is INFINITY and has
-	bra	Lf$infty	| the opposite sign
-
-|=============================================================================
-|                             __mulsf3
-|=============================================================================
-
-| float __mulsf3(float, float);
-	FUNC(__mulsf3)
-SYM (__mulsf3):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)
-	moveml	d2-d7,sp@-
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	movel	a6@(8),d0	| get a into d0
-	movel	a6@(12),d1	| and b into d1
-	movel	d0,d7		| d7 will hold the sign of the product
-	eorl	d1,d7		|
-	andl	IMM (0x80000000),d7
-	movel	IMM (INFINITY),d6	| useful constant (+INFINITY)
-	movel	d6,d5			| another (mask for fraction)
-	notl	d5			|
-	movel	IMM (0x00800000),d4	| this is to put hidden bit back
-	bclr	IMM (31),d0		| get rid of a's sign bit '
-	movel	d0,d2			|
-	beq	Lmulsf$a$0		| branch if a is zero
-	bclr	IMM (31),d1		| get rid of b's sign bit '
-	movel	d1,d3		|
-	beq	Lmulsf$b$0	| branch if b is zero
-	cmpl	d6,d0		| is a big?
-	bhi	Lmulsf$inop	| if a is NaN return NaN
-	beq	Lmulsf$inf	| if a is INFINITY we have to check b
-	cmpl	d6,d1		| now compare b with INFINITY
-	bhi	Lmulsf$inop	| is b NaN?
-	beq	Lmulsf$overflow | is b INFINITY?
-| Here we have both numbers finite and nonzero (and with no sign bit).
-| Now we get the exponents into d2 and d3.
-	andl	d6,d2		| and isolate exponent in d2
-	beq	Lmulsf$a$den	| if exponent is zero we have a denormalized
-	andl	d5,d0		| and isolate fraction
-	orl	d4,d0		| and put hidden bit back
-	swap	d2		| I like exponents in the first byte
-#ifndef __mcoldfire__
-	lsrw	IMM (7),d2	| 
-#else
-	lsrl	IMM (7),d2	| 
-#endif
-Lmulsf$1:			| number
-	andl	d6,d3		|
-	beq	Lmulsf$b$den	|
-	andl	d5,d1		|
-	orl	d4,d1		|
-	swap	d3		|
-#ifndef __mcoldfire__
-	lsrw	IMM (7),d3	|
-#else
-	lsrl	IMM (7),d3	|
-#endif
-Lmulsf$2:			|
-#ifndef __mcoldfire__
-	addw	d3,d2		| add exponents
-	subw	IMM (F_BIAS+1),d2 | and subtract bias (plus one)
+#ifndef __FASTCALL__
+#include "lb1sf68-std.asm"
 #else
-	addl	d3,d2		| add exponents
-	subl	IMM (F_BIAS+1),d2 | and subtract bias (plus one)
+#include "lb1sf68-fast.asm"
 #endif
 
-| We are now ready to do the multiplication. The situation is as follows:
-| both a and b have bit FLT_MANT_DIG-1 set (even if they were 
-| denormalized to start with!), which means that in the product 
-| bit 2*(FLT_MANT_DIG-1) (that is, bit 2*FLT_MANT_DIG-2-32 of the 
-| high long) is set. 
-
-| To do the multiplication let us move the number a little bit around ...
-	movel	d1,d6		| second operand in d6
-	movel	d0,d5		| first operand in d4-d5
-	movel	IMM (0),d4
-	movel	d4,d1		| the sums will go in d0-d1
-	movel	d4,d0
-
-| now bit FLT_MANT_DIG-1 becomes bit 31:
-	lsll	IMM (31-FLT_MANT_DIG+1),d6		
-
-| Start the loop (we loop #FLT_MANT_DIG times):
-	moveq	IMM (FLT_MANT_DIG-1),d3	
-1:	addl	d1,d1		| shift sum 
-	addxl	d0,d0
-	lsll	IMM (1),d6	| get bit bn
-	bcc	2f		| if not set skip sum
-	addl	d5,d1		| add a
-	addxl	d4,d0
-2:
-#ifndef __mcoldfire__
-	dbf	d3,1b		| loop back
-#else
-	subql	IMM (1),d3
-	bpl	1b
-#endif
-
-| Now we have the product in d0-d1, with bit (FLT_MANT_DIG - 1) + FLT_MANT_DIG
-| (mod 32) of d0 set. The first thing to do now is to normalize it so bit 
-| FLT_MANT_DIG is set (to do the rounding).
-#ifndef __mcoldfire__
-	rorl	IMM (6),d1
-	swap	d1
-	movew	d1,d3
-	andw	IMM (0x03ff),d3
-	andw	IMM (0xfd00),d1
-#else
-	movel	d1,d3
-	lsll	IMM (8),d1
-	addl	d1,d1
-	addl	d1,d1
-	moveq	IMM (22),d5
-	lsrl	d5,d3
-	orl	d3,d1
-	andl	IMM (0xfffffd00),d1
-#endif
-	lsll	IMM (8),d0
-	addl	d0,d0
-	addl	d0,d0
-#ifndef __mcoldfire__
-	orw	d3,d0
-#else
-	orl	d3,d0
-#endif
-
-	moveq	IMM (MULTIPLY),d5
-	
-	btst	IMM (FLT_MANT_DIG+1),d0
-	beq	Lround$exit
-#ifndef __mcoldfire__
-	lsrl	IMM (1),d0
-	roxrl	IMM (1),d1
-	addw	IMM (1),d2
-#else
-	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	10f
-	bset	IMM (31),d1
-10:	lsrl	IMM (1),d0
-	addql	IMM (1),d2
-#endif
-	bra	Lround$exit
-
-Lmulsf$inop:
-	moveq	IMM (MULTIPLY),d5
-	bra	Lf$inop
-
-Lmulsf$overflow:
-	moveq	IMM (MULTIPLY),d5
-	bra	Lf$overflow
-
-Lmulsf$inf:
-	moveq	IMM (MULTIPLY),d5
-| If either is NaN return NaN; else both are (maybe infinite) numbers, so
-| return INFINITY with the correct sign (which is in d7).
-	cmpl	d6,d1		| is b NaN?
-	bhi	Lf$inop		| if so return NaN
-	bra	Lf$overflow	| else return +/-INFINITY
-
-| If either number is zero return zero, unless the other is +/-INFINITY, 
-| or NaN, in which case we return NaN.
-Lmulsf$b$0:
-| Here d1 (==b) is zero.
-	movel	a6@(8),d1	| get a again to check for non-finiteness
-	bra	1f
-Lmulsf$a$0:
-	movel	a6@(12),d1	| get b again to check for non-finiteness
-1:	bclr	IMM (31),d1	| clear sign bit 
-	cmpl	IMM (INFINITY),d1 | and check for a large exponent
-	bge	Lf$inop		| if b is +/-INFINITY or NaN return NaN
-	movel	d7,d0		| else return signed zero
-	PICLEA	SYM (_fpCCR),a0	|
-	movew	IMM (0),a0@	| 
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7	| 
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6		| 
-	rts			| 
-
-| If a number is denormalized we put an exponent of 1 but do not put the 
-| hidden bit back into the fraction; instead we shift left until bit 23
-| (the hidden bit) is set, adjusting the exponent accordingly. We do this
-| to ensure that the product of the fractions is close to 1.
-Lmulsf$a$den:
-	movel	IMM (1),d2
-	andl	d5,d0
-1:	addl	d0,d0		| shift a left (until bit 23 is set)
-#ifndef __mcoldfire__
-	subw	IMM (1),d2	| and adjust exponent
-#else
-	subql	IMM (1),d2	| and adjust exponent
-#endif
-	btst	IMM (FLT_MANT_DIG-1),d0
-	bne	Lmulsf$1	|
-	bra	1b		| else loop back
-
-Lmulsf$b$den:
-	movel	IMM (1),d3
-	andl	d5,d1
-1:	addl	d1,d1		| shift b left until bit 23 is set
-#ifndef __mcoldfire__
-	subw	IMM (1),d3	| and adjust exponent
-#else
-	subql	IMM (1),d3	| and adjust exponent
-#endif
-	btst	IMM (FLT_MANT_DIG-1),d1
-	bne	Lmulsf$2	|
-	bra	1b		| else loop back
-
-|=============================================================================
-|                             __divsf3
-|=============================================================================
-
-| float __divsf3(float, float);
-	FUNC(__divsf3)
-SYM (__divsf3):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)
-	moveml	d2-d7,sp@-
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	movel	a6@(8),d0		| get a into d0
-	movel	a6@(12),d1		| and b into d1
-	movel	d0,d7			| d7 will hold the sign of the result
-	eorl	d1,d7			|
-	andl	IMM (0x80000000),d7	| 
-	movel	IMM (INFINITY),d6	| useful constant (+INFINITY)
-	movel	d6,d5			| another (mask for fraction)
-	notl	d5			|
-	movel	IMM (0x00800000),d4	| this is to put hidden bit back
-	bclr	IMM (31),d0		| get rid of a's sign bit '
-	movel	d0,d2			|
-	beq	Ldivsf$a$0		| branch if a is zero
-	bclr	IMM (31),d1		| get rid of b's sign bit '
-	movel	d1,d3			|
-	beq	Ldivsf$b$0		| branch if b is zero
-	cmpl	d6,d0			| is a big?
-	bhi	Ldivsf$inop		| if a is NaN return NaN
-	beq	Ldivsf$inf		| if a is INFINITY we have to check b
-	cmpl	d6,d1			| now compare b with INFINITY 
-	bhi	Ldivsf$inop		| if b is NaN return NaN
-	beq	Ldivsf$underflow
-| Here we have both numbers finite and nonzero (and with no sign bit).
-| Now we get the exponents into d2 and d3 and normalize the numbers to
-| ensure that the ratio of the fractions is close to 1. We do this by
-| making sure that bit #FLT_MANT_DIG-1 (hidden bit) is set.
-	andl	d6,d2		| and isolate exponent in d2
-	beq	Ldivsf$a$den	| if exponent is zero we have a denormalized
-	andl	d5,d0		| and isolate fraction
-	orl	d4,d0		| and put hidden bit back
-	swap	d2		| I like exponents in the first byte
-#ifndef __mcoldfire__
-	lsrw	IMM (7),d2	| 
-#else
-	lsrl	IMM (7),d2	| 
-#endif
-Ldivsf$1:			| 
-	andl	d6,d3		|
-	beq	Ldivsf$b$den	|
-	andl	d5,d1		|
-	orl	d4,d1		|
-	swap	d3		|
-#ifndef __mcoldfire__
-	lsrw	IMM (7),d3	|
-#else
-	lsrl	IMM (7),d3	|
-#endif
-Ldivsf$2:			|
-#ifndef __mcoldfire__
-	subw	d3,d2		| subtract exponents
- 	addw	IMM (F_BIAS),d2	| and add bias
-#else
-	subl	d3,d2		| subtract exponents
- 	addl	IMM (F_BIAS),d2	| and add bias
-#endif
- 
-| We are now ready to do the division. We have prepared things in such a way
-| that the ratio of the fractions will be less than 2 but greater than 1/2.
-| At this point the registers in use are:
-| d0	holds a (first operand, bit FLT_MANT_DIG=0, bit FLT_MANT_DIG-1=1)
-| d1	holds b (second operand, bit FLT_MANT_DIG=1)
-| d2	holds the difference of the exponents, corrected by the bias
-| d7	holds the sign of the ratio
-| d4, d5, d6 hold some constants
-	movel	d7,a0		| d6-d7 will hold the ratio of the fractions
-	movel	IMM (0),d6	| 
-	movel	d6,d7
-
-	moveq	IMM (FLT_MANT_DIG+1),d3
-1:	cmpl	d0,d1		| is a < b?
-	bhi	2f		|
-	bset	d3,d6		| set a bit in d6
-	subl	d1,d0		| if a >= b  a <-- a-b
-	beq	3f		| if a is zero, exit
-2:	addl	d0,d0		| multiply a by 2
-#ifndef __mcoldfire__
-	dbra	d3,1b
-#else
-	subql	IMM (1),d3
-	bpl	1b
-#endif
-
-| Now we keep going to set the sticky bit ...
-	moveq	IMM (FLT_MANT_DIG),d3
-1:	cmpl	d0,d1
-	ble	2f
-	addl	d0,d0
-#ifndef __mcoldfire__
-	dbra	d3,1b
-#else
-	subql	IMM(1),d3
-	bpl	1b
-#endif
-	movel	IMM (0),d1
-	bra	3f
-2:	movel	IMM (0),d1
-#ifndef __mcoldfire__
-	subw	IMM (FLT_MANT_DIG),d3
-	addw	IMM (31),d3
-#else
-	subl	IMM (FLT_MANT_DIG),d3
-	addl	IMM (31),d3
-#endif
-	bset	d3,d1
-3:
-	movel	d6,d0		| put the ratio in d0-d1
-	movel	a0,d7		| get sign back
-
-| Because of the normalization we did before we are guaranteed that 
-| d0 is smaller than 2^26 but larger than 2^24. Thus bit 26 is not set,
-| bit 25 could be set, and if it is not set then bit 24 is necessarily set.
-	btst	IMM (FLT_MANT_DIG+1),d0		
-	beq	1f              | if it is not set, then bit 24 is set
-	lsrl	IMM (1),d0	|
-#ifndef __mcoldfire__
-	addw	IMM (1),d2	|
-#else
-	addl	IMM (1),d2	|
-#endif
-1:
-| Now round, check for over- and underflow, and exit.
-	moveq	IMM (DIVIDE),d5
-	bra	Lround$exit
-
-Ldivsf$inop:
-	moveq	IMM (DIVIDE),d5
-	bra	Lf$inop
-
-Ldivsf$overflow:
-	moveq	IMM (DIVIDE),d5
-	bra	Lf$overflow
-
-Ldivsf$underflow:
-	moveq	IMM (DIVIDE),d5
-	bra	Lf$underflow
-
-Ldivsf$a$0:
-	moveq	IMM (DIVIDE),d5
-| If a is zero check to see whether b is zero also. In that case return
-| NaN; then check if b is NaN, and return NaN also in that case. Else
-| return a properly signed zero.
-	andl	IMM (0x7fffffff),d1	| clear sign bit and test b
-	beq	Lf$inop			| if b is also zero return NaN
-	cmpl	IMM (INFINITY),d1	| check for NaN
-	bhi	Lf$inop			| 
-	movel	d7,d0			| else return signed zero
-	PICLEA	SYM (_fpCCR),a0		|
-	movew	IMM (0),a0@		|
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7		| 
-#else
-	moveml	sp@,d2-d7		| 
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6			| 
-	rts				| 
-	
-Ldivsf$b$0:
-	moveq	IMM (DIVIDE),d5
-| If we got here a is not zero. Check if a is NaN; in that case return NaN,
-| else return +/-INFINITY. Remember that a is in d0 with the sign bit 
-| cleared already.
-	cmpl	IMM (INFINITY),d0	| compare d0 with INFINITY
-	bhi	Lf$inop			| if larger it is NaN
-	bra	Lf$div$0		| else signal DIVIDE_BY_ZERO
-
-Ldivsf$inf:
-	moveq	IMM (DIVIDE),d5
-| If a is INFINITY we have to check b
-	cmpl	IMM (INFINITY),d1	| compare b with INFINITY 
-	bge	Lf$inop			| if b is NaN or INFINITY return NaN
-	bra	Lf$overflow		| else return overflow
-
-| If a number is denormalized we put an exponent of 1 but do not put the 
-| bit back into the fraction.
-Ldivsf$a$den:
-	movel	IMM (1),d2
-	andl	d5,d0
-1:	addl	d0,d0		| shift a left until bit FLT_MANT_DIG-1 is set
-#ifndef __mcoldfire__
-	subw	IMM (1),d2	| and adjust exponent
-#else
-	subl	IMM (1),d2	| and adjust exponent
-#endif
-	btst	IMM (FLT_MANT_DIG-1),d0
-	bne	Ldivsf$1
-	bra	1b
-
-Ldivsf$b$den:
-	movel	IMM (1),d3
-	andl	d5,d1
-1:	addl	d1,d1		| shift b left until bit FLT_MANT_DIG is set
-#ifndef __mcoldfire__
-	subw	IMM (1),d3	| and adjust exponent
-#else
-	subl	IMM (1),d3	| and adjust exponent
-#endif
-	btst	IMM (FLT_MANT_DIG-1),d1
-	bne	Ldivsf$2
-	bra	1b
-
-Lround$exit:
-| This is a common exit point for __mulsf3 and __divsf3. 
-
-| First check for underlow in the exponent:
-#ifndef __mcoldfire__
-	cmpw	IMM (-FLT_MANT_DIG-1),d2		
-#else
-	cmpl	IMM (-FLT_MANT_DIG-1),d2		
-#endif
-	blt	Lf$underflow	
-| It could happen that the exponent is less than 1, in which case the 
-| number is denormalized. In this case we shift right and adjust the 
-| exponent until it becomes 1 or the fraction is zero (in the latter case 
-| we signal underflow and return zero).
-	movel	IMM (0),d6	| d6 is used temporarily
-#ifndef __mcoldfire__
-	cmpw	IMM (1),d2	| if the exponent is less than 1 we 
-#else
-	cmpl	IMM (1),d2	| if the exponent is less than 1 we 
-#endif
-	bge	2f		| have to shift right (denormalize)
-1:
-#ifndef __mcoldfire__
-	addw	IMM (1),d2	| adjust the exponent
-	lsrl	IMM (1),d0	| shift right once 
-	roxrl	IMM (1),d1	|
-	roxrl	IMM (1),d6	| d6 collect bits we would lose otherwise
-	cmpw	IMM (1),d2	| is the exponent 1 already?
-#else
-	addql	IMM (1),d2	| adjust the exponent
-	lsrl	IMM (1),d6
-	btst	IMM (0),d1
-	beq	11f
-	bset	IMM (31),d6
-11:	lsrl	IMM (1),d1
-	btst	IMM (0),d0
-	beq	10f
-	bset	IMM (31),d1
-10:	lsrl	IMM (1),d0
-	cmpl	IMM (1),d2	| is the exponent 1 already?
-#endif
-	beq	2f		| if not loop back
-	bra	1b              |
-	bra	Lf$underflow	| safety check, shouldn't execute '
-2:	orl	d6,d1		| this is a trick so we don't lose  '
-				| the extra bits which were flushed right
-| Now call the rounding routine (which takes care of denormalized numbers):
-	lea	pc@(Lround$0),a0 | to return from rounding routine
-	PICLEA	SYM (_fpCCR),a1	| check the rounding mode
-#ifdef __mcoldfire__
-	clrl	d6
-#endif
-	movew	a1@(6),d6	| rounding mode in d6
-	beq	Lround$to$nearest
-#ifndef __mcoldfire__
-	cmpw	IMM (ROUND_TO_PLUS),d6
-#else
-	cmpl	IMM (ROUND_TO_PLUS),d6
-#endif
-	bhi	Lround$to$minus
-	blt	Lround$to$zero
-	bra	Lround$to$plus
-Lround$0:
-| Here we have a correctly rounded result (either normalized or denormalized).
-
-| Here we should have either a normalized number or a denormalized one, and
-| the exponent is necessarily larger or equal to 1 (so we don't have to  '
-| check again for underflow!). We have to check for overflow or for a 
-| denormalized number (which also signals underflow).
-| Check for overflow (i.e., exponent >= 255).
-#ifndef __mcoldfire__
-	cmpw	IMM (0x00ff),d2
-#else
-	cmpl	IMM (0x00ff),d2
-#endif
-	bge	Lf$overflow
-| Now check for a denormalized number (exponent==0).
-	movew	d2,d2
-	beq	Lf$den
-1:
-| Put back the exponents and sign and return.
-#ifndef __mcoldfire__
-	lslw	IMM (7),d2	| exponent back to fourth byte
-#else
-	lsll	IMM (7),d2	| exponent back to fourth byte
-#endif
-	bclr	IMM (FLT_MANT_DIG-1),d0
-	swap	d0		| and put back exponent
-#ifndef __mcoldfire__
-	orw	d2,d0		| 
-#else
-	orl	d2,d0
-#endif
-	swap	d0		|
-	orl	d7,d0		| and sign also
-
-	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-
-|=============================================================================
-|                             __negsf2
-|=============================================================================
-
-| This is trivial and could be shorter if we didn't bother checking for NaN '
-| and +/-INFINITY.
-
-| float __negsf2(float);
-	FUNC(__negsf2)
-SYM (__negsf2):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)
-	moveml	d2-d7,sp@-
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	moveq	IMM (NEGATE),d5
-	movel	a6@(8),d0	| get number to negate in d0
-	bchg	IMM (31),d0	| negate
-	movel	d0,d1		| make a positive copy
-	bclr	IMM (31),d1	|
-	tstl	d1		| check for zero
-	beq	2f		| if zero (either sign) return +zero
-	cmpl	IMM (INFINITY),d1 | compare to +INFINITY
-	blt	1f		|
-	bhi	Lf$inop		| if larger (fraction not zero) is NaN
-	movel	d0,d7		| else get sign and return INFINITY
-	andl	IMM (0x80000000),d7
-	bra	Lf$infty		
-1:	PICLEA	SYM (_fpCCR),a0
-	movew	IMM (0),a0@
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-2:	bclr	IMM (31),d0
-	bra	1b
-
-|=============================================================================
-|                             __cmpsf2
-|=============================================================================
-
-GREATER =  1
-LESS    = -1
-EQUAL   =  0
-
-| int __cmpsf2_internal(float, float, int);
-SYM (__cmpsf2_internal):
-#ifndef __mcoldfire__
-	link	a6,IMM (0)
-	moveml	d2-d7,sp@- 	| save registers
-#else
-	link	a6,IMM (-24)
-	moveml	d2-d7,sp@
-#endif
-	moveq	IMM (COMPARE),d5
-	movel	a6@(8),d0	| get first operand
-	movel	a6@(12),d1	| get second operand
-| Check if either is NaN, and in that case return garbage and signal
-| INVALID_OPERATION. Check also if either is zero, and clear the signs
-| if necessary.
-	movel	d0,d6
-	andl	IMM (0x7fffffff),d0
-	beq	Lcmpsf$a$0
-	cmpl	IMM (0x7f800000),d0
-	bhi	Lcmpf$inop
-Lcmpsf$1:
-	movel	d1,d7
-	andl	IMM (0x7fffffff),d1
-	beq	Lcmpsf$b$0
-	cmpl	IMM (0x7f800000),d1
-	bhi	Lcmpf$inop
-Lcmpsf$2:
-| Check the signs
-	eorl	d6,d7
-	bpl	1f
-| If the signs are not equal check if a >= 0
-	tstl	d6
-	bpl	Lcmpsf$a$gt$b	| if (a >= 0 && b < 0) => a > b
-	bmi	Lcmpsf$b$gt$a	| if (a < 0 && b >= 0) => a < b
-1:
-| If the signs are equal check for < 0
-	tstl	d6
-	bpl	1f
-| If both are negative exchange them
-#ifndef __mcoldfire__
-	exg	d0,d1
-#else
-	movel	d0,d7
-	movel	d1,d0
-	movel	d7,d1
-#endif
-1:
-| Now that they are positive we just compare them as longs (does this also
-| work for denormalized numbers?).
-	cmpl	d0,d1
-	bhi	Lcmpsf$b$gt$a	| |b| > |a|
-	bne	Lcmpsf$a$gt$b	| |b| < |a|
-| If we got here a == b.
-	movel	IMM (EQUAL),d0
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7 	| put back the registers
-#else
-	moveml	sp@,d2-d7
-#endif
-	unlk	a6
-	rts
-Lcmpsf$a$gt$b:
-	movel	IMM (GREATER),d0
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7 	| put back the registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-Lcmpsf$b$gt$a:
-	movel	IMM (LESS),d0
-#ifndef __mcoldfire__
-	moveml	sp@+,d2-d7 	| put back the registers
-#else
-	moveml	sp@,d2-d7
-	| XXX if frame pointer is ever removed, stack pointer must
-	| be adjusted here.
-#endif
-	unlk	a6
-	rts
-
-Lcmpsf$a$0:	
-	bclr	IMM (31),d6
-	bra	Lcmpsf$1
-Lcmpsf$b$0:
-	bclr	IMM (31),d7
-	bra	Lcmpsf$2
-
-Lcmpf$inop:
-	movl	a6@(16),d0
-	moveq	IMM (INEXACT_RESULT+INVALID_OPERATION),d7
-	moveq	IMM (SINGLE_FLOAT),d6
-	PICJUMP	$_exception_handler
-
-| int __cmpsf2(float, float);
-	FUNC(__cmpsf2)
-SYM (__cmpsf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL SYM (__cmpsf2_internal)
-	unlk	a6
-	rts
-
-|=============================================================================
-|                           rounding routines
-|=============================================================================
-
-| The rounding routines expect the number to be normalized in registers
-| d0-d1, with the exponent in register d2. They assume that the 
-| exponent is larger or equal to 1. They return a properly normalized number
-| if possible, and a denormalized number otherwise. The exponent is returned
-| in d2.
-
-Lround$to$nearest:
-| We now normalize as suggested by D. Knuth ("Seminumerical Algorithms"):
-| Here we assume that the exponent is not too small (this should be checked
-| before entering the rounding routine), but the number could be denormalized.
-
-| Check for denormalized numbers:
-1:	btst	IMM (FLT_MANT_DIG),d0
-	bne	2f		| if set the number is normalized
-| Normalize shifting left until bit #FLT_MANT_DIG is set or the exponent 
-| is one (remember that a denormalized number corresponds to an 
-| exponent of -F_BIAS+1).
-#ifndef __mcoldfire__
-	cmpw	IMM (1),d2	| remember that the exponent is at least one
-#else
-	cmpl	IMM (1),d2	| remember that the exponent is at least one
-#endif
- 	beq	2f		| an exponent of one means denormalized
-	addl	d1,d1		| else shift and adjust the exponent
-	addxl	d0,d0		|
-#ifndef __mcoldfire__
-	dbra	d2,1b		|
-#else
-	subql	IMM (1),d2
-	bpl	1b
-#endif
-2:
-| Now round: we do it as follows: after the shifting we can write the
-| fraction part as f + delta, where 1 < f < 2^25, and 0 <= delta <= 2.
-| If delta < 1, do nothing. If delta > 1, add 1 to f. 
-| If delta == 1, we make sure the rounded number will be even (odd?) 
-| (after shifting).
-	btst	IMM (0),d0	| is delta < 1?
-	beq	2f		| if so, do not do anything
-	tstl	d1		| is delta == 1?
-	bne	1f		| if so round to even
-	movel	d0,d1		| 
-	andl	IMM (2),d1	| bit 1 is the last significant bit
-	addl	d1,d0		| 
-	bra	2f		| 
-1:	movel	IMM (1),d1	| else add 1 
-	addl	d1,d0		|
-| Shift right once (because we used bit #FLT_MANT_DIG!).
-2:	lsrl	IMM (1),d0		
-| Now check again bit #FLT_MANT_DIG (rounding could have produced a
-| 'fraction overflow' ...).
-	btst	IMM (FLT_MANT_DIG),d0	
-	beq	1f
-	lsrl	IMM (1),d0
-#ifndef __mcoldfire__
-	addw	IMM (1),d2
-#else
-	addql	IMM (1),d2
-#endif
-1:
-| If bit #FLT_MANT_DIG-1 is clear we have a denormalized number, so we 
-| have to put the exponent to zero and return a denormalized number.
-	btst	IMM (FLT_MANT_DIG-1),d0
-	beq	1f
-	jmp	a0@
-1:	movel	IMM (0),d2
-	jmp	a0@
-
-Lround$to$zero:
-Lround$to$plus:
-Lround$to$minus:
-	jmp	a0@
-#endif /* L_float */
-
-| gcc expects the routines __eqdf2, __nedf2, __gtdf2, __gedf2,
-| __ledf2, __ltdf2 to all return the same value as a direct call to
-| __cmpdf2 would.  In this implementation, each of these routines
-| simply calls __cmpdf2.  It would be more efficient to give the
-| __cmpdf2 routine several names, but separating them out will make it
-| easier to write efficient versions of these routines someday.
-| If the operands recompare unordered unordered __gtdf2 and __gedf2 return -1.
-| The other routines return 1.
-
-#ifdef  L_eqdf2
-	.text
-	FUNC(__eqdf2)
-	.globl	SYM (__eqdf2)
-SYM (__eqdf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(20),sp@-
-	movl	a6@(16),sp@-
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpdf2_internal)
-	unlk	a6
-	rts
-#endif /* L_eqdf2 */
-
-#ifdef  L_nedf2
-	.text
-	FUNC(__nedf2)
-	.globl	SYM (__nedf2)
-SYM (__nedf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(20),sp@-
-	movl	a6@(16),sp@-
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpdf2_internal)
-	unlk	a6
-	rts
-#endif /* L_nedf2 */
-
-#ifdef  L_gtdf2
-	.text
-	FUNC(__gtdf2)
-	.globl	SYM (__gtdf2)
-SYM (__gtdf2):
-	link	a6,IMM (0)
-	pea	-1
-	movl	a6@(20),sp@-
-	movl	a6@(16),sp@-
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpdf2_internal)
-	unlk	a6
-	rts
-#endif /* L_gtdf2 */
-
-#ifdef  L_gedf2
-	.text
-	FUNC(__gedf2)
-	.globl	SYM (__gedf2)
-SYM (__gedf2):
-	link	a6,IMM (0)
-	pea	-1
-	movl	a6@(20),sp@-
-	movl	a6@(16),sp@-
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpdf2_internal)
-	unlk	a6
-	rts
-#endif /* L_gedf2 */
-
-#ifdef  L_ltdf2
-	.text
-	FUNC(__ltdf2)
-	.globl	SYM (__ltdf2)
-SYM (__ltdf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(20),sp@-
-	movl	a6@(16),sp@-
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpdf2_internal)
-	unlk	a6
-	rts
-#endif /* L_ltdf2 */
-
-#ifdef  L_ledf2
-	.text
-	FUNC(__ledf2)
-	.globl	SYM (__ledf2)
-SYM (__ledf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(20),sp@-
-	movl	a6@(16),sp@-
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpdf2_internal)
-	unlk	a6
-	rts
-#endif /* L_ledf2 */
-
-| The comments above about __eqdf2, et. al., also apply to __eqsf2,
-| et. al., except that the latter call __cmpsf2 rather than __cmpdf2.
-
-#ifdef  L_eqsf2
-	.text
-	FUNC(__eqsf2)
-	.globl	SYM (__eqsf2)
-SYM (__eqsf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpsf2_internal)
-	unlk	a6
-	rts
-#endif /* L_eqsf2 */
-
-#ifdef  L_nesf2
-	.text
-	FUNC(__nesf2)
-	.globl	SYM (__nesf2)
-SYM (__nesf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpsf2_internal)
-	unlk	a6
-	rts
-#endif /* L_nesf2 */
-
-#ifdef  L_gtsf2
-	.text
-	FUNC(__gtsf2)
-	.globl	SYM (__gtsf2)
-SYM (__gtsf2):
-	link	a6,IMM (0)
-	pea	-1
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpsf2_internal)
-	unlk	a6
-	rts
-#endif /* L_gtsf2 */
-
-#ifdef  L_gesf2
-	.text
-	FUNC(__gesf2)
-	.globl	SYM (__gesf2)
-SYM (__gesf2):
-	link	a6,IMM (0)
-	pea	-1
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpsf2_internal)
-	unlk	a6
-	rts
-#endif /* L_gesf2 */
-
-#ifdef  L_ltsf2
-	.text
-	FUNC(__ltsf2)
-	.globl	SYM (__ltsf2)
-SYM (__ltsf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpsf2_internal)
-	unlk	a6
-	rts
-#endif /* L_ltsf2 */
-
-#ifdef  L_lesf2
-	.text
-	FUNC(__lesf2)
-	.globl	SYM (__lesf2)
-SYM (__lesf2):
-	link	a6,IMM (0)
-	pea	1
-	movl	a6@(12),sp@-
-	movl	a6@(8),sp@-
-	PICCALL	SYM (__cmpsf2_internal)
-	unlk	a6
-	rts
-#endif /* L_lesf2 */
 
 #if defined (__ELF__) && defined (__linux__)
 	/* Make stack non-executable for ELF linux targets.  */
diff -rupN gcc-4.6.4.test/gcc/config/m68k/linux.h gcc-4.6.4-fastcall/gcc/config/m68k/linux.h
--- gcc-4.6.4.test/gcc/config/m68k/linux.h	2011-01-26 21:30:12.000000000 +0100
+++ gcc-4.6.4-fastcall/gcc/config/m68k/linux.h	2017-04-30 19:10:52.555947001 +0200
@@ -171,7 +171,7 @@ along with GCC; see the file COPYING3.
 
 #undef FUNCTION_VALUE
 #define FUNCTION_VALUE(VALTYPE, FUNC)					\
-  m68k_function_value (VALTYPE, FUNC)
+  m68k_function_value (VALTYPE, FUNC, true)
 
 /* Define how to find the value returned by a library function
    assuming the value has mode MODE.
@@ -180,7 +180,7 @@ along with GCC; see the file COPYING3.
 
 #undef LIBCALL_VALUE
 #define LIBCALL_VALUE(MODE)						\
-  m68k_libcall_value (MODE)
+  m68k_libcall_value (MODE, true)
 
 /* For m68k SVR4, structures are returned using the reentrant
    technique.  */
diff -rupN gcc-4.6.4.test/gcc/config/m68k/m68k-protos.h gcc-4.6.4-fastcall/gcc/config/m68k/m68k-protos.h
--- gcc-4.6.4.test/gcc/config/m68k/m68k-protos.h	2010-09-16 13:59:39.000000000 +0200
+++ gcc-4.6.4-fastcall/gcc/config/m68k/m68k-protos.h	2017-04-30 19:10:52.555947001 +0200
@@ -62,8 +62,8 @@ extern rtx legitimize_pic_address (rtx,
 extern rtx m68k_legitimize_tls_address (rtx);
 extern bool m68k_tls_reference_p (rtx, bool);
 extern int valid_dbcc_comparison_p_2 (rtx, enum machine_mode);
-extern rtx m68k_libcall_value (enum machine_mode);
-extern rtx m68k_function_value (const_tree, const_tree);
+extern rtx m68k_libcall_value (enum machine_mode, bool);
+extern rtx m68k_function_value (const_tree, const_tree, bool);
 extern int emit_move_sequence (rtx *, enum machine_mode, rtx);
 extern bool m68k_movem_pattern_p (rtx, rtx, HOST_WIDE_INT, bool);
 extern const char *m68k_output_movem (rtx *, rtx, HOST_WIDE_INT, bool);
@@ -77,6 +77,8 @@ extern rtx m68k_unwrap_symbol (rtx, bool
 extern enum attr_cpu m68k_sched_cpu;
 extern enum attr_mac m68k_sched_mac;
 
+extern void m68k_setup_sched_options ();
+
 extern enum attr_opx_type m68k_sched_attr_opx_type (rtx, int);
 extern enum attr_opy_type m68k_sched_attr_opy_type (rtx, int);
 extern enum attr_size m68k_sched_attr_size (rtx);
@@ -100,3 +102,12 @@ extern void init_68881_table (void);
 extern rtx m68k_legitimize_call_address (rtx);
 extern rtx m68k_legitimize_sibcall_address (rtx);
 extern int m68k_hard_regno_rename_ok(unsigned int, unsigned int);
+
+#ifdef RTX_CODE
+#ifdef TREE_CODE
+extern void m68k_init_cumulative_args (CUMULATIVE_ARGS *, tree);
+extern void m68k_function_arg_advance (CUMULATIVE_ARGS *);
+extern rtx m68k_function_arg (CUMULATIVE_ARGS *, enum machine_mode,
+			      const_tree, bool);
+#endif
+#endif
diff -rupN gcc-4.6.4.test/gcc/config/m68k/m68k-sched.inc gcc-4.6.4-fastcall/gcc/config/m68k/m68k-sched.inc
--- gcc-4.6.4.test/gcc/config/m68k/m68k-sched.inc	1970-01-01 01:00:00.000000000 +0100
+++ gcc-4.6.4-fastcall/gcc/config/m68k/m68k-sched.inc	2017-04-30 19:10:52.555947001 +0200
@@ -0,0 +1,1235 @@
+static int m68k_sched_adjust_cost (rtx, rtx, rtx, int);
+static int m68k_sched_issue_rate (void);
+static int m68k_sched_variable_issue (FILE *, int, rtx, int);
+static void m68k_sched_md_init_global (FILE *, int, int);
+static void m68k_sched_md_finish_global (FILE *, int);
+static void m68k_sched_md_init (FILE *, int, int);
+static void m68k_sched_dfa_pre_advance_cycle (void);
+static void m68k_sched_dfa_post_advance_cycle (void);
+static int m68k_sched_first_cycle_multipass_dfa_lookahead (void);
+
+
+#undef TARGET_SCHED_ADJUST_COST
+#define TARGET_SCHED_ADJUST_COST m68k_sched_adjust_cost
+
+#undef TARGET_SCHED_ISSUE_RATE
+#define TARGET_SCHED_ISSUE_RATE m68k_sched_issue_rate
+
+#undef TARGET_SCHED_VARIABLE_ISSUE
+#define TARGET_SCHED_VARIABLE_ISSUE m68k_sched_variable_issue
+
+#undef TARGET_SCHED_INIT_GLOBAL
+#define TARGET_SCHED_INIT_GLOBAL m68k_sched_md_init_global
+
+#undef TARGET_SCHED_FINISH_GLOBAL
+#define TARGET_SCHED_FINISH_GLOBAL m68k_sched_md_finish_global
+
+#undef TARGET_SCHED_INIT
+#define TARGET_SCHED_INIT m68k_sched_md_init
+
+#undef TARGET_SCHED_DFA_PRE_ADVANCE_CYCLE
+#define TARGET_SCHED_DFA_PRE_ADVANCE_CYCLE m68k_sched_dfa_pre_advance_cycle
+
+#undef TARGET_SCHED_DFA_POST_ADVANCE_CYCLE
+#define TARGET_SCHED_DFA_POST_ADVANCE_CYCLE m68k_sched_dfa_post_advance_cycle
+
+#undef TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD
+#define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD	\
+  m68k_sched_first_cycle_multipass_dfa_lookahead
+
+
+/* CPU to schedule the program for.  */
+enum attr_cpu m68k_sched_cpu;
+
+/* MAC to schedule the program for.  */
+enum attr_mac m68k_sched_mac;
+
+/* Operand type.  */
+enum attr_op_type
+  {
+    /* No operand.  */
+    OP_TYPE_NONE,
+
+    /* Integer register.  */
+    OP_TYPE_RN,
+
+    /* FP register.  */
+    OP_TYPE_FPN,
+
+    /* Implicit mem reference (e.g. stack).  */
+    OP_TYPE_MEM1,
+
+    /* Memory without offset or indexing.  EA modes 2, 3 and 4.  */
+    OP_TYPE_MEM234,
+
+    /* Memory with offset but without indexing.  EA mode 5.  */
+    OP_TYPE_MEM5,
+
+    /* Memory with indexing.  EA mode 6.  */
+    OP_TYPE_MEM6,
+
+    /* Memory referenced by absolute address.  EA mode 7.  */
+    OP_TYPE_MEM7,
+
+    /* Immediate operand that doesn't require extension word.  */
+    OP_TYPE_IMM_Q,
+
+    /* Immediate 16 bit operand.  */
+    OP_TYPE_IMM_W,
+
+    /* Immediate 32 bit operand.  */
+    OP_TYPE_IMM_L
+  };
+
+/* Setup scheduling options.  */
+void 
+m68k_setup_sched_options ()
+{
+  if (TUNE_CFV1)
+    m68k_sched_cpu = CPU_CFV1;
+  else if (TUNE_CFV2)
+    m68k_sched_cpu = CPU_CFV2;
+  else if (TUNE_CFV3)
+    m68k_sched_cpu = CPU_CFV3;
+  else if (TUNE_CFV4)
+    m68k_sched_cpu = CPU_CFV4;
+  else
+    {
+      m68k_sched_cpu = CPU_UNKNOWN;
+      flag_schedule_insns = 0;
+      flag_schedule_insns_after_reload = 0;
+      flag_modulo_sched = 0;
+    }
+
+  if (m68k_sched_cpu != CPU_UNKNOWN)
+    {
+      if ((m68k_cpu_flags & (FL_CF_EMAC | FL_CF_EMAC_B)) != 0)
+        m68k_sched_mac = MAC_CF_EMAC;
+      else if ((m68k_cpu_flags & FL_CF_MAC) != 0)
+        m68k_sched_mac = MAC_CF_MAC;
+      else
+        m68k_sched_mac = MAC_NO;
+    }
+}
+
+/* Return type of memory ADDR_RTX refers to.  */
+static enum attr_op_type
+sched_address_type (enum machine_mode mode, rtx addr_rtx)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  struct m68k_address address;
+
+  if (symbolic_operand (addr_rtx, VOIDmode))
+    return OP_TYPE_MEM7;
+
+  if (!m68k_decompose_address (mode, addr_rtx,
+        		       reload_completed, &address))
+    {
+      gcc_assert (!reload_completed);
+      /* Reload will likely fix the address to be in the register.  */
+      return OP_TYPE_MEM234;
+    }
+
+  if (address.scale != 0)
+    return OP_TYPE_MEM6;
+
+  if (address.base != NULL_RTX)
+    {
+      if (address.offset == NULL_RTX)
+        return OP_TYPE_MEM234;
+
+      return OP_TYPE_MEM5;
+    }
+
+  gcc_assert (address.offset != NULL_RTX);
+
+  return OP_TYPE_MEM7;
+}
+
+/* Return X or Y (depending on OPX_P) operand of INSN.  */
+static rtx
+sched_get_operand (rtx insn, bool opx_p)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  int i;
+
+  if (recog_memoized (insn) < 0)
+    gcc_unreachable ();
+
+  extract_constrain_insn_cached (insn);
+
+  if (opx_p)
+    i = get_attr_opx (insn);
+  else
+    i = get_attr_opy (insn);
+
+  if (i >= recog_data.n_operands)
+    return NULL;
+
+  return recog_data.operand[i];
+}
+
+/* Return type of INSN's operand X (if OPX_P) or operand Y (if !OPX_P).
+   If ADDRESS_P is true, return type of memory location operand refers to.  */
+static enum attr_op_type
+sched_attr_op_type (rtx insn, bool opx_p, bool address_p)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  rtx op;
+
+  op = sched_get_operand (insn, opx_p);
+
+  if (op == NULL)
+    {
+      gcc_assert (!reload_completed);
+      return OP_TYPE_RN;
+    }
+
+  if (address_p)
+    return sched_address_type (QImode, op);
+
+  if (memory_operand (op, VOIDmode))
+    return sched_address_type (GET_MODE (op), XEXP (op, 0));
+
+  if (register_operand (op, VOIDmode))
+    {
+      if ((!reload_completed && FLOAT_MODE_P (GET_MODE (op)))
+          || (reload_completed && FP_REG_P (op)))
+        return OP_TYPE_FPN;
+
+      return OP_TYPE_RN;
+    }
+
+  if (GET_CODE (op) == CONST_INT)
+    {
+      int ival;
+
+      ival = INTVAL (op);
+
+      /* Check for quick constants.  */
+      switch (get_attr_type (insn))
+        {
+        case TYPE_ALUQ_L:
+          if (IN_RANGE (ival, 1, 8) || IN_RANGE (ival, -8, -1))
+            return OP_TYPE_IMM_Q;
+
+          gcc_assert (!reload_completed);
+          break;
+
+        case TYPE_MOVEQ_L:
+          if (USE_MOVQ (ival))
+            return OP_TYPE_IMM_Q;
+
+          gcc_assert (!reload_completed);
+          break;
+
+        case TYPE_MOV3Q_L:
+          if (valid_mov3q_const (ival))
+            return OP_TYPE_IMM_Q;
+
+          gcc_assert (!reload_completed);
+          break;
+
+        default:
+          break;
+        }
+
+      if (IN_RANGE (ival, -0x8000, 0x7fff))
+        return OP_TYPE_IMM_W;
+
+      return OP_TYPE_IMM_L;
+    }
+
+  if (GET_CODE (op) == CONST_DOUBLE)
+    {
+      switch (GET_MODE (op))
+        {
+        case SFmode:
+          return OP_TYPE_IMM_W;
+
+        case VOIDmode:
+        case DFmode:
+          return OP_TYPE_IMM_L;
+
+        default:
+          gcc_unreachable ();
+        }
+    }
+
+  if (GET_CODE (op) == CONST
+      || symbolic_operand (op, VOIDmode)
+      || LABEL_P (op))
+    {
+      switch (GET_MODE (op))
+        {
+        case QImode:
+          return OP_TYPE_IMM_Q;
+
+        case HImode:
+          return OP_TYPE_IMM_W;
+
+        case SImode:
+          return OP_TYPE_IMM_L;
+
+        default:
+          if (symbolic_operand (m68k_unwrap_symbol (op, false), VOIDmode))
+            /* Just a guess.  */
+            return OP_TYPE_IMM_W;
+
+          return OP_TYPE_IMM_L;
+        }
+    }
+
+  gcc_assert (!reload_completed);
+
+  if (FLOAT_MODE_P (GET_MODE (op)))
+    return OP_TYPE_FPN;
+
+  return OP_TYPE_RN;
+}
+
+/* Implement opx_type attribute.
+   Return type of INSN's operand X.
+   If ADDRESS_P is true, return type of memory location operand refers to.  */
+enum attr_opx_type
+m68k_sched_attr_opx_type (rtx insn, int address_p)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  switch (sched_attr_op_type (insn, true, address_p != 0))
+    {
+    case OP_TYPE_RN:
+      return OPX_TYPE_RN;
+
+    case OP_TYPE_FPN:
+      return OPX_TYPE_FPN;
+
+    case OP_TYPE_MEM1:
+      return OPX_TYPE_MEM1;
+
+    case OP_TYPE_MEM234:
+      return OPX_TYPE_MEM234;
+
+    case OP_TYPE_MEM5:
+      return OPX_TYPE_MEM5;
+
+    case OP_TYPE_MEM6:
+      return OPX_TYPE_MEM6;
+
+    case OP_TYPE_MEM7:
+      return OPX_TYPE_MEM7;
+
+    case OP_TYPE_IMM_Q:
+      return OPX_TYPE_IMM_Q;
+
+    case OP_TYPE_IMM_W:
+      return OPX_TYPE_IMM_W;
+
+    case OP_TYPE_IMM_L:
+      return OPX_TYPE_IMM_L;
+
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Implement opy_type attribute.
+   Return type of INSN's operand Y.
+   If ADDRESS_P is true, return type of memory location operand refers to.  */
+enum attr_opy_type
+m68k_sched_attr_opy_type (rtx insn, int address_p)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  switch (sched_attr_op_type (insn, false, address_p != 0))
+    {
+    case OP_TYPE_RN:
+      return OPY_TYPE_RN;
+
+    case OP_TYPE_FPN:
+      return OPY_TYPE_FPN;
+
+    case OP_TYPE_MEM1:
+      return OPY_TYPE_MEM1;
+
+    case OP_TYPE_MEM234:
+      return OPY_TYPE_MEM234;
+
+    case OP_TYPE_MEM5:
+      return OPY_TYPE_MEM5;
+
+    case OP_TYPE_MEM6:
+      return OPY_TYPE_MEM6;
+
+    case OP_TYPE_MEM7:
+      return OPY_TYPE_MEM7;
+
+    case OP_TYPE_IMM_Q:
+      return OPY_TYPE_IMM_Q;
+
+    case OP_TYPE_IMM_W:
+      return OPY_TYPE_IMM_W;
+
+    case OP_TYPE_IMM_L:
+      return OPY_TYPE_IMM_L;
+
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Return size of INSN as int.  */
+static int
+sched_get_attr_size_int (rtx insn)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  int size;
+
+  switch (get_attr_type (insn))
+    {
+    case TYPE_IGNORE:
+      /* There should be no references to m68k_sched_attr_size for 'ignore'
+         instructions.  */
+      gcc_unreachable ();
+      return 0;
+
+    case TYPE_MUL_L:
+      size = 2;
+      break;
+
+    default:
+      size = 1;
+      break;
+    }
+
+  switch (get_attr_opx_type (insn))
+    {
+    case OPX_TYPE_NONE:
+    case OPX_TYPE_RN:
+    case OPX_TYPE_FPN:
+    case OPX_TYPE_MEM1:
+    case OPX_TYPE_MEM234:
+    case OPY_TYPE_IMM_Q:
+      break;
+
+    case OPX_TYPE_MEM5:
+    case OPX_TYPE_MEM6:
+      /* Here we assume that most absolute references are short.  */
+    case OPX_TYPE_MEM7:
+    case OPY_TYPE_IMM_W:
+      ++size;
+      break;
+
+    case OPY_TYPE_IMM_L:
+      size += 2;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  switch (get_attr_opy_type (insn))
+    {
+    case OPY_TYPE_NONE:
+    case OPY_TYPE_RN:
+    case OPY_TYPE_FPN:
+    case OPY_TYPE_MEM1:
+    case OPY_TYPE_MEM234:
+    case OPY_TYPE_IMM_Q:
+      break;
+
+    case OPY_TYPE_MEM5:
+    case OPY_TYPE_MEM6:
+      /* Here we assume that most absolute references are short.  */
+    case OPY_TYPE_MEM7:
+    case OPY_TYPE_IMM_W:
+      ++size;
+      break;
+
+    case OPY_TYPE_IMM_L:
+      size += 2;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  if (size > 3)
+    {
+      gcc_assert (!reload_completed);
+
+      size = 3;
+    }
+
+  return size;
+}
+
+/* Return size of INSN as attribute enum value.  */
+enum attr_size
+m68k_sched_attr_size (rtx insn)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  switch (sched_get_attr_size_int (insn))
+    {
+    case 1:
+      return SIZE_1;
+
+    case 2:
+      return SIZE_2;
+
+    case 3:
+      return SIZE_3;
+
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Return operand X or Y (depending on OPX_P) of INSN,
+   if it is a MEM, or NULL overwise.  */
+static enum attr_op_type
+sched_get_opxy_mem_type (rtx insn, bool opx_p)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  if (opx_p)
+    {
+      switch (get_attr_opx_type (insn))
+        {
+        case OPX_TYPE_NONE:
+        case OPX_TYPE_RN:
+        case OPX_TYPE_FPN:
+        case OPX_TYPE_IMM_Q:
+        case OPX_TYPE_IMM_W:
+        case OPX_TYPE_IMM_L:
+          return OP_TYPE_RN;
+
+        case OPX_TYPE_MEM1:
+        case OPX_TYPE_MEM234:
+        case OPX_TYPE_MEM5:
+        case OPX_TYPE_MEM7:
+          return OP_TYPE_MEM1;
+
+        case OPX_TYPE_MEM6:
+          return OP_TYPE_MEM6;
+
+        default:
+          gcc_unreachable ();
+        }
+    }
+  else
+    {
+      switch (get_attr_opy_type (insn))
+        {
+        case OPY_TYPE_NONE:
+        case OPY_TYPE_RN:
+        case OPY_TYPE_FPN:
+        case OPY_TYPE_IMM_Q:
+        case OPY_TYPE_IMM_W:
+        case OPY_TYPE_IMM_L:
+          return OP_TYPE_RN;
+
+        case OPY_TYPE_MEM1:
+        case OPY_TYPE_MEM234:
+        case OPY_TYPE_MEM5:
+        case OPY_TYPE_MEM7:
+          return OP_TYPE_MEM1;
+
+        case OPY_TYPE_MEM6:
+          return OP_TYPE_MEM6;
+
+        default:
+          gcc_unreachable ();
+        }
+    }
+}
+
+/* Implement op_mem attribute.  */
+enum attr_op_mem
+m68k_sched_attr_op_mem (rtx insn)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  enum attr_op_type opx;
+  enum attr_op_type opy;
+
+  opx = sched_get_opxy_mem_type (insn, true);
+  opy = sched_get_opxy_mem_type (insn, false);
+
+  if (opy == OP_TYPE_RN && opx == OP_TYPE_RN)
+    return OP_MEM_00;
+
+  if (opy == OP_TYPE_RN && opx == OP_TYPE_MEM1)
+    {
+      switch (get_attr_opx_access (insn))
+        {
+        case OPX_ACCESS_R:
+          return OP_MEM_10;
+
+        case OPX_ACCESS_W:
+          return OP_MEM_01;
+
+        case OPX_ACCESS_RW:
+          return OP_MEM_11;
+
+        default:
+          gcc_unreachable ();
+        }
+    }
+
+  if (opy == OP_TYPE_RN && opx == OP_TYPE_MEM6)
+    {
+      switch (get_attr_opx_access (insn))
+        {
+        case OPX_ACCESS_R:
+          return OP_MEM_I0;
+
+        case OPX_ACCESS_W:
+          return OP_MEM_0I;
+
+        case OPX_ACCESS_RW:
+          return OP_MEM_I1;
+
+        default:
+          gcc_unreachable ();
+        }
+    }
+
+  if (opy == OP_TYPE_MEM1 && opx == OP_TYPE_RN)
+    return OP_MEM_10;
+
+  if (opy == OP_TYPE_MEM1 && opx == OP_TYPE_MEM1)
+    {
+      switch (get_attr_opx_access (insn))
+        {
+        case OPX_ACCESS_W:
+          return OP_MEM_11;
+
+        default:
+          gcc_assert (!reload_completed);
+          return OP_MEM_11;
+        }
+    }
+
+  if (opy == OP_TYPE_MEM1 && opx == OP_TYPE_MEM6)
+    {
+      switch (get_attr_opx_access (insn))
+        {
+        case OPX_ACCESS_W:
+          return OP_MEM_1I;
+
+        default:
+          gcc_assert (!reload_completed);
+          return OP_MEM_1I;
+        }
+    }
+
+  if (opy == OP_TYPE_MEM6 && opx == OP_TYPE_RN)
+    return OP_MEM_I0;
+
+  if (opy == OP_TYPE_MEM6 && opx == OP_TYPE_MEM1)
+    {
+      switch (get_attr_opx_access (insn))
+        {
+        case OPX_ACCESS_W:
+          return OP_MEM_I1;
+
+        default:
+          gcc_assert (!reload_completed);
+          return OP_MEM_I1;
+        }
+    }
+
+  gcc_assert (opy == OP_TYPE_MEM6 && opx == OP_TYPE_MEM6);
+  gcc_assert (!reload_completed);
+  return OP_MEM_I1;
+}
+
+/* Jump instructions types.  Indexed by INSN_UID.
+   The same rtl insn can be expanded into different asm instructions
+   depending on the cc0_status.  To properly determine type of jump
+   instructions we scan instruction stream and map jumps types to this
+   array.  */
+static enum attr_type *sched_branch_type;
+
+/* Return the type of the jump insn.  */
+enum attr_type
+m68k_sched_branch_type (rtx insn)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  enum attr_type type;
+
+  type = sched_branch_type[INSN_UID (insn)];
+
+  gcc_assert (type != 0);
+
+  return type;
+}
+
+/* Data for ColdFire V4 index bypass.
+   Producer modifies register that is used as index in consumer with
+   specified scale.  */
+static struct
+{
+  /* Producer instruction.  */
+  rtx pro;
+
+  /* Consumer instruction.  */
+  rtx con;
+
+  /* Scale of indexed memory access within consumer.
+     Or zero if bypass should not be effective at the moment.  */
+  int scale;
+} sched_cfv4_bypass_data;
+
+/* An empty state that is used in m68k_sched_adjust_cost.  */
+static state_t sched_adjust_cost_state;
+
+/* Implement adjust_cost scheduler hook.
+   Return adjusted COST of dependency LINK between DEF_INSN and INSN.  */
+static int
+m68k_sched_adjust_cost (rtx insn, rtx link ATTRIBUTE_UNUSED, rtx def_insn,
+        		int cost)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  int delay;
+
+  if (recog_memoized (def_insn) < 0
+      || recog_memoized (insn) < 0)
+    return cost;
+
+  if (sched_cfv4_bypass_data.scale == 1)
+    /* Handle ColdFire V4 bypass for indexed address with 1x scale.  */
+    {
+      /* haifa-sched.c: insn_cost () calls bypass_p () just before
+         targetm.sched.adjust_cost ().  Hence, we can be relatively sure
+         that the data in sched_cfv4_bypass_data is up to date.  */
+      gcc_assert (sched_cfv4_bypass_data.pro == def_insn
+        	  && sched_cfv4_bypass_data.con == insn);
+
+      if (cost < 3)
+        cost = 3;
+
+      sched_cfv4_bypass_data.pro = NULL;
+      sched_cfv4_bypass_data.con = NULL;
+      sched_cfv4_bypass_data.scale = 0;
+    }
+  else
+    gcc_assert (sched_cfv4_bypass_data.pro == NULL
+        	&& sched_cfv4_bypass_data.con == NULL
+        	&& sched_cfv4_bypass_data.scale == 0);
+
+  /* Don't try to issue INSN earlier than DFA permits.
+     This is especially useful for instructions that write to memory,
+     as their true dependence (default) latency is better to be set to 0
+     to workaround alias analysis limitations.
+     This is, in fact, a machine independent tweak, so, probably,
+     it should be moved to haifa-sched.c: insn_cost ().  */
+  delay = min_insn_conflict_delay (sched_adjust_cost_state, def_insn, insn);
+  if (delay > cost)
+    cost = delay;
+
+  return cost;
+}
+
+/* Return maximal number of insns that can be scheduled on a single cycle.  */
+static int
+m68k_sched_issue_rate (void)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  switch (m68k_sched_cpu)
+    {
+    case CPU_CFV1:
+    case CPU_CFV2:
+    case CPU_CFV3:
+      return 1;
+
+    case CPU_CFV4:
+      return 2;
+
+    default:
+      gcc_unreachable ();
+      return 0;
+    }
+}
+
+/* Maximal length of instruction for current CPU.
+   E.g. it is 3 for any ColdFire core.  */
+static int max_insn_size;
+
+/* Data to model instruction buffer of CPU.  */
+struct _sched_ib
+{
+  /* True if instruction buffer model is modeled for current CPU.  */
+  bool enabled_p;
+
+  /* Size of the instruction buffer in words.  */
+  int size;
+
+  /* Number of filled words in the instruction buffer.  */
+  int filled;
+
+  /* Additional information about instruction buffer for CPUs that have
+     a buffer of instruction records, rather then a plain buffer
+     of instruction words.  */
+  struct _sched_ib_records
+  {
+    /* Size of buffer in records.  */
+    int n_insns;
+
+    /* Array to hold data on adjustements made to the size of the buffer.  */
+    int *adjust;
+
+    /* Index of the above array.  */
+    int adjust_index;
+  } records;
+
+  /* An insn that reserves (marks empty) one word in the instruction buffer.  */
+  rtx insn;
+};
+
+static struct _sched_ib sched_ib;
+
+/* ID of memory unit.  */
+static int sched_mem_unit_code;
+
+/* Implementation of the targetm.sched.variable_issue () hook.
+   It is called after INSN was issued.  It returns the number of insns
+   that can possibly get scheduled on the current cycle.
+   It is used here to determine the effect of INSN on the instruction
+   buffer.  */
+static int
+m68k_sched_variable_issue (FILE *sched_dump ATTRIBUTE_UNUSED,
+        		   int sched_verbose ATTRIBUTE_UNUSED,
+        		   rtx insn, int can_issue_more)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  int insn_size;
+
+  if (recog_memoized (insn) >= 0 && get_attr_type (insn) != TYPE_IGNORE)
+    {
+      switch (m68k_sched_cpu)
+        {
+        case CPU_CFV1:
+        case CPU_CFV2:
+          insn_size = sched_get_attr_size_int (insn);
+          break;
+
+        case CPU_CFV3:
+          insn_size = sched_get_attr_size_int (insn);
+          
+          /* ColdFire V3 and V4 cores have instruction buffers that can
+             accumulate up to 8 instructions regardless of instructions'
+             sizes.  So we should take care not to "prefetch" 24 one-word
+             or 12 two-words instructions.
+             To model this behavior we temporarily decrease size of the
+             buffer by (max_insn_size - insn_size) for next 7 instructions.  */
+          {
+            int adjust;
+
+            adjust = max_insn_size - insn_size;
+            sched_ib.size -= adjust;
+
+            if (sched_ib.filled > sched_ib.size)
+              sched_ib.filled = sched_ib.size;
+
+            sched_ib.records.adjust[sched_ib.records.adjust_index] = adjust;
+          }
+
+          ++sched_ib.records.adjust_index;
+          if (sched_ib.records.adjust_index == sched_ib.records.n_insns)
+            sched_ib.records.adjust_index = 0;
+
+          /* Undo adjustement we did 7 instructions ago.  */
+          sched_ib.size
+            += sched_ib.records.adjust[sched_ib.records.adjust_index];
+
+          break;
+
+        case CPU_CFV4:
+          gcc_assert (!sched_ib.enabled_p);
+          insn_size = 0;
+          break;
+
+        default:
+          gcc_unreachable ();
+        }
+
+      gcc_assert (insn_size <= sched_ib.filled);
+      --can_issue_more;
+    }
+  else if (GET_CODE (PATTERN (insn)) == ASM_INPUT
+           || asm_noperands (PATTERN (insn)) >= 0)
+    insn_size = sched_ib.filled;
+  else
+    insn_size = 0;
+
+  sched_ib.filled -= insn_size;
+
+  return can_issue_more;
+}
+
+/* Return how many instructions should scheduler lookahead to choose the
+   best one.  */
+static int
+m68k_sched_first_cycle_multipass_dfa_lookahead (void)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  return m68k_sched_issue_rate () - 1;
+}
+
+/* Implementation of targetm.sched.init_global () hook.
+   It is invoked once per scheduling pass and is used here
+   to initialize scheduler constants.  */
+static void
+m68k_sched_md_init_global (FILE *sched_dump ATTRIBUTE_UNUSED,
+        		   int sched_verbose ATTRIBUTE_UNUSED,
+        		   int n_insns ATTRIBUTE_UNUSED)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  /* Init branch types.  */
+  {
+    rtx insn;
+
+    sched_branch_type = XCNEWVEC (enum attr_type, get_max_uid () + 1);
+
+    for (insn = get_insns (); insn != NULL_RTX; insn = NEXT_INSN (insn))
+      {
+        if (JUMP_P (insn))
+          /* !!! FIXME: Implement real scan here.  */
+          sched_branch_type[INSN_UID (insn)] = TYPE_BCC;
+      }
+  }
+
+#ifdef ENABLE_CHECKING
+  /* Check that all instructions have DFA reservations and
+     that all instructions can be issued from a clean state.  */
+  {
+    rtx insn;
+    state_t state;
+
+    state = alloca (state_size ());
+
+    for (insn = get_insns (); insn != NULL_RTX; insn = NEXT_INSN (insn))
+      {
+ 	if (INSN_P (insn) && recog_memoized (insn) >= 0)
+          {
+ 	    gcc_assert (insn_has_dfa_reservation_p (insn));
+
+ 	    state_reset (state);
+ 	    if (state_transition (state, insn) >= 0)
+ 	      gcc_unreachable ();
+ 	  }
+      }
+  }
+#endif
+
+  /* Setup target cpu.  */
+
+  /* ColdFire V4 has a set of features to keep its instruction buffer full
+     (e.g., a separate memory bus for instructions) and, hence, we do not model
+     buffer for this CPU.  */
+  sched_ib.enabled_p = (m68k_sched_cpu != CPU_CFV4);
+
+  switch (m68k_sched_cpu)
+    {
+    case CPU_CFV4:
+      sched_ib.filled = 0;
+
+      /* FALLTHRU */
+
+    case CPU_CFV1:
+    case CPU_CFV2:
+      max_insn_size = 3;
+      sched_ib.records.n_insns = 0;
+      sched_ib.records.adjust = NULL;
+      break;
+
+    case CPU_CFV3:
+      max_insn_size = 3;
+      sched_ib.records.n_insns = 8;
+      sched_ib.records.adjust = XNEWVEC (int, sched_ib.records.n_insns);
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  sched_mem_unit_code = get_cpu_unit_code ("cf_mem1");
+
+  sched_adjust_cost_state = xmalloc (state_size ());
+  state_reset (sched_adjust_cost_state);
+
+  start_sequence ();
+  emit_insn (gen_ib ());
+  sched_ib.insn = get_insns ();
+  end_sequence ();
+}
+
+/* Scheduling pass is now finished.  Free/reset static variables.  */
+static void
+m68k_sched_md_finish_global (FILE *dump ATTRIBUTE_UNUSED,
+        		     int verbose ATTRIBUTE_UNUSED)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  sched_ib.insn = NULL;
+
+  free (sched_adjust_cost_state);
+  sched_adjust_cost_state = NULL;
+
+  sched_mem_unit_code = 0;
+
+  free (sched_ib.records.adjust);
+  sched_ib.records.adjust = NULL;
+  sched_ib.records.n_insns = 0;
+  max_insn_size = 0;
+
+  free (sched_branch_type);
+  sched_branch_type = NULL;
+}
+
+/* Implementation of targetm.sched.init () hook.
+   It is invoked each time scheduler starts on the new block (basic block or
+   extended basic block).  */
+static void
+m68k_sched_md_init (FILE *sched_dump ATTRIBUTE_UNUSED,
+        	    int sched_verbose ATTRIBUTE_UNUSED,
+        	    int n_insns ATTRIBUTE_UNUSED)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  switch (m68k_sched_cpu)
+    {
+    case CPU_CFV1:
+    case CPU_CFV2:
+      sched_ib.size = 6;
+      break;
+
+    case CPU_CFV3:
+      sched_ib.size = sched_ib.records.n_insns * max_insn_size;
+
+      memset (sched_ib.records.adjust, 0,
+              sched_ib.records.n_insns * sizeof (*sched_ib.records.adjust));
+      sched_ib.records.adjust_index = 0;
+      break;
+
+    case CPU_CFV4:
+      gcc_assert (!sched_ib.enabled_p);
+      sched_ib.size = 0;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  if (sched_ib.enabled_p)
+    /* haifa-sched.c: schedule_block () calls advance_cycle () just before
+       the first cycle.  Workaround that.  */
+    sched_ib.filled = -2;
+}
+
+/* Implementation of targetm.sched.dfa_pre_advance_cycle () hook.
+   It is invoked just before current cycle finishes and is used here
+   to track if instruction buffer got its two words this cycle.  */
+static void
+m68k_sched_dfa_pre_advance_cycle (void)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  if (!sched_ib.enabled_p)
+    return;
+
+  if (!cpu_unit_reservation_p (curr_state, sched_mem_unit_code))
+    {
+      sched_ib.filled += 2;
+
+      if (sched_ib.filled > sched_ib.size)
+        sched_ib.filled = sched_ib.size;
+    }
+}
+
+/* Implementation of targetm.sched.dfa_post_advance_cycle () hook.
+   It is invoked just after new cycle begins and is used here
+   to setup number of filled words in the instruction buffer so that
+   instructions which won't have all their words prefetched would be
+   stalled for a cycle.  */
+static void
+m68k_sched_dfa_post_advance_cycle (void)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  int i;
+
+  if (!sched_ib.enabled_p)
+    return;
+
+  /* Setup number of prefetched instruction words in the instruction
+     buffer.  */
+  i = max_insn_size - sched_ib.filled;
+
+  while (--i >= 0)
+    {
+      if (state_transition (curr_state, sched_ib.insn) >= 0)
+        gcc_unreachable ();
+    }
+}
+
+/* Return X or Y (depending on OPX_P) operand of INSN,
+   if it is an integer register, or NULL overwise.  */
+static rtx
+sched_get_reg_operand (rtx insn, bool opx_p)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  rtx op = NULL;
+
+  if (opx_p)
+    {
+      if (get_attr_opx_type (insn) == OPX_TYPE_RN)
+        {
+          op = sched_get_operand (insn, true);
+          gcc_assert (op != NULL);
+
+          if (!reload_completed && !REG_P (op))
+            return NULL;
+        }
+    }
+  else
+    {
+      if (get_attr_opy_type (insn) == OPY_TYPE_RN)
+        {
+          op = sched_get_operand (insn, false);
+          gcc_assert (op != NULL);
+
+          if (!reload_completed && !REG_P (op))
+            return NULL;
+        }
+    }
+
+  return op;
+}
+
+/* Return true, if X or Y (depending on OPX_P) operand of INSN
+   is a MEM.  */
+static bool
+sched_mem_operand_p (rtx insn, bool opx_p)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  switch (sched_get_opxy_mem_type (insn, opx_p))
+    {
+    case OP_TYPE_MEM1:
+    case OP_TYPE_MEM6:
+      return true;
+
+    default:
+      return false;
+    }
+}
+
+/* Return X or Y (depending on OPX_P) operand of INSN,
+   if it is a MEM, or NULL overwise.  */
+static rtx
+sched_get_mem_operand (rtx insn, bool must_read_p, bool must_write_p)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  bool opx_p;
+  bool opy_p;
+
+  opx_p = false;
+  opy_p = false;
+
+  if (must_read_p)
+    {
+      opx_p = true;
+      opy_p = true;
+    }
+
+  if (must_write_p)
+    {
+      opx_p = true;
+      opy_p = false;
+    }
+
+  if (opy_p && sched_mem_operand_p (insn, false))
+    return sched_get_operand (insn, false);
+
+  if (opx_p && sched_mem_operand_p (insn, true))
+    return sched_get_operand (insn, true);
+
+  gcc_unreachable ();
+  return NULL;
+}
+
+/* Return non-zero if PRO modifies register used as part of
+   address in CON.  */
+int
+m68k_sched_address_bypass_p (rtx pro, rtx con)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  rtx pro_x;
+  rtx con_mem_read;
+
+  pro_x = sched_get_reg_operand (pro, true);
+  if (pro_x == NULL)
+    return 0;
+
+  con_mem_read = sched_get_mem_operand (con, true, false);
+  gcc_assert (con_mem_read != NULL);
+
+  if (reg_mentioned_p (pro_x, con_mem_read))
+    return 1;
+
+  return 0;
+}
+
+/* Helper function for m68k_sched_indexed_address_bypass_p.
+   if PRO modifies register used as index in CON,
+   return scale of indexed memory access in CON.  Return zero overwise.  */
+static int
+sched_get_indexed_address_scale (rtx pro, rtx con)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  rtx reg;
+  rtx mem;
+  struct m68k_address address;
+
+  reg = sched_get_reg_operand (pro, true);
+  if (reg == NULL)
+    return 0;
+
+  mem = sched_get_mem_operand (con, true, false);
+  gcc_assert (mem != NULL && MEM_P (mem));
+
+  if (!m68k_decompose_address (GET_MODE (mem), XEXP (mem, 0), reload_completed,
+        		       &address))
+    gcc_unreachable ();
+
+  if (REGNO (reg) == REGNO (address.index))
+    {
+      gcc_assert (address.scale != 0);
+      return address.scale;
+    }
+
+  return 0;
+}
+
+/* Return non-zero if PRO modifies register used
+   as index with scale 2 or 4 in CON.  */
+int
+m68k_sched_indexed_address_bypass_p (rtx pro, rtx con)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  gcc_assert (sched_cfv4_bypass_data.pro == NULL
+              && sched_cfv4_bypass_data.con == NULL
+              && sched_cfv4_bypass_data.scale == 0);
+
+  switch (sched_get_indexed_address_scale (pro, con))
+    {
+    case 1:
+      /* We can't have a variable latency bypass, so
+         remember to adjust the insn cost in adjust_cost hook.  */
+      sched_cfv4_bypass_data.pro = pro;
+      sched_cfv4_bypass_data.con = con;
+      sched_cfv4_bypass_data.scale = 1;
+      return 0;
+
+    case 2:
+    case 4:
+      return 1;
+
+    default:
+      return 0;
+    }
+}
\ No newline at end of file
diff -rupN gcc-4.6.4.test/gcc/config/m68k/m68k.c gcc-4.6.4-fastcall/gcc/config/m68k/m68k.c
--- gcc-4.6.4.test/gcc/config/m68k/m68k.c	2011-07-31 17:09:25.000000000 +0200
+++ gcc-4.6.4-fastcall/gcc/config/m68k/m68k.c	2017-04-30 19:10:52.556947001 +0200
@@ -59,6 +59,11 @@ enum reg_class regno_reg_class[] =
 };
 
 
+#define DPRINTFA(...)
+//#define DPRINTFA printf
+//#define DPRINTFB(...)
+#define DPRINTFB printf
+
 /* The minimum number of integer registers that we want to save with the
    movem instruction.  Using two movel instructions instead of a single
    moveml is about 15% faster for the 68020 and 68030 at no expense in
@@ -121,17 +126,8 @@ struct m68k_address {
   int scale;
 };
 
-static int m68k_sched_adjust_cost (rtx, rtx, rtx, int);
-static int m68k_sched_issue_rate (void);
-static int m68k_sched_variable_issue (FILE *, int, rtx, int);
-static void m68k_sched_md_init_global (FILE *, int, int);
-static void m68k_sched_md_finish_global (FILE *, int);
-static void m68k_sched_md_init (FILE *, int, int);
-static void m68k_sched_dfa_pre_advance_cycle (void);
-static void m68k_sched_dfa_post_advance_cycle (void);
-static int m68k_sched_first_cycle_multipass_dfa_lookahead (void);
-
 static bool m68k_can_eliminate (const int, const int);
+static void m68k_set_current_function (tree);
 static void m68k_conditional_register_usage (void);
 static bool m68k_legitimate_address_p (enum machine_mode, rtx, bool);
 static bool m68k_handle_option (size_t, const char *, int);
@@ -140,11 +136,14 @@ static void m68k_override_options_after_
 static rtx find_addr_reg (rtx);
 static const char *singlemove_string (rtx *);
 static void m68k_output_mi_thunk (FILE *, tree, HOST_WIDE_INT,
-					  HOST_WIDE_INT, tree);
+        				  HOST_WIDE_INT, tree);
+static bool m68k_promote_prototypes (const_tree);
 static rtx m68k_struct_value_rtx (tree, int);
 static tree m68k_handle_fndecl_attribute (tree *node, tree name,
-					  tree args, int flags,
-					  bool *no_add_attrs);
+        				  tree args, int flags,
+        				  bool *no_add_attrs);
+static tree m68k_handle_type_attribute (tree *, tree, tree, int, bool *);
+static int m68k_comp_type_attributes (tree, tree);
 static void m68k_compute_frame_layout (void);
 static bool m68k_save_reg (unsigned int regno, bool interrupt_handler);
 static bool m68k_ok_for_sibcall_p (tree, tree);
@@ -158,14 +157,11 @@ static void m68k_output_dwarf_dtprel (FI
 static void m68k_trampoline_init (rtx, tree, rtx);
 static int m68k_return_pops_args (tree, tree, int);
 static rtx m68k_delegitimize_address (rtx);
-static void m68k_function_arg_advance (CUMULATIVE_ARGS *, enum machine_mode,
-				       const_tree, bool);
-static rtx m68k_function_arg (CUMULATIVE_ARGS *, enum machine_mode,
-			      const_tree, bool);
 
 
 /* Specify the identification number of the library being built */
 const char *m68k_library_id_string = "_current_shared_library_a5_offset_";
+
 
 /* Initialize the GCC target structure.  */
 
@@ -208,34 +204,6 @@ const char *m68k_library_id_string = "_c
 #undef TARGET_LEGITIMIZE_ADDRESS
 #define TARGET_LEGITIMIZE_ADDRESS m68k_legitimize_address
 
-#undef TARGET_SCHED_ADJUST_COST
-#define TARGET_SCHED_ADJUST_COST m68k_sched_adjust_cost
-
-#undef TARGET_SCHED_ISSUE_RATE
-#define TARGET_SCHED_ISSUE_RATE m68k_sched_issue_rate
-
-#undef TARGET_SCHED_VARIABLE_ISSUE
-#define TARGET_SCHED_VARIABLE_ISSUE m68k_sched_variable_issue
-
-#undef TARGET_SCHED_INIT_GLOBAL
-#define TARGET_SCHED_INIT_GLOBAL m68k_sched_md_init_global
-
-#undef TARGET_SCHED_FINISH_GLOBAL
-#define TARGET_SCHED_FINISH_GLOBAL m68k_sched_md_finish_global
-
-#undef TARGET_SCHED_INIT
-#define TARGET_SCHED_INIT m68k_sched_md_init
-
-#undef TARGET_SCHED_DFA_PRE_ADVANCE_CYCLE
-#define TARGET_SCHED_DFA_PRE_ADVANCE_CYCLE m68k_sched_dfa_pre_advance_cycle
-
-#undef TARGET_SCHED_DFA_POST_ADVANCE_CYCLE
-#define TARGET_SCHED_DFA_POST_ADVANCE_CYCLE m68k_sched_dfa_post_advance_cycle
-
-#undef TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD
-#define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD	\
-  m68k_sched_first_cycle_multipass_dfa_lookahead
-
 #undef TARGET_HANDLE_OPTION
 #define TARGET_HANDLE_OPTION m68k_handle_option
 
@@ -252,7 +220,7 @@ const char *m68k_library_id_string = "_c
 #define TARGET_ATTRIBUTE_TABLE m68k_attribute_table
 
 #undef TARGET_PROMOTE_PROTOTYPES
-#define TARGET_PROMOTE_PROTOTYPES hook_bool_const_tree_true
+#define TARGET_PROMOTE_PROTOTYPES m68k_promote_prototypes
 
 #undef TARGET_STRUCT_VALUE_RTX
 #define TARGET_STRUCT_VALUE_RTX m68k_struct_value_rtx
@@ -282,6 +250,9 @@ const char *m68k_library_id_string = "_c
 #undef TARGET_CAN_ELIMINATE
 #define TARGET_CAN_ELIMINATE m68k_can_eliminate
 
+#undef TARGET_SET_CURRENT_FUNCTION
+#define TARGET_SET_CURRENT_FUNCTION m68k_set_current_function
+
 #undef TARGET_CONDITIONAL_REGISTER_USAGE
 #define TARGET_CONDITIONAL_REGISTER_USAGE m68k_conditional_register_usage
 
@@ -303,12 +274,24 @@ const char *m68k_library_id_string = "_c
 static const struct attribute_spec m68k_attribute_table[] =
 {
   /* { name, min_len, max_len, decl_req, type_req, fn_type_req, handler } */
-  { "interrupt", 0, 0, true,  false, false, m68k_handle_fndecl_attribute },
+  { "interrupt",         0, 0, true,  false, false, m68k_handle_fndecl_attribute },
   { "interrupt_handler", 0, 0, true,  false, false, m68k_handle_fndecl_attribute },
-  { "interrupt_thread", 0, 0, true,  false, false, m68k_handle_fndecl_attribute },
+  { "interrupt_thread",  0, 0, true,  false, false, m68k_handle_fndecl_attribute },
+
+ /* Stkparm attribute specifies to pass arguments on the stack */
+  { "stkparm",           0, 0, false, true,  true,  m68k_handle_type_attribute },
+  /* Regparm attribute specifies how many integer arguments are to be
+     passed in registers.  */
+  { "regparm",           0, 1, false, true,  true,  m68k_handle_type_attribute },
+  /* Fastcall attribute says callee is responsible for popping arguments
+   if they are not variable.  */
+  { "fastcall",          0, 0, false, true,  true,  m68k_handle_type_attribute },
   { NULL,                0, 0, false, false, false, NULL }
 };
 
+#undef TARGET_COMP_TYPE_ATTRIBUTES
+#define TARGET_COMP_TYPE_ATTRIBUTES m68k_comp_type_attributes
+
 struct gcc_target targetm = TARGET_INITIALIZER;
 
 /* Base flags for 68k ISAs.  */
@@ -318,7 +301,7 @@ struct gcc_target targetm = TARGET_INITI
    generated 68881 code for 68020 and 68030 targets unless explicitly told
    not to.  */
 #define FL_FOR_isa_20    (FL_FOR_isa_10 | FL_ISA_68020 \
-			  | FL_BITFIELD | FL_68881)
+        		  | FL_BITFIELD | FL_68881)
 #define FL_FOR_isa_40    (FL_FOR_isa_20 | FL_ISA_68040)
 #define FL_FOR_isa_cpu32 (FL_FOR_isa_10 | FL_ISA_68020)
 
@@ -387,12 +370,12 @@ static const struct m68k_target_selectio
   { "68060",    m68060,     NULL,  u68060,   isa_40,    FL_FOR_isa_40 },
   { "cpu32",    cpu32,      NULL,  ucpu32,   isa_20,    FL_FOR_isa_cpu32 },
   { "isaa",     mcf5206e,   NULL,  ucfv2,    isa_a,     (FL_FOR_isa_a
-							 | FL_CF_HWDIV) },
+        						 | FL_CF_HWDIV) },
   { "isaaplus", mcf5271,    NULL,  ucfv2,    isa_aplus, (FL_FOR_isa_aplus
-							 | FL_CF_HWDIV) },
+        						 | FL_CF_HWDIV) },
   { "isab",     mcf5407,    NULL,  ucfv4,    isa_b,     FL_FOR_isa_b },
   { "isac",     unk_device, NULL,  ucfv4,    isa_c,     (FL_FOR_isa_c
-							 | FL_CF_HWDIV) },
+        						 | FL_CF_HWDIV) },
   { NULL,       unk_device, NULL,  unk_arch, isa_max,   0 }
 };
 
@@ -412,12 +395,12 @@ static const struct m68k_target_selectio
   { "cfv1",     mcf51qe,    NULL,  ucfv1,     isa_c,   FL_FOR_isa_c },
   { "cfv2",     mcf5206,    NULL,  ucfv2,     isa_a,   FL_FOR_isa_a },
   { "cfv3",     mcf5307,    NULL,  ucfv3,     isa_a,   (FL_FOR_isa_a
-							| FL_CF_HWDIV) },
+        						| FL_CF_HWDIV) },
   { "cfv4",     mcf5407,    NULL,  ucfv4,     isa_b,   FL_FOR_isa_b },
   { "cfv4e",    mcf547x,    NULL,  ucfv4e,    isa_b,   (FL_FOR_isa_b
-							| FL_CF_USP
-							| FL_CF_EMAC
-							| FL_CF_FPU) },
+        						| FL_CF_USP
+        						| FL_CF_EMAC
+        						| FL_CF_FPU) },
   { NULL,       unk_device, NULL,  unk_arch,  isa_max, 0 }
 };
 
@@ -452,22 +435,24 @@ const char *m68k_symbolic_jump;
 enum M68K_SYMBOLIC_CALL m68k_symbolic_call_var;
 
 
+
 /* See whether TABLE has an entry with name NAME.  Return true and
    store the entry in *ENTRY if so, otherwise return false and
    leave *ENTRY alone.  */
 
 static bool
 m68k_find_selection (const struct m68k_target_selection **entry,
-		     const struct m68k_target_selection *table,
-		     const char *name)
+        	     const struct m68k_target_selection *table,
+        	     const char *name)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   size_t i;
 
   for (i = 0; table[i].name; i++)
     if (strcmp (table[i].name, name) == 0)
       {
-	*entry = table + i;
-	return true;
+        *entry = table + i;
+        return true;
       }
   return false;
 }
@@ -477,6 +462,7 @@ m68k_find_selection (const struct m68k_t
 static bool
 m68k_handle_option (size_t code, const char *arg, int value)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   switch (code)
     {
     case OPT_march_:
@@ -519,13 +505,13 @@ m68k_handle_option (size_t code, const c
 
     case OPT_m68020_40:
       return (m68k_find_selection (&m68k_tune_entry, all_microarchs,
-				   "68020-40")
-	      && m68k_find_selection (&m68k_cpu_entry, all_devices, "68020"));
+        			   "68020-40")
+              && m68k_find_selection (&m68k_cpu_entry, all_devices, "68020"));
 
     case OPT_m68020_60:
       return (m68k_find_selection (&m68k_tune_entry, all_microarchs,
-				   "68020-60")
-	      && m68k_find_selection (&m68k_cpu_entry, all_devices, "68020"));
+        			   "68020-60")
+              && m68k_find_selection (&m68k_cpu_entry, all_devices, "68020"));
 
     case OPT_m68030:
       return m68k_find_selection (&m68k_cpu_entry, all_devices, "68030");
@@ -545,14 +531,14 @@ m68k_handle_option (size_t code, const c
 
     case OPT_mshared_library_id_:
       if (value > MAX_LIBRARY_ID)
-	error ("-mshared-library-id=%s is not between 0 and %d",
-	       arg, MAX_LIBRARY_ID);
+        error ("-mshared-library-id=%s is not between 0 and %d",
+               arg, MAX_LIBRARY_ID);
       else
         {
-	  char *tmp;
-	  asprintf (&tmp, "%d", (value * -4) - 4);
-	  m68k_library_id_string = tmp;
-	}
+          char *tmp;
+          asprintf (&tmp, "%d", (value * -4) - 4);
+          m68k_library_id_string = tmp;
+        }
       return true;
 
     default:
@@ -565,6 +551,7 @@ m68k_handle_option (size_t code, const c
 static void
 m68k_option_override (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   const struct m68k_target_selection *entry;
   unsigned long target_mask;
 
@@ -583,12 +570,12 @@ m68k_option_override (void)
   if (m68k_cpu_entry)
     {
       /* Complain if the -march setting is for a different microarchitecture,
-	 or includes flags that the -mcpu setting doesn't.  */
+         or includes flags that the -mcpu setting doesn't.  */
       if (m68k_arch_entry
-	  && (m68k_arch_entry->microarch != m68k_cpu_entry->microarch
-	      || (m68k_arch_entry->flags & ~m68k_cpu_entry->flags) != 0))
-	warning (0, "-mcpu=%s conflicts with -march=%s",
-		 m68k_cpu_entry->name, m68k_arch_entry->name);
+          && (m68k_arch_entry->microarch != m68k_cpu_entry->microarch
+              || (m68k_arch_entry->flags & ~m68k_cpu_entry->flags) != 0))
+        warning (0, "-mcpu=%s conflicts with -march=%s",
+        	 m68k_cpu_entry->name, m68k_arch_entry->name);
 
       entry = m68k_cpu_entry;
     }
@@ -639,8 +626,8 @@ m68k_option_override (void)
 
   /* Set the type of FPU.  */
   m68k_fpu = (!TARGET_HARD_FLOAT ? FPUTYPE_NONE
-	      : (m68k_cpu_flags & FL_COLDFIRE) != 0 ? FPUTYPE_COLDFIRE
-	      : FPUTYPE_68881);
+              : (m68k_cpu_flags & FL_COLDFIRE) != 0 ? FPUTYPE_COLDFIRE
+              : FPUTYPE_68881);
 
   /* Sanity check to ensure that msep-data and mid-sahred-library are not
    * both specified together.  Doing so simply doesn't make sense.
@@ -678,20 +665,20 @@ m68k_option_override (void)
   else if (TARGET_68020 || TARGET_ISAB || TARGET_ISAC)
     {
       if (TARGET_PCREL)
-	m68k_symbolic_call_var = M68K_SYMBOLIC_CALL_BSR_C;
+        m68k_symbolic_call_var = M68K_SYMBOLIC_CALL_BSR_C;
       else
-	m68k_symbolic_call_var = M68K_SYMBOLIC_CALL_BSR_P;
+        m68k_symbolic_call_var = M68K_SYMBOLIC_CALL_BSR_P;
 
       if (TARGET_ISAC)
-	/* No unconditional long branch */;
+        /* No unconditional long branch */;
       else if (TARGET_PCREL)
-	m68k_symbolic_jump = "bra%.l %c0";
+        m68k_symbolic_jump = "bra%.l %c0";
       else
-	m68k_symbolic_jump = "bra%.l %p0";
+        m68k_symbolic_jump = "bra%.l %p0";
       /* Turn off function cse if we are doing PIC.  We always want
-	 function call to be done as `bsr foo@PLTPC'.  */
+         function call to be done as `bsr foo@PLTPC'.  */
       /* ??? It's traditional to do this for -mpcrel too, but it isn't
-	 clear how intentional that is.  */
+         clear how intentional that is.  */
       flag_no_function_cse = 1;
     }
 
@@ -730,34 +717,15 @@ m68k_option_override (void)
     }
 #endif
 
+  if (optimize >= 1 && !global_options_set.x_flag_omit_frame_pointer)
+    flag_omit_frame_pointer = 1;
+
+
   SUBTARGET_OVERRIDE_OPTIONS;
 
   /* Setup scheduling options.  */
-  if (TUNE_CFV1)
-    m68k_sched_cpu = CPU_CFV1;
-  else if (TUNE_CFV2)
-    m68k_sched_cpu = CPU_CFV2;
-  else if (TUNE_CFV3)
-    m68k_sched_cpu = CPU_CFV3;
-  else if (TUNE_CFV4)
-    m68k_sched_cpu = CPU_CFV4;
-  else
-    {
-      m68k_sched_cpu = CPU_UNKNOWN;
-      flag_schedule_insns = 0;
-      flag_schedule_insns_after_reload = 0;
-      flag_modulo_sched = 0;
-    }
+  m68k_setup_sched_options ();
 
-  if (m68k_sched_cpu != CPU_UNKNOWN)
-    {
-      if ((m68k_cpu_flags & (FL_CF_EMAC | FL_CF_EMAC_B)) != 0)
-	m68k_sched_mac = MAC_CF_EMAC;
-      else if ((m68k_cpu_flags & FL_CF_MAC) != 0)
-	m68k_sched_mac = MAC_CF_MAC;
-      else
-	m68k_sched_mac = MAC_NO;
-    }
 }
 
 /* Implement TARGET_OVERRIDE_OPTIONS_AFTER_CHANGE.  */
@@ -765,6 +733,7 @@ m68k_option_override (void)
 static void
 m68k_override_options_after_change (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (m68k_sched_cpu == CPU_UNKNOWN)
     {
       flag_schedule_insns = 0;
@@ -780,6 +749,7 @@ m68k_override_options_after_change (void
 const char *
 m68k_cpp_cpu_ident (const char *prefix)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (!m68k_cpu_entry)
     return NULL;
   return concat ("__m", prefix, "_cpu_", m68k_cpu_entry->name, NULL);
@@ -792,11 +762,14 @@ m68k_cpp_cpu_ident (const char *prefix)
 const char *
 m68k_cpp_cpu_family (const char *prefix)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (!m68k_cpu_entry)
     return NULL;
   return concat ("__m", prefix, "_family_", m68k_cpu_entry->family, NULL);
 }
 
+/* Attributes support.  */
+
 /* Return m68k_fk_interrupt_handler if FUNC has an "interrupt" or
    "interrupt_handler" attribute and interrupt_thread if FUNC has an
    "interrupt_thread" attribute.  Otherwise, return
@@ -805,6 +778,7 @@ m68k_cpp_cpu_family (const char *prefix)
 enum m68k_function_kind
 m68k_get_function_kind (tree func)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   tree a;
 
   gcc_assert (TREE_CODE (func) == FUNCTION_DECL);
@@ -828,14 +802,15 @@ m68k_get_function_kind (tree func)
    struct attribute_spec.handler.  */
 static tree
 m68k_handle_fndecl_attribute (tree *node, tree name,
-			      tree args ATTRIBUTE_UNUSED,
-			      int flags ATTRIBUTE_UNUSED,
-			      bool *no_add_attrs)
+        		      tree args ATTRIBUTE_UNUSED,
+        		      int flags ATTRIBUTE_UNUSED,
+        		      bool *no_add_attrs)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (TREE_CODE (*node) != FUNCTION_DECL)
     {
       warning (OPT_Wattributes, "%qE attribute only applies to functions",
-	       name);
+               name);
       *no_add_attrs = true;
     }
 
@@ -855,9 +830,205 @@ m68k_handle_fndecl_attribute (tree *node
   return NULL_TREE;
 }
 
+/* Handle a "regparm" or "stkparm" attribute;
+   arguments as in struct attribute_spec.handler.  */
+
+static void
+m68k_validate_mutually_exclusive_attribute (char *attr1, char *attr2, tree *node, tree name)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  if (lookup_attribute (attr2, TYPE_ATTRIBUTES(*node)))
+    error ("`%s` and `%s` are mutually exclusive", attr1, attr2);
+}
+
+static tree
+m68k_handle_type_attribute (tree *node, tree name, tree args,
+        		    int flags ATTRIBUTE_UNUSED, bool *no_add_attrs)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  if (TREE_CODE (*node) == FUNCTION_TYPE || TREE_CODE (*node) == METHOD_TYPE)
+    {
+      if (is_attribute_p ("stkparm", name))
+        {
+          m68k_validate_mutually_exclusive_attribute ("stkparm", "fastcall", node, name);
+        }
+      else if (is_attribute_p ("stkparm", name))
+        {
+          m68k_validate_mutually_exclusive_attribute ("fastcall", "stkparm", node, name);
+        }
+    }
+  else
+    {
+      warning ("`%s' attribute only applies to functions",
+               IDENTIFIER_POINTER (name));
+      *no_add_attrs = true;
+    }
+
+  return NULL_TREE;
+}
+
+/* Return zero if the attributes on TYPE1 and TYPE2 are incompatible,
+   one if they are compatible, and two if they are nearly compatible
+   (which causes a warning to be generated). */
+
+static int
+m68k_comp_exclusive_type_attributes(char *name1, char *name2, tree type1, tree type2)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+	return (!! lookup_attribute (name1, TYPE_ATTRIBUTES (type1)) !=
+            !! lookup_attribute (name1, TYPE_ATTRIBUTES (type2))
+         || !! lookup_attribute (name2, TYPE_ATTRIBUTES (type1)) !=
+            !! lookup_attribute (name2, TYPE_ATTRIBUTES (type2)));
+}
+
+static int
+m68k_comp_type_attributes (tree type1, tree type2)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  /* Functions or methods are incompatible if they specify mutually
+     exclusive ways of passing arguments.  */
+  if (TREE_CODE (type1) == FUNCTION_TYPE || TREE_CODE (type1) == METHOD_TYPE)
+    {
+      tree arg1, arg2;
+      if (m68k_comp_exclusive_type_attributes ("stkparm", "regparm", type1, type2)
+	   || m68k_comp_exclusive_type_attributes ("stkparm", "fastcall", type1, type2)
+	   || m68k_comp_exclusive_type_attributes ("regparm", "fastcall", type1, type2))
+        return 0; /* 'regparm' and 'stkparm' are mutually exclusive.  */
+
+      arg1 = lookup_attribute ("regparm", TYPE_ATTRIBUTES (type1));
+      arg2 = lookup_attribute ("regparm", TYPE_ATTRIBUTES (type2));
+      if (arg1 && arg2)
+        {
+          int num1 = 0, num2 = 0;
+          if (TREE_VALUE (arg1) && TREE_CODE (TREE_VALUE (arg1)) == TREE_LIST)
+            {
+              tree numofregs = TREE_VALUE (TREE_VALUE (arg1));
+              if (numofregs)
+        	num1 = TREE_INT_CST_LOW (numofregs);
+            }
+          if (TREE_VALUE (arg2) && TREE_CODE (TREE_VALUE (arg2)) == TREE_LIST)
+            {
+              tree numofregs = TREE_VALUE (TREE_VALUE (arg2));
+              if (numofregs)
+        	num2 = TREE_INT_CST_LOW (numofregs);
+            }
+          if (num1 != num2)
+            return 0; /* Different numbers, or no number in one type.  */
+        }
+    }
+  return 1;
+}
+
+
+/* Initialize a variable CUM of type CUMULATIVE_ARGS
+   for a call to a function whose data type is FNTYPE.
+   For a library call, FNTYPE is 0.  */
+
+void
+m68k_init_cumulative_args (CUMULATIVE_ARGS *cum,  /* Argument info to initialize */
+			      tree fntype)	/* tree ptr for function decl */
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  cum->last_arg_reg = -1;
+  cum->regs_already_used = 0;
+	
+#if ! defined (PCC_STATIC_STRUCT_RETURN) && defined (M68K_STRUCT_VALUE_REGNUM)
+  /* If return value is a structure, and we pass the buffer address in a
+     register, we can't use this register for our own purposes.
+     FIXME: Something similar would be useful for static chain.  */
+  if (fntype && aggregate_value_p (TREE_TYPE (fntype), fntype))
+    cum->regs_already_used |= (1 << M68K_STRUCT_VALUE_REGNUM);
+#endif
+}
+
+/* Define where to put the arguments to a function.
+   Value is zero to push the argument on the stack,
+   or a hard register in which to store the argument.
+
+   MODE is the argument's machine mode.
+   TYPE is the data type of the argument (as a tree).
+    This is null for libcalls where that information may
+    not be available.
+   CUM is a variable of type CUMULATIVE_ARGS which gives info about
+    the preceding args and about the function being called.  */
+
+rtx m68k_function_arg (CUMULATIVE_ARGS *cum, enum machine_mode mode,
+        		      const_tree type, bool named)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  if (TARGET_FASTCALL)
+    {
+      int regbegin = -1, regend, len;
+      long mask;
+      
+      /* FIXME: The last condition below is a workaround for a bug.  */
+      if (!TARGET_68881 && FLOAT_MODE_P (mode) && GET_MODE_UNIT_SIZE (mode) <= 4 &&
+	  (GET_MODE_CLASS (mode) != MODE_COMPLEX_FLOAT || mode == SCmode))
+        {
+            regbegin = 0; /* Dx */
+    	    regend = regbegin + M68K_FASTCALL_DATA_PARM;
+            len = (GET_MODE_SIZE (mode) + (UNITS_PER_WORD - 1)) / UNITS_PER_WORD;
+        }
+      else if (TARGET_68881 && FLOAT_MODE_P (mode) &&
+          GET_MODE_UNIT_SIZE (mode) <= 12 &&
+          (GET_MODE_CLASS (mode) != MODE_COMPLEX_FLOAT || mode == SCmode))
+        {
+          regbegin = 16; /* FPx */
+	  regend = regbegin + M68K_FASTCALL_DATA_PARM;
+          len = GET_MODE_NUNITS (mode);
+        }
+      /* FIXME: Two last conditions below are workarounds for bugs.  */
+      else if (INTEGRAL_MODE_P (mode) && mode !=CQImode && mode != CHImode)
+        {
+          len = (GET_MODE_SIZE (mode) + (UNITS_PER_WORD - 1)) / UNITS_PER_WORD;
+	  if (len == 1)
+	    {
+          if (type && POINTER_TYPE_P (type))  // THIS
+          {
+            regbegin = 8; /* Ax */
+	    regend = regbegin + M68K_FASTCALL_ADDR_PARM;
+          }
+          else
+          {
+            regbegin = 0; /* Dx */
+	    regend = regbegin + M68K_FASTCALL_DATA_PARM;
+          }
+          }
+        }
+
+      if (regbegin != -1)
+        {
+          int reg;
+          for (reg = regbegin; reg < regend; reg++)
+	    {
+	      long mask = 1 << reg;
+              if (!(cum->regs_already_used & mask) &&
+	          (reg + len  <= regend))
+                {
+                  cum->last_arg_reg = reg;
+        	  cum->last_arg_len = len;
+        	  break;
+                }
+             }
+         }
+	
+      if (!named)
+        cum->regs_already_used = -1;
+
+      if (cum->last_arg_reg != -1)
+        {
+          return gen_rtx_REG (mode, cum->last_arg_reg);
+        }
+    }
+  return NULL_RTX;
+}
+
+
+
 static void
 m68k_compute_frame_layout (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int regno, saved;
   unsigned int mask;
   enum m68k_function_kind func_kind =
@@ -879,10 +1050,10 @@ m68k_compute_frame_layout (void)
   if (!interrupt_thread)
     for (regno = 0; regno < 16; regno++)
       if (m68k_save_reg (regno, interrupt_handler))
-	{
-	  mask |= 1 << (regno - D0_REG);
-	  saved++;
-	}
+        {
+          mask |= 1 << (regno - D0_REG);
+          saved++;
+        }
   current_frame.offset = saved * 4;
   current_frame.reg_no = saved;
   current_frame.reg_mask = mask;
@@ -893,12 +1064,12 @@ m68k_compute_frame_layout (void)
     {
       /* Interrupt thread does not need to save any register.  */
       if (!interrupt_thread)
-	for (regno = 16; regno < 24; regno++)
-	  if (m68k_save_reg (regno, interrupt_handler))
-	    {
-	      mask |= 1 << (regno - FP0_REG);
-	      saved++;
-	    }
+        for (regno = 16; regno < 24; regno++)
+          if (m68k_save_reg (regno, interrupt_handler))
+            {
+              mask |= 1 << (regno - FP0_REG);
+              saved++;
+            }
       current_frame.foffset = saved * TARGET_FP_REG_SIZE;
       current_frame.offset += current_frame.foffset;
     }
@@ -914,12 +1085,14 @@ m68k_compute_frame_layout (void)
 bool
 m68k_can_eliminate (const int from ATTRIBUTE_UNUSED, const int to)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   return (to == STACK_POINTER_REGNUM ? ! frame_pointer_needed : true);
 }
 
 HOST_WIDE_INT
 m68k_initial_elimination_offset (int from, int to)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int argptr_offset;
   /* The arg pointer points 8 bytes before the start of the arguments,
      as defined by FIRST_PARM_OFFSET.  This makes it coincident with the
@@ -951,32 +1124,33 @@ m68k_initial_elimination_offset (int fro
 static bool
 m68k_save_reg (unsigned int regno, bool interrupt_handler)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (flag_pic && regno == PIC_REG)
     {
       if (crtl->saves_all_registers)
-	return true;
+        return true;
       if (crtl->uses_pic_offset_table)
-	return true;
+        return true;
       /* Reload may introduce constant pool references into a function
-	 that thitherto didn't need a PIC register.  Note that the test
-	 above will not catch that case because we will only set
-	 crtl->uses_pic_offset_table when emitting
-	 the address reloads.  */
+         that thitherto didn't need a PIC register.  Note that the test
+         above will not catch that case because we will only set
+         crtl->uses_pic_offset_table when emitting
+         the address reloads.  */
       if (crtl->uses_const_pool)
-	return true;
+        return true;
     }
 
   if (crtl->calls_eh_return)
     {
       unsigned int i;
       for (i = 0; ; i++)
-	{
-	  unsigned int test = EH_RETURN_DATA_REGNO (i);
-	  if (test == INVALID_REGNUM)
-	    break;
-	  if (test == regno)
-	    return true;
-	}
+        {
+          unsigned int test = EH_RETURN_DATA_REGNO (i);
+          if (test == INVALID_REGNUM)
+            break;
+          if (test == regno)
+            return true;
+        }
     }
 
   /* Fixed regs we never touch.  */
@@ -992,17 +1166,17 @@ m68k_save_reg (unsigned int regno, bool
   if (interrupt_handler)
     {
       if (df_regs_ever_live_p (regno))
-	return true;
+        return true;
 
       if (!current_function_is_leaf && call_used_regs[regno])
-	return true;
+        return true;
     }
 
   /* Never need to save registers that aren't touched.  */
   if (!df_regs_ever_live_p (regno))
     return false;
 
-  /* Otherwise save everything that isn't call-clobbered.  */
+  /* Otherwise save everything that isn't call-used.  */
   return !call_used_regs[regno];
 }
 
@@ -1015,9 +1189,10 @@ m68k_save_reg (unsigned int regno, bool
 
 static rtx
 m68k_emit_movem (rtx base, HOST_WIDE_INT offset,
-		 unsigned int count, unsigned int regno,
-		 unsigned int mask, bool store_p, bool adjust_stack_p)
+        	 unsigned int count, unsigned int regno,
+        	 unsigned int mask, bool store_p, bool adjust_stack_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int i;
   rtx body, addr, src, operands[2];
   enum machine_mode mode;
@@ -1029,20 +1204,20 @@ m68k_emit_movem (rtx base, HOST_WIDE_INT
   if (adjust_stack_p)
     {
       src = plus_constant (base, (count
-				  * GET_MODE_SIZE (mode)
-				  * (HOST_WIDE_INT) (store_p ? -1 : 1)));
+        			  * GET_MODE_SIZE (mode)
+        			  * (HOST_WIDE_INT) (store_p ? -1 : 1)));
       XVECEXP (body, 0, i++) = gen_rtx_SET (VOIDmode, base, src);
     }
 
   for (; mask != 0; mask >>= 1, regno++)
     if (mask & 1)
       {
-	addr = plus_constant (base, offset);
-	operands[!store_p] = gen_frame_mem (mode, addr);
-	operands[store_p] = gen_rtx_REG (mode, regno);
-	XVECEXP (body, 0, i++)
-	  = gen_rtx_SET (VOIDmode, operands[0], operands[1]);
-	offset += GET_MODE_SIZE (mode);
+        addr = plus_constant (base, offset);
+        operands[!store_p] = gen_frame_mem (mode, addr);
+        operands[store_p] = gen_rtx_REG (mode, regno);
+        XVECEXP (body, 0, i++)
+          = gen_rtx_SET (VOIDmode, operands[0], operands[1]);
+        offset += GET_MODE_SIZE (mode);
       }
   gcc_assert (i == XVECLEN (body, 0));
 
@@ -1054,6 +1229,7 @@ m68k_emit_movem (rtx base, HOST_WIDE_INT
 static void
 m68k_set_frame_related (rtx insn)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx body;
   int i;
 
@@ -1069,6 +1245,7 @@ m68k_set_frame_related (rtx insn)
 void
 m68k_expand_prologue (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   HOST_WIDE_INT fsize_with_regs;
   rtx limit, src, dest;
 
@@ -1081,89 +1258,89 @@ m68k_expand_prologue (void)
     {
       limit = plus_constant (stack_limit_rtx, current_frame.size + 4);
       if (!LEGITIMATE_CONSTANT_P (limit))
-	{
-	  emit_move_insn (gen_rtx_REG (Pmode, D0_REG), limit);
-	  limit = gen_rtx_REG (Pmode, D0_REG);
-	}
+        {
+          emit_move_insn (gen_rtx_REG (Pmode, D0_REG), limit);
+          limit = gen_rtx_REG (Pmode, D0_REG);
+        }
       emit_insn (gen_ctrapsi4 (gen_rtx_LTU (VOIDmode,
-					    stack_pointer_rtx, limit),
-			       stack_pointer_rtx, limit,
-			       const1_rtx));
+        				    stack_pointer_rtx, limit),
+        		       stack_pointer_rtx, limit,
+        		       const1_rtx));
     }
 
   fsize_with_regs = current_frame.size;
   if (TARGET_COLDFIRE)
     {
       /* ColdFire's move multiple instructions do not allow pre-decrement
-	 addressing.  Add the size of movem saves to the initial stack
-	 allocation instead.  */
+         addressing.  Add the size of movem saves to the initial stack
+         allocation instead.  */
       if (current_frame.reg_no >= MIN_MOVEM_REGS)
-	fsize_with_regs += current_frame.reg_no * GET_MODE_SIZE (SImode);
+        fsize_with_regs += current_frame.reg_no * GET_MODE_SIZE (SImode);
       if (current_frame.fpu_no >= MIN_FMOVEM_REGS)
-	fsize_with_regs += current_frame.fpu_no * GET_MODE_SIZE (DFmode);
+        fsize_with_regs += current_frame.fpu_no * GET_MODE_SIZE (DFmode);
     }
 
   if (frame_pointer_needed)
     {
       if (fsize_with_regs == 0 && TUNE_68040)
-	{
-	  /* On the 68040, two separate moves are faster than link.w 0.  */
-	  dest = gen_frame_mem (Pmode,
-				gen_rtx_PRE_DEC (Pmode, stack_pointer_rtx));
-	  m68k_set_frame_related (emit_move_insn (dest, frame_pointer_rtx));
-	  m68k_set_frame_related (emit_move_insn (frame_pointer_rtx,
-						  stack_pointer_rtx));
-	}
+        {
+          /* On the 68040, two separate moves are faster than link.w 0.  */
+          dest = gen_frame_mem (Pmode,
+        			gen_rtx_PRE_DEC (Pmode, stack_pointer_rtx));
+          m68k_set_frame_related (emit_move_insn (dest, frame_pointer_rtx));
+          m68k_set_frame_related (emit_move_insn (frame_pointer_rtx,
+        					  stack_pointer_rtx));
+        }
       else if (fsize_with_regs < 0x8000 || TARGET_68020)
-	m68k_set_frame_related
-	  (emit_insn (gen_link (frame_pointer_rtx,
-				GEN_INT (-4 - fsize_with_regs))));
+        m68k_set_frame_related
+          (emit_insn (gen_link (frame_pointer_rtx,
+        			GEN_INT (-4 - fsize_with_regs))));
       else
  	{
-	  m68k_set_frame_related
-	    (emit_insn (gen_link (frame_pointer_rtx, GEN_INT (-4))));
-	  m68k_set_frame_related
-	    (emit_insn (gen_addsi3 (stack_pointer_rtx,
-				    stack_pointer_rtx,
-				    GEN_INT (-fsize_with_regs))));
-	}
+          m68k_set_frame_related
+            (emit_insn (gen_link (frame_pointer_rtx, GEN_INT (-4))));
+          m68k_set_frame_related
+            (emit_insn (gen_addsi3 (stack_pointer_rtx,
+        			    stack_pointer_rtx,
+        			    GEN_INT (-fsize_with_regs))));
+        }
 
       /* If the frame pointer is needed, emit a special barrier that
-	 will prevent the scheduler from moving stores to the frame
-	 before the stack adjustment.  */
+         will prevent the scheduler from moving stores to the frame
+         before the stack adjustment.  */
       emit_insn (gen_stack_tie (stack_pointer_rtx, frame_pointer_rtx));
     }
   else if (fsize_with_regs != 0)
     m68k_set_frame_related
       (emit_insn (gen_addsi3 (stack_pointer_rtx,
-			      stack_pointer_rtx,
-			      GEN_INT (-fsize_with_regs))));
+        		      stack_pointer_rtx,
+        		      GEN_INT (-fsize_with_regs))));
 
   if (current_frame.fpu_mask)
     {
       gcc_assert (current_frame.fpu_no >= MIN_FMOVEM_REGS);
       if (TARGET_68881)
-	m68k_set_frame_related
-	  (m68k_emit_movem (stack_pointer_rtx,
-			    current_frame.fpu_no * -GET_MODE_SIZE (XFmode),
-			    current_frame.fpu_no, FP0_REG,
-			    current_frame.fpu_mask, true, true));
+        m68k_set_frame_related
+          (m68k_emit_movem (stack_pointer_rtx,
+        		    current_frame.fpu_no * -GET_MODE_SIZE (XFmode),
+        		    current_frame.fpu_no, FP0_REG,
+        		    current_frame.fpu_mask, true, true));
       else
-	{
-	  int offset;
+        {
+          int offset;
 
-	  /* If we're using moveml to save the integer registers,
-	     the stack pointer will point to the bottom of the moveml
-	     save area.  Find the stack offset of the first FP register.  */
-	  if (current_frame.reg_no < MIN_MOVEM_REGS)
-	    offset = 0;
-	  else
-	    offset = current_frame.reg_no * GET_MODE_SIZE (SImode);
-	  m68k_set_frame_related
-	    (m68k_emit_movem (stack_pointer_rtx, offset,
-			      current_frame.fpu_no, FP0_REG,
-			      current_frame.fpu_mask, true, false));
-	}
+          /* If we're using moveml to save the integer registers,
+             the stack pointer will point to the bottom of the moveml
+             save area.  Find the stack offset of the first FP register.  */
+          if (current_frame.reg_no < MIN_MOVEM_REGS)
+            offset = 0;
+          else
+            offset = current_frame.reg_no * GET_MODE_SIZE (SImode);
+          m68k_set_frame_related
+            (m68k_emit_movem (stack_pointer_rtx, offset,
+        		      current_frame.fpu_no, FP0_REG,
+        		      current_frame.fpu_mask, true, false));
+        }
     }
 
   /* If the stack limit is not a symbol, check it here.
@@ -1172,12 +1349,12 @@ m68k_expand_prologue (void)
     {
       if (REG_P (stack_limit_rtx))
         emit_insn (gen_ctrapsi4 (gen_rtx_LTU (VOIDmode, stack_pointer_rtx,
-					      stack_limit_rtx),
-			         stack_pointer_rtx, stack_limit_rtx,
-			         const1_rtx));
+        				      stack_limit_rtx),
+        		         stack_pointer_rtx, stack_limit_rtx,
+        		         const1_rtx));
 
       else if (GET_CODE (stack_limit_rtx) != SYMBOL_REF)
-	warning (0, "stack limit expression is not supported");
+        warning (0, "stack limit expression is not supported");
     }
 
   if (current_frame.reg_no < MIN_MOVEM_REGS)
@@ -1186,29 +1363,29 @@ m68k_expand_prologue (void)
       int i;
 
       for (i = 16; i-- > 0; )
-	if (current_frame.reg_mask & (1 << i))
-	  {
-	    src = gen_rtx_REG (SImode, D0_REG + i);
-	    dest = gen_frame_mem (SImode,
-				  gen_rtx_PRE_DEC (Pmode, stack_pointer_rtx));
-	    m68k_set_frame_related (emit_insn (gen_movsi (dest, src)));
-	  }
+        if (current_frame.reg_mask & (1 << i))
+          {
+            src = gen_rtx_REG (SImode, D0_REG + i);
+            dest = gen_frame_mem (SImode,
+        			  gen_rtx_PRE_DEC (Pmode, stack_pointer_rtx));
+            m68k_set_frame_related (emit_insn (gen_movsi (dest, src)));
+          }
     }
   else
     {
       if (TARGET_COLDFIRE)
-	/* The required register save space has already been allocated.
-	   The first register should be stored at (%sp).  */
-	m68k_set_frame_related
-	  (m68k_emit_movem (stack_pointer_rtx, 0,
-			    current_frame.reg_no, D0_REG,
-			    current_frame.reg_mask, true, false));
-      else
-	m68k_set_frame_related
-	  (m68k_emit_movem (stack_pointer_rtx,
-			    current_frame.reg_no * -GET_MODE_SIZE (SImode),
-			    current_frame.reg_no, D0_REG,
-			    current_frame.reg_mask, true, true));
+        /* The required register save space has already been allocated.
+           The first register should be stored at (%sp).  */
+        m68k_set_frame_related
+          (m68k_emit_movem (stack_pointer_rtx, 0,
+        		    current_frame.reg_no, D0_REG,
+        		    current_frame.reg_mask, true, false));
+      else
+        m68k_set_frame_related
+          (m68k_emit_movem (stack_pointer_rtx,
+        		    current_frame.reg_no * -GET_MODE_SIZE (SImode),
+        		    current_frame.reg_no, D0_REG,
+        		    current_frame.reg_mask, true, true));
     }
 
   if (!TARGET_SEP_DATA
@@ -1222,6 +1399,7 @@ m68k_expand_prologue (void)
 bool
 m68k_use_return_insn (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (!reload_completed || frame_pointer_needed || get_frame_size () != 0)
     return false;
 
@@ -1240,6 +1418,7 @@ m68k_use_return_insn (void)
 void
 m68k_expand_epilogue (bool sibcall_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   HOST_WIDE_INT fsize, fsize_with_regs;
   bool big, restore_from_sp;
 
@@ -1253,8 +1432,8 @@ m68k_expand_epilogue (bool sibcall_p)
      What we really need to know there is if there could be pending
      stack adjustment needed at that point.  */
   restore_from_sp = (!frame_pointer_needed
-		     || (!cfun->calls_alloca
-			 && current_function_is_leaf));
+        	     || (!cfun->calls_alloca
+        		 && current_function_is_leaf));
 
   /* fsize_with_regs is the size we need to adjust the sp when
      popping the frame.  */
@@ -1262,12 +1441,12 @@ m68k_expand_epilogue (bool sibcall_p)
   if (TARGET_COLDFIRE && restore_from_sp)
     {
       /* ColdFire's move multiple instructions do not allow post-increment
-	 addressing.  Add the size of movem loads to the final deallocation
-	 instead.  */
+         addressing.  Add the size of movem loads to the final deallocation
+         instead.  */
       if (current_frame.reg_no >= MIN_MOVEM_REGS)
-	fsize_with_regs += current_frame.reg_no * GET_MODE_SIZE (SImode);
+        fsize_with_regs += current_frame.reg_no * GET_MODE_SIZE (SImode);
       if (current_frame.fpu_no >= MIN_FMOVEM_REGS)
-	fsize_with_regs += current_frame.fpu_no * GET_MODE_SIZE (DFmode);
+        fsize_with_regs += current_frame.fpu_no * GET_MODE_SIZE (DFmode);
     }
 
   if (current_frame.offset + fsize >= 0x8000
@@ -1275,25 +1454,25 @@ m68k_expand_epilogue (bool sibcall_p)
       && (current_frame.reg_mask || current_frame.fpu_mask))
     {
       if (TARGET_COLDFIRE
-	  && (current_frame.reg_no >= MIN_MOVEM_REGS
-	      || current_frame.fpu_no >= MIN_FMOVEM_REGS))
-	{
-	  /* ColdFire's move multiple instructions do not support the
-	     (d8,Ax,Xi) addressing mode, so we're as well using a normal
-	     stack-based restore.  */
-	  emit_move_insn (gen_rtx_REG (Pmode, A1_REG),
-			  GEN_INT (-(current_frame.offset + fsize)));
-	  emit_insn (gen_addsi3 (stack_pointer_rtx,
-				 gen_rtx_REG (Pmode, A1_REG),
-				 frame_pointer_rtx));
-	  restore_from_sp = true;
-	}
+          && (current_frame.reg_no >= MIN_MOVEM_REGS
+              || current_frame.fpu_no >= MIN_FMOVEM_REGS))
+        {
+          /* ColdFire's move multiple instructions do not support the
+             (d8,Ax,Xi) addressing mode, so we're as well using a normal
+             stack-based restore.  */
+          emit_move_insn (gen_rtx_REG (Pmode, A1_REG),
+        		  GEN_INT (-(current_frame.offset + fsize)));
+          emit_insn (gen_addsi3 (stack_pointer_rtx,
+        			 gen_rtx_REG (Pmode, A1_REG),
+        			 frame_pointer_rtx));
+          restore_from_sp = true;
+        }
       else
-	{
-	  emit_move_insn (gen_rtx_REG (Pmode, A1_REG), GEN_INT (-fsize));
-	  fsize = 0;
-	  big = true;
-	}
+        {
+          emit_move_insn (gen_rtx_REG (Pmode, A1_REG), GEN_INT (-fsize));
+          fsize = 0;
+          big = true;
+        }
     }
 
   if (current_frame.reg_no < MIN_MOVEM_REGS)
@@ -1306,95 +1485,95 @@ m68k_expand_epilogue (bool sibcall_p)
       for (i = 0; i < 16; i++)
         if (current_frame.reg_mask & (1 << i))
           {
-	    rtx addr;
+            rtx addr;
 
-	    if (big)
-	      {
-		/* Generate the address -OFFSET(%fp,%a1.l).  */
-		addr = gen_rtx_REG (Pmode, A1_REG);
-		addr = gen_rtx_PLUS (Pmode, addr, frame_pointer_rtx);
-		addr = plus_constant (addr, -offset);
-	      }
-	    else if (restore_from_sp)
-	      addr = gen_rtx_POST_INC (Pmode, stack_pointer_rtx);
-	    else
-	      addr = plus_constant (frame_pointer_rtx, -offset);
-	    emit_move_insn (gen_rtx_REG (SImode, D0_REG + i),
-			    gen_frame_mem (SImode, addr));
-	    offset -= GET_MODE_SIZE (SImode);
-	  }
+            if (big)
+              {
+        	/* Generate the address -OFFSET(%fp,%a1.l).  */
+        	addr = gen_rtx_REG (Pmode, A1_REG);
+        	addr = gen_rtx_PLUS (Pmode, addr, frame_pointer_rtx);
+        	addr = plus_constant (addr, -offset);
+              }
+            else if (restore_from_sp)
+              addr = gen_rtx_POST_INC (Pmode, stack_pointer_rtx);
+            else
+              addr = plus_constant (frame_pointer_rtx, -offset);
+            emit_move_insn (gen_rtx_REG (SImode, D0_REG + i),
+        		    gen_frame_mem (SImode, addr));
+            offset -= GET_MODE_SIZE (SImode);
+          }
     }
   else if (current_frame.reg_mask)
     {
       if (big)
-	m68k_emit_movem (gen_rtx_PLUS (Pmode,
-				       gen_rtx_REG (Pmode, A1_REG),
-				       frame_pointer_rtx),
-			 -(current_frame.offset + fsize),
-			 current_frame.reg_no, D0_REG,
-			 current_frame.reg_mask, false, false);
+        m68k_emit_movem (gen_rtx_PLUS (Pmode,
+        			       gen_rtx_REG (Pmode, A1_REG),
+        			       frame_pointer_rtx),
+        		 -(current_frame.offset + fsize),
+        		 current_frame.reg_no, D0_REG,
+        		 current_frame.reg_mask, false, false);
       else if (restore_from_sp)
-	m68k_emit_movem (stack_pointer_rtx, 0,
-			 current_frame.reg_no, D0_REG,
-			 current_frame.reg_mask, false,
-			 !TARGET_COLDFIRE);
-      else
-	m68k_emit_movem (frame_pointer_rtx,
-			 -(current_frame.offset + fsize),
-			 current_frame.reg_no, D0_REG,
-			 current_frame.reg_mask, false, false);
+        m68k_emit_movem (stack_pointer_rtx, 0,
+        		 current_frame.reg_no, D0_REG,
+        		 current_frame.reg_mask, false,
+        		 !TARGET_COLDFIRE);
+      else
+        m68k_emit_movem (frame_pointer_rtx,
+        		 -(current_frame.offset + fsize),
+        		 current_frame.reg_no, D0_REG,
+        		 current_frame.reg_mask, false, false);
     }
 
   if (current_frame.fpu_no > 0)
     {
       if (big)
-	m68k_emit_movem (gen_rtx_PLUS (Pmode,
-				       gen_rtx_REG (Pmode, A1_REG),
-				       frame_pointer_rtx),
-			 -(current_frame.foffset + fsize),
-			 current_frame.fpu_no, FP0_REG,
-			 current_frame.fpu_mask, false, false);
+        m68k_emit_movem (gen_rtx_PLUS (Pmode,
+        			       gen_rtx_REG (Pmode, A1_REG),
+        			       frame_pointer_rtx),
+        		 -(current_frame.foffset + fsize),
+        		 current_frame.fpu_no, FP0_REG,
+        		 current_frame.fpu_mask, false, false);
       else if (restore_from_sp)
-	{
-	  if (TARGET_COLDFIRE)
-	    {
-	      int offset;
-
-	      /* If we used moveml to restore the integer registers, the
-		 stack pointer will still point to the bottom of the moveml
-		 save area.  Find the stack offset of the first FP
-		 register.  */
-	      if (current_frame.reg_no < MIN_MOVEM_REGS)
-		offset = 0;
-	      else
-		offset = current_frame.reg_no * GET_MODE_SIZE (SImode);
-	      m68k_emit_movem (stack_pointer_rtx, offset,
-			       current_frame.fpu_no, FP0_REG,
-			       current_frame.fpu_mask, false, false);
-	    }
-	  else
-	    m68k_emit_movem (stack_pointer_rtx, 0,
-			     current_frame.fpu_no, FP0_REG,
-			     current_frame.fpu_mask, false, true);
-	}
+        {
+          if (TARGET_COLDFIRE)
+            {
+              int offset;
+
+              /* If we used moveml to restore the integer registers, the
+        	 stack pointer will still point to the bottom of the moveml
+        	 save area.  Find the stack offset of the first FP
+        	 register.  */
+              if (current_frame.reg_no < MIN_MOVEM_REGS)
+        	offset = 0;
+              else
+        	offset = current_frame.reg_no * GET_MODE_SIZE (SImode);
+              m68k_emit_movem (stack_pointer_rtx, offset,
+        		       current_frame.fpu_no, FP0_REG,
+        		       current_frame.fpu_mask, false, false);
+            }
+          else
+            m68k_emit_movem (stack_pointer_rtx, 0,
+        		     current_frame.fpu_no, FP0_REG,
+        		     current_frame.fpu_mask, false, true);
+        }
       else
-	m68k_emit_movem (frame_pointer_rtx,
-			 -(current_frame.foffset + fsize),
-			 current_frame.fpu_no, FP0_REG,
-			 current_frame.fpu_mask, false, false);
+        m68k_emit_movem (frame_pointer_rtx,
+        		 -(current_frame.foffset + fsize),
+        		 current_frame.fpu_no, FP0_REG,
+        		 current_frame.fpu_mask, false, false);
     }
 
   if (frame_pointer_needed)
     emit_insn (gen_unlink (frame_pointer_rtx));
   else if (fsize_with_regs)
     emit_insn (gen_addsi3 (stack_pointer_rtx,
-			   stack_pointer_rtx,
-			   GEN_INT (fsize_with_regs)));
+        		   stack_pointer_rtx,
+        		   GEN_INT (fsize_with_regs)));
 
   if (crtl->calls_eh_return)
     emit_insn (gen_addsi3 (stack_pointer_rtx,
-			   stack_pointer_rtx,
-			   EH_RETURN_STACKADJ_RTX));
+        		   stack_pointer_rtx,
+        		   EH_RETURN_STACKADJ_RTX));
 
   if (!sibcall_p)
     emit_jump_insn (gen_rtx_RETURN (VOIDmode));
@@ -1411,6 +1590,7 @@ m68k_expand_epilogue (bool sibcall_p)
 int
 valid_dbcc_comparison_p_2 (rtx x, enum machine_mode mode ATTRIBUTE_UNUSED)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   switch (GET_CODE (x))
     {
       case EQ: case NE: case GTU: case LTU:
@@ -1430,6 +1610,7 @@ valid_dbcc_comparison_p_2 (rtx x, enum m
 int
 flags_in_68881 (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   /* We could add support for these in the future */
   return cc_status.flags & CC_IN_68881;
 }
@@ -1438,6 +1619,7 @@ flags_in_68881 (void)
 static bool
 m68k_reg_present_p (const_rtx parallel, unsigned int regno)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int i;
 
   if (REG_P (parallel) && REGNO (parallel) == regno)
@@ -1452,7 +1634,7 @@ m68k_reg_present_p (const_rtx parallel,
 
       x = XEXP (XVECEXP (parallel, 0, i), 0);
       if (REG_P (x) && REGNO (x) == regno)
-	return true;
+        return true;
     }
 
   return false;
@@ -1463,31 +1645,45 @@ m68k_reg_present_p (const_rtx parallel,
 static bool
 m68k_ok_for_sibcall_p (tree decl, tree exp)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   enum m68k_function_kind kind;
+  tree type;
   
   /* We cannot use sibcalls for nested functions because we use the
      static chain register for indirect calls.  */
   if (CALL_EXPR_STATIC_CHAIN (exp))
     return false;
 
+  if (decl)
+    {
+      type = TREE_TYPE (decl);
+    }
+  else
+    {
+      /* We're looking at the CALL_EXPR, we need the type of the function.  */
+      type = CALL_EXPR_FN (exp);		/* pointer expression */
+      type = TREE_TYPE (type);			/* pointer type */
+      type = TREE_TYPE (type);			/* function type */
+    }
+
   if (!VOID_TYPE_P (TREE_TYPE (DECL_RESULT (cfun->decl))))
     {
       /* Check that the return value locations are the same.  For
-	 example that we aren't returning a value from the sibling in
-	 a D0 register but then need to transfer it to a A0 register.  */
+         example that we aren't returning a value from the sibling in
+         a D0 register but then need to transfer it to a A0 register.  */
       rtx cfun_value;
       rtx call_value;
 
       cfun_value = FUNCTION_VALUE (TREE_TYPE (DECL_RESULT (cfun->decl)),
-				   cfun->decl);
+        			   cfun->decl);
       call_value = FUNCTION_VALUE (TREE_TYPE (exp), decl);
 
       /* Check that the values are equal or that the result the callee
-	 function returns is superset of what the current function returns.  */
+         function returns is superset of what the current function returns.  */
       if (!(rtx_equal_p (cfun_value, call_value)
-	    || (REG_P (cfun_value)
-		&& m68k_reg_present_p (call_value, REGNO (cfun_value)))))
-	return false;
+            || (REG_P (cfun_value)
+        	&& m68k_reg_present_p (call_value, REGNO (cfun_value)))))
+        return false;
     }
 
   kind = m68k_get_function_kind (current_function_decl);
@@ -1504,24 +1700,17 @@ m68k_ok_for_sibcall_p (tree decl, tree e
   return false;
 }
 
-/* On the m68k all args are always pushed.  */
-
-static rtx
-m68k_function_arg (CUMULATIVE_ARGS *cum ATTRIBUTE_UNUSED,
-		   enum machine_mode mode ATTRIBUTE_UNUSED,
-		   const_tree type ATTRIBUTE_UNUSED,
-		   bool named ATTRIBUTE_UNUSED)
-{
-  return NULL_RTX;
-}
-
-static void
-m68k_function_arg_advance (CUMULATIVE_ARGS *cum, enum machine_mode mode,
-			   const_tree type, bool named ATTRIBUTE_UNUSED)
+void
+m68k_function_arg_advance (CUMULATIVE_ARGS *cum)
 {
-  *cum += (mode != BLKmode
-	   ? (GET_MODE_SIZE (mode) + 3) & ~3
-	   : (int_size_in_bytes (type) + 3) & ~3);
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+    if (cum->last_arg_reg != -1)
+      {
+        int count;
+        for (count = 0; count < cum->last_arg_len; count++)
+  	    cum->regs_already_used |= (1 << (cum->last_arg_reg + count));
+        cum->last_arg_reg = -1;
+      }
 }
 
 /* Convert X to a legitimate function call memory reference and return the
@@ -1530,6 +1719,7 @@ m68k_function_arg_advance (CUMULATIVE_AR
 rtx
 m68k_legitimize_call_address (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   gcc_assert (MEM_P (x));
   if (call_operand (XEXP (x, 0), VOIDmode))
     return x;
@@ -1541,6 +1731,7 @@ m68k_legitimize_call_address (rtx x)
 rtx
 m68k_legitimize_sibcall_address (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   gcc_assert (MEM_P (x));
   if (sibcall_operand (XEXP (x, 0), VOIDmode))
     return x;
@@ -1560,6 +1751,7 @@ m68k_legitimize_sibcall_address (rtx x)
 static rtx
 m68k_legitimize_address (rtx x, rtx oldx, enum machine_mode mode)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (m68k_tls_symbol_p (x))
     return m68k_legitimize_tls_address (x);
 
@@ -1571,58 +1763,58 @@ m68k_legitimize_address (rtx x, rtx oldx
 #define COPY_ONCE(Y) if (!copied) { Y = copy_rtx (Y); copied = ch = 1; }
 
       if (GET_CODE (XEXP (x, 0)) == MULT)
-	{
-	  COPY_ONCE (x);
-	  XEXP (x, 0) = force_operand (XEXP (x, 0), 0);
-	}
+        {
+          COPY_ONCE (x);
+          XEXP (x, 0) = force_operand (XEXP (x, 0), 0);
+        }
       if (GET_CODE (XEXP (x, 1)) == MULT)
-	{
-	  COPY_ONCE (x);
-	  XEXP (x, 1) = force_operand (XEXP (x, 1), 0);
-	}
+        {
+          COPY_ONCE (x);
+          XEXP (x, 1) = force_operand (XEXP (x, 1), 0);
+        }
       if (ch)
-	{
+        {
           if (GET_CODE (XEXP (x, 1)) == REG
-	      && GET_CODE (XEXP (x, 0)) == REG)
-	    {
-	      if (TARGET_COLDFIRE_FPU && GET_MODE_CLASS (mode) == MODE_FLOAT)
-	        {
-	          COPY_ONCE (x);
-	          x = force_operand (x, 0);
-	        }
-	      return x;
-	    }
-	  if (memory_address_p (mode, x))
-	    return x;
-	}
+              && GET_CODE (XEXP (x, 0)) == REG)
+            {
+              if (TARGET_COLDFIRE_FPU && GET_MODE_CLASS (mode) == MODE_FLOAT)
+                {
+                  COPY_ONCE (x);
+                  x = force_operand (x, 0);
+                }
+              return x;
+            }
+          if (memory_address_p (mode, x))
+            return x;
+        }
       if (GET_CODE (XEXP (x, 0)) == REG
-	  || (GET_CODE (XEXP (x, 0)) == SIGN_EXTEND
-	      && GET_CODE (XEXP (XEXP (x, 0), 0)) == REG
-	      && GET_MODE (XEXP (XEXP (x, 0), 0)) == HImode))
-	{
-	  rtx temp = gen_reg_rtx (Pmode);
-	  rtx val = force_operand (XEXP (x, 1), 0);
-	  emit_move_insn (temp, val);
-	  COPY_ONCE (x);
-	  XEXP (x, 1) = temp;
-	  if (TARGET_COLDFIRE_FPU && GET_MODE_CLASS (mode) == MODE_FLOAT
-	      && GET_CODE (XEXP (x, 0)) == REG)
-	    x = force_operand (x, 0);
-	}
+          || (GET_CODE (XEXP (x, 0)) == SIGN_EXTEND
+              && GET_CODE (XEXP (XEXP (x, 0), 0)) == REG
+              && GET_MODE (XEXP (XEXP (x, 0), 0)) == HImode))
+        {
+          rtx temp = gen_reg_rtx (Pmode);
+          rtx val = force_operand (XEXP (x, 1), 0);
+          emit_move_insn (temp, val);
+          COPY_ONCE (x);
+          XEXP (x, 1) = temp;
+          if (TARGET_COLDFIRE_FPU && GET_MODE_CLASS (mode) == MODE_FLOAT
+              && GET_CODE (XEXP (x, 0)) == REG)
+            x = force_operand (x, 0);
+        }
       else if (GET_CODE (XEXP (x, 1)) == REG
-	       || (GET_CODE (XEXP (x, 1)) == SIGN_EXTEND
-		   && GET_CODE (XEXP (XEXP (x, 1), 0)) == REG
-		   && GET_MODE (XEXP (XEXP (x, 1), 0)) == HImode))
-	{
-	  rtx temp = gen_reg_rtx (Pmode);
-	  rtx val = force_operand (XEXP (x, 0), 0);
-	  emit_move_insn (temp, val);
-	  COPY_ONCE (x);
-	  XEXP (x, 0) = temp;
-	  if (TARGET_COLDFIRE_FPU && GET_MODE_CLASS (mode) == MODE_FLOAT
-	      && GET_CODE (XEXP (x, 1)) == REG)
-	    x = force_operand (x, 0);
-	}
+               || (GET_CODE (XEXP (x, 1)) == SIGN_EXTEND
+        	   && GET_CODE (XEXP (XEXP (x, 1), 0)) == REG
+        	   && GET_MODE (XEXP (XEXP (x, 1), 0)) == HImode))
+        {
+          rtx temp = gen_reg_rtx (Pmode);
+          rtx val = force_operand (XEXP (x, 0), 0);
+          emit_move_insn (temp, val);
+          COPY_ONCE (x);
+          XEXP (x, 0) = temp;
+          if (TARGET_COLDFIRE_FPU && GET_MODE_CLASS (mode) == MODE_FLOAT
+              && GET_CODE (XEXP (x, 1)) == REG)
+            x = force_operand (x, 0);
+        }
     }
 
   return x;
@@ -1638,50 +1830,51 @@ m68k_legitimize_address (rtx x, rtx oldx
 void
 output_dbcc_and_branch (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   switch (GET_CODE (operands[3]))
     {
       case EQ:
-	output_asm_insn ("dbeq %0,%l1\n\tjeq %l2", operands);
-	break;
+        output_asm_insn ("dbeq %0,%l1\n\tjeq %l2", operands);
+        break;
 
       case NE:
-	output_asm_insn ("dbne %0,%l1\n\tjne %l2", operands);
-	break;
+        output_asm_insn ("dbne %0,%l1\n\tjne %l2", operands);
+        break;
 
       case GT:
-	output_asm_insn ("dbgt %0,%l1\n\tjgt %l2", operands);
-	break;
+        output_asm_insn ("dbgt %0,%l1\n\tjgt %l2", operands);
+        break;
 
       case GTU:
-	output_asm_insn ("dbhi %0,%l1\n\tjhi %l2", operands);
-	break;
+        output_asm_insn ("dbhi %0,%l1\n\tjhi %l2", operands);
+        break;
 
       case LT:
-	output_asm_insn ("dblt %0,%l1\n\tjlt %l2", operands);
-	break;
+        output_asm_insn ("dblt %0,%l1\n\tjlt %l2", operands);
+        break;
 
       case LTU:
-	output_asm_insn ("dbcs %0,%l1\n\tjcs %l2", operands);
-	break;
+        output_asm_insn ("dbcs %0,%l1\n\tjcs %l2", operands);
+        break;
 
       case GE:
-	output_asm_insn ("dbge %0,%l1\n\tjge %l2", operands);
-	break;
+        output_asm_insn ("dbge %0,%l1\n\tjge %l2", operands);
+        break;
 
       case GEU:
-	output_asm_insn ("dbcc %0,%l1\n\tjcc %l2", operands);
-	break;
+        output_asm_insn ("dbcc %0,%l1\n\tjcc %l2", operands);
+        break;
 
       case LE:
-	output_asm_insn ("dble %0,%l1\n\tjle %l2", operands);
-	break;
+        output_asm_insn ("dble %0,%l1\n\tjle %l2", operands);
+        break;
 
       case LEU:
-	output_asm_insn ("dbls %0,%l1\n\tjls %l2", operands);
-	break;
+        output_asm_insn ("dbls %0,%l1\n\tjls %l2", operands);
+        break;
 
       default:
-	gcc_unreachable ();
+        gcc_unreachable ();
     }
 
   /* If the decrement is to be done in SImode, then we have
@@ -1703,9 +1896,11 @@ output_dbcc_and_branch (rtx *operands)
 const char *
 output_scc_di (rtx op, rtx operand1, rtx operand2, rtx dest)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx loperands[7];
   enum rtx_code op_code = GET_CODE (op);
 
+
   /* This does not produce a useful cc.  */
   CC_STATUS_INIT;
 
@@ -1729,9 +1924,9 @@ output_scc_di (rtx op, rtx operand1, rtx
     {
       loperands[2] = operand2;
       if (GET_CODE (operand2) == REG)
-	loperands[3] = gen_rtx_REG (SImode, REGNO (operand2) + 1);
+        loperands[3] = gen_rtx_REG (SImode, REGNO (operand2) + 1);
       else
-	loperands[3] = adjust_address (operand2, SImode, 4);
+        loperands[3] = adjust_address (operand2, SImode, 4);
     }
   loperands[4] = gen_label_rtx ();
   if (operand2 != const0_rtx)
@@ -1739,16 +1934,16 @@ output_scc_di (rtx op, rtx operand1, rtx
   else
     {
       if (TARGET_68020 || TARGET_COLDFIRE || ! ADDRESS_REG_P (loperands[0]))
-	output_asm_insn ("tst%.l %0", loperands);
+        output_asm_insn ("tst%.l %0", loperands);
       else
-	output_asm_insn ("cmp%.w #0,%0", loperands);
+        output_asm_insn ("cmp%.w #0,%0", loperands);
 
       output_asm_insn ("jne %l4", loperands);
 
       if (TARGET_68020 || TARGET_COLDFIRE || ! ADDRESS_REG_P (loperands[1]))
-	output_asm_insn ("tst%.l %1", loperands);
+        output_asm_insn ("tst%.l %1", loperands);
       else
-	output_asm_insn ("cmp%.w #0,%1", loperands);
+        output_asm_insn ("cmp%.w #0,%1", loperands);
     }
 
   loperands[5] = dest;
@@ -1757,13 +1952,13 @@ output_scc_di (rtx op, rtx operand1, rtx
     {
       case EQ:
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("seq %5", loperands);
         break;
 
       case NE:
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("sne %5", loperands);
         break;
 
@@ -1771,15 +1966,15 @@ output_scc_di (rtx op, rtx operand1, rtx
         loperands[6] = gen_label_rtx ();
         output_asm_insn ("shi %5\n\tjra %l6", loperands);
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("sgt %5", loperands);
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[6]));
+        				   CODE_LABEL_NUMBER (loperands[6]));
         break;
 
       case GTU:
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("shi %5", loperands);
         break;
 
@@ -1787,15 +1982,15 @@ output_scc_di (rtx op, rtx operand1, rtx
         loperands[6] = gen_label_rtx ();
         output_asm_insn ("scs %5\n\tjra %l6", loperands);
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("slt %5", loperands);
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[6]));
+        				   CODE_LABEL_NUMBER (loperands[6]));
         break;
 
       case LTU:
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("scs %5", loperands);
         break;
 
@@ -1803,15 +1998,15 @@ output_scc_di (rtx op, rtx operand1, rtx
         loperands[6] = gen_label_rtx ();
         output_asm_insn ("scc %5\n\tjra %l6", loperands);
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("sge %5", loperands);
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[6]));
+        				   CODE_LABEL_NUMBER (loperands[6]));
         break;
 
       case GEU:
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("scc %5", loperands);
         break;
 
@@ -1819,20 +2014,20 @@ output_scc_di (rtx op, rtx operand1, rtx
         loperands[6] = gen_label_rtx ();
         output_asm_insn ("sls %5\n\tjra %l6", loperands);
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("sle %5", loperands);
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[6]));
+        				   CODE_LABEL_NUMBER (loperands[6]));
         break;
 
       case LEU:
         (*targetm.asm_out.internal_label) (asm_out_file, "L",
-					   CODE_LABEL_NUMBER (loperands[4]));
+        				   CODE_LABEL_NUMBER (loperands[4]));
         output_asm_insn ("sls %5", loperands);
         break;
 
       default:
-	gcc_unreachable ();
+        gcc_unreachable ();
     }
   return "";
 }
@@ -1840,6 +2035,7 @@ output_scc_di (rtx op, rtx operand1, rtx
 const char *
 output_btst (rtx *operands, rtx countop, rtx dataop, rtx insn, int signpos)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   operands[0] = countop;
   operands[1] = dataop;
 
@@ -1847,50 +2043,50 @@ output_btst (rtx *operands, rtx countop,
     {
       register int count = INTVAL (countop);
       /* If COUNT is bigger than size of storage unit in use,
-	 advance to the containing unit of same size.  */
+         advance to the containing unit of same size.  */
       if (count > signpos)
-	{
-	  int offset = (count & ~signpos) / 8;
-	  count = count & signpos;
-	  operands[1] = dataop = adjust_address (dataop, QImode, offset);
-	}
+        {
+          int offset = (count & ~signpos) / 8;
+          count = count & signpos;
+          operands[1] = dataop = adjust_address (dataop, QImode, offset);
+        }
       if (count == signpos)
-	cc_status.flags = CC_NOT_POSITIVE | CC_Z_IN_NOT_N;
+        cc_status.flags = CC_NOT_POSITIVE | CC_Z_IN_NOT_N;
       else
-	cc_status.flags = CC_NOT_NEGATIVE | CC_Z_IN_NOT_N;
+        cc_status.flags = CC_NOT_NEGATIVE | CC_Z_IN_NOT_N;
 
       /* These three statements used to use next_insns_test_no...
-	 but it appears that this should do the same job.  */
+         but it appears that this should do the same job.  */
       if (count == 31
-	  && next_insn_tests_no_inequality (insn))
-	return "tst%.l %1";
+          && next_insn_tests_no_inequality (insn))
+        return "tst%.l %1";
       if (count == 15
-	  && next_insn_tests_no_inequality (insn))
-	return "tst%.w %1";
+          && next_insn_tests_no_inequality (insn))
+        return "tst%.w %1";
       if (count == 7
-	  && next_insn_tests_no_inequality (insn))
-	return "tst%.b %1";
+          && next_insn_tests_no_inequality (insn))
+        return "tst%.b %1";
       /* Try to use `movew to ccr' followed by the appropriate branch insn.
          On some m68k variants unfortunately that's slower than btst.
          On 68000 and higher, that should also work for all HImode operands. */
       if (TUNE_CPU32 || TARGET_COLDFIRE || optimize_size)
-	{
-	  if (count == 3 && DATA_REG_P (operands[1])
-	      && next_insn_tests_no_inequality (insn))
-	    {
-	    cc_status.flags = CC_NOT_NEGATIVE | CC_Z_IN_NOT_N | CC_NO_OVERFLOW;
-	    return "move%.w %1,%%ccr";
-	    }
-	  if (count == 2 && DATA_REG_P (operands[1])
-	      && next_insn_tests_no_inequality (insn))
-	    {
-	    cc_status.flags = CC_NOT_NEGATIVE | CC_INVERTED | CC_NO_OVERFLOW;
-	    return "move%.w %1,%%ccr";
-	    }
-	  /* count == 1 followed by bvc/bvs and
-	     count == 0 followed by bcc/bcs are also possible, but need
-	     m68k-specific CC_Z_IN_NOT_V and CC_Z_IN_NOT_C flags. */
-	}
+        {
+          if (count == 3 && DATA_REG_P (operands[1])
+              && next_insn_tests_no_inequality (insn))
+            {
+            cc_status.flags = CC_NOT_NEGATIVE | CC_Z_IN_NOT_N | CC_NO_OVERFLOW;
+            return "move%.w %1,%%ccr";
+            }
+          if (count == 2 && DATA_REG_P (operands[1])
+              && next_insn_tests_no_inequality (insn))
+            {
+            cc_status.flags = CC_NOT_NEGATIVE | CC_INVERTED | CC_NO_OVERFLOW;
+            return "move%.w %1,%%ccr";
+            }
+          /* count == 1 followed by bvc/bvs and
+             count == 0 followed by bcc/bcs are also possible, but need
+             m68k-specific CC_Z_IN_NOT_V and CC_Z_IN_NOT_C flags. */
+        }
 
       cc_status.flags = CC_NOT_NEGATIVE;
     }
@@ -1903,14 +2099,15 @@ output_btst (rtx *operands, rtx countop,
 bool
 m68k_legitimate_base_reg_p (rtx x, bool strict_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   /* Allow SUBREG everywhere we allow REG.  This results in better code.  */
   if (!strict_p && GET_CODE (x) == SUBREG)
     x = SUBREG_REG (x);
 
   return (REG_P (x)
-	  && (strict_p
-	      ? REGNO_OK_FOR_BASE_P (REGNO (x))
-	      : REGNO_OK_FOR_BASE_NONSTRICT_P (REGNO (x))));
+          && (strict_p
+              ? REGNO_OK_FOR_BASE_P (REGNO (x))
+              : REGNO_OK_FOR_BASE_NONSTRICT_P (REGNO (x))));
 }
 
 /* Return true if X is a legitimate index register.  STRICT_P says
@@ -1919,13 +2116,14 @@ m68k_legitimate_base_reg_p (rtx x, bool
 bool
 m68k_legitimate_index_reg_p (rtx x, bool strict_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (!strict_p && GET_CODE (x) == SUBREG)
     x = SUBREG_REG (x);
 
   return (REG_P (x)
-	  && (strict_p
-	      ? REGNO_OK_FOR_INDEX_P (REGNO (x))
-	      : REGNO_OK_FOR_INDEX_NONSTRICT_P (REGNO (x))));
+          && (strict_p
+              ? REGNO_OK_FOR_INDEX_P (REGNO (x))
+              : REGNO_OK_FOR_INDEX_NONSTRICT_P (REGNO (x))));
 }
 
 /* Return true if X is a legitimate index expression for a (d8,An,Xn) or
@@ -1935,6 +2133,7 @@ m68k_legitimate_index_reg_p (rtx x, bool
 static bool
 m68k_decompose_index (rtx x, bool strict_p, struct m68k_address *address)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int scale;
 
   /* Check for a scale factor.  */
@@ -1943,9 +2142,9 @@ m68k_decompose_index (rtx x, bool strict
       && GET_CODE (x) == MULT
       && GET_CODE (XEXP (x, 1)) == CONST_INT
       && (INTVAL (XEXP (x, 1)) == 2
-	  || INTVAL (XEXP (x, 1)) == 4
-	  || (INTVAL (XEXP (x, 1)) == 8
-	      && (TARGET_COLDFIRE_FPU || !TARGET_COLDFIRE))))
+          || INTVAL (XEXP (x, 1)) == 4
+          || (INTVAL (XEXP (x, 1)) == 8
+              && (TARGET_COLDFIRE_FPU || !TARGET_COLDFIRE))))
     {
       scale = INTVAL (XEXP (x, 1));
       x = XEXP (x, 0);
@@ -1972,14 +2171,15 @@ m68k_decompose_index (rtx x, bool strict
 bool
 m68k_illegitimate_symbolic_constant_p (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx base, offset;
 
   if (M68K_OFFSETS_MUST_BE_WITHIN_SECTIONS_P)
     {
       split_const (x, &base, &offset);
       if (GET_CODE (base) == SYMBOL_REF
-	  && !offset_within_block_p (base, INTVAL (offset)))
-	return true;
+          && !offset_within_block_p (base, INTVAL (offset)))
+        return true;
     }
   return m68k_tls_reference_p (x, false);
 }
@@ -1991,6 +2191,7 @@ m68k_illegitimate_symbolic_constant_p (r
 static bool
 m68k_legitimate_constant_address_p (rtx x, unsigned int reach, bool strict_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx base, offset;
 
   if (!CONSTANT_ADDRESS_P (x))
@@ -2005,8 +2206,8 @@ m68k_legitimate_constant_address_p (rtx
     {
       split_const (x, &base, &offset);
       if (GET_CODE (base) == SYMBOL_REF
-	  && !offset_within_block_p (base, INTVAL (offset) + reach - 1))
-	return false;
+          && !offset_within_block_p (base, INTVAL (offset) + reach - 1))
+        return false;
     }
 
   return !m68k_tls_reference_p (x, false);
@@ -2018,6 +2219,7 @@ m68k_legitimate_constant_address_p (rtx
 static bool
 m68k_jump_table_ref_p (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (GET_CODE (x) != LABEL_REF)
     return false;
 
@@ -2035,8 +2237,9 @@ m68k_jump_table_ref_p (rtx x)
 
 static bool
 m68k_decompose_address (enum machine_mode mode, rtx x,
-			bool strict_p, struct m68k_address *address)
+        		bool strict_p, struct m68k_address *address)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   unsigned int reach;
 
   memset (address, 0, sizeof (*address));
@@ -2080,13 +2283,13 @@ m68k_decompose_address (enum machine_mod
       && XEXP (x, 0) == pic_offset_table_rtx)
     {
       /* As we are processing a PLUS, do not unwrap RELOC32 symbols --
-	 they are invalid in this context.  */
+         they are invalid in this context.  */
       if (m68k_unwrap_symbol (XEXP (x, 1), false) != XEXP (x, 1))
-	{
-	  address->base = XEXP (x, 0);
-	  address->offset = XEXP (x, 1);
-	  return true;
-	}
+        {
+          address->base = XEXP (x, 0);
+          address->offset = XEXP (x, 1);
+          return true;
+        }
     }
 
   /* The ColdFire FPU only accepts addressing modes 2-5.  */
@@ -2124,56 +2327,56 @@ m68k_decompose_address (enum machine_mod
     {
       /* Check for a nonzero base displacement.  */
       if (GET_CODE (x) == PLUS
-	  && m68k_legitimate_constant_address_p (XEXP (x, 1), reach, strict_p))
-	{
-	  address->offset = XEXP (x, 1);
-	  x = XEXP (x, 0);
-	}
+          && m68k_legitimate_constant_address_p (XEXP (x, 1), reach, strict_p))
+        {
+          address->offset = XEXP (x, 1);
+          x = XEXP (x, 0);
+        }
 
       /* Check for a suppressed index register.  */
       if (m68k_legitimate_base_reg_p (x, strict_p))
-	{
-	  address->base = x;
-	  return true;
-	}
+        {
+          address->base = x;
+          return true;
+        }
 
       /* Check for a suppressed base register.  Do not allow this case
-	 for non-symbolic offsets as it effectively gives gcc freedom
-	 to treat data registers as base registers, which can generate
-	 worse code.  */
+         for non-symbolic offsets as it effectively gives gcc freedom
+         to treat data registers as base registers, which can generate
+         worse code.  */
       if (address->offset
-	  && symbolic_operand (address->offset, VOIDmode)
-	  && m68k_decompose_index (x, strict_p, address))
-	return true;
+          && symbolic_operand (address->offset, VOIDmode)
+          && m68k_decompose_index (x, strict_p, address))
+        return true;
     }
   else
     {
       /* Check for a nonzero base displacement.  */
       if (GET_CODE (x) == PLUS
-	  && GET_CODE (XEXP (x, 1)) == CONST_INT
-	  && IN_RANGE (INTVAL (XEXP (x, 1)), -0x80, 0x80 - reach))
-	{
-	  address->offset = XEXP (x, 1);
-	  x = XEXP (x, 0);
-	}
+          && GET_CODE (XEXP (x, 1)) == CONST_INT
+          && IN_RANGE (INTVAL (XEXP (x, 1)), -0x80, 0x80 - reach))
+        {
+          address->offset = XEXP (x, 1);
+          x = XEXP (x, 0);
+        }
     }
 
   /* We now expect the sum of a base and an index.  */
   if (GET_CODE (x) == PLUS)
     {
       if (m68k_legitimate_base_reg_p (XEXP (x, 0), strict_p)
-	  && m68k_decompose_index (XEXP (x, 1), strict_p, address))
-	{
-	  address->base = XEXP (x, 0);
-	  return true;
-	}
+          && m68k_decompose_index (XEXP (x, 1), strict_p, address))
+        {
+          address->base = XEXP (x, 0);
+          return true;
+        }
 
       if (m68k_legitimate_base_reg_p (XEXP (x, 1), strict_p)
-	  && m68k_decompose_index (XEXP (x, 0), strict_p, address))
-	{
-	  address->base = XEXP (x, 1);
-	  return true;
-	}
+          && m68k_decompose_index (XEXP (x, 0), strict_p, address))
+        {
+          address->base = XEXP (x, 1);
+          return true;
+        }
     }
   return false;
 }
@@ -2184,6 +2387,7 @@ m68k_decompose_address (enum machine_mod
 bool
 m68k_legitimate_address_p (enum machine_mode mode, rtx x, bool strict_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   struct m68k_address address;
 
   return m68k_decompose_address (mode, x, strict_p, &address);
@@ -2195,10 +2399,11 @@ m68k_legitimate_address_p (enum machine_
 static bool
 m68k_legitimate_mem_p (rtx x, struct m68k_address *address)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   return (MEM_P (x)
-	  && m68k_decompose_address (GET_MODE (x), XEXP (x, 0),
-				     reload_in_progress || reload_completed,
-				     address));
+          && m68k_decompose_address (GET_MODE (x), XEXP (x, 0),
+        			     reload_in_progress || reload_completed,
+        			     address));
 }
 
 /* Return true if X matches the 'Q' constraint.  It must be a memory
@@ -2207,13 +2412,14 @@ m68k_legitimate_mem_p (rtx x, struct m68
 bool
 m68k_matches_q_p (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   struct m68k_address address;
 
   return (m68k_legitimate_mem_p (x, &address)
-	  && address.code == UNKNOWN
-	  && address.base
-	  && !address.offset
-	  && !address.index);
+          && address.code == UNKNOWN
+          && address.base
+          && !address.offset
+          && !address.index);
 }
 
 /* Return true if X matches the 'U' constraint.  It must be a base address
@@ -2222,13 +2428,14 @@ m68k_matches_q_p (rtx x)
 bool
 m68k_matches_u_p (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   struct m68k_address address;
 
   return (m68k_legitimate_mem_p (x, &address)
-	  && address.code == UNKNOWN
-	  && address.base
-	  && address.offset
-	  && !address.index);
+          && address.code == UNKNOWN
+          && address.base
+          && address.offset
+          && !address.index);
 }
 
 /* Return GOT pointer.  */
@@ -2236,6 +2443,7 @@ m68k_matches_u_p (rtx x)
 static rtx
 m68k_get_gp (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (pic_offset_table_rtx == NULL_RTX)
     pic_offset_table_rtx = gen_rtx_REG (Pmode, PIC_REG);
 
@@ -2247,7 +2455,7 @@ m68k_get_gp (void)
 /* M68K relocations, used to distinguish GOT and TLS relocations in UNSPEC
    wrappers.  */
 enum m68k_reloc { RELOC_GOT, RELOC_TLSGD, RELOC_TLSLDM, RELOC_TLSLDO,
-		  RELOC_TLSIE, RELOC_TLSLE };
+        	  RELOC_TLSIE, RELOC_TLSLE };
 
 #define TLS_RELOC_P(RELOC) ((RELOC) != RELOC_GOT)
 
@@ -2258,6 +2466,7 @@ enum m68k_reloc { RELOC_GOT, RELOC_TLSGD
 static rtx
 m68k_wrap_symbol (rtx x, enum m68k_reloc reloc, rtx base_reg, rtx temp_reg)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   bool use_x_p;
 
   use_x_p = (base_reg == pic_offset_table_rtx) ? TARGET_XGOT : TARGET_XTLS;
@@ -2269,16 +2478,16 @@ m68k_wrap_symbol (rtx x, enum m68k_reloc
        add.l <BASE_REG>,<TEMP_REG>  */
     {
       /* Wrap X in UNSPEC_??? to tip m68k_output_addr_const_extra
-	 to put @RELOC after reference.  */
+         to put @RELOC after reference.  */
       x = gen_rtx_UNSPEC (Pmode, gen_rtvec (2, x, GEN_INT (reloc)),
-			  UNSPEC_RELOC32);
+        		  UNSPEC_RELOC32);
       x = gen_rtx_CONST (Pmode, x);
 
       if (temp_reg == NULL)
-	{
-	  gcc_assert (can_create_pseudo_p ());
-	  temp_reg = gen_reg_rtx (Pmode);
-	}
+        {
+          gcc_assert (can_create_pseudo_p ());
+          temp_reg = gen_reg_rtx (Pmode);
+        }
 
       emit_move_insn (temp_reg, x);
       emit_insn (gen_addsi3 (temp_reg, temp_reg, base_reg));
@@ -2287,7 +2496,7 @@ m68k_wrap_symbol (rtx x, enum m68k_reloc
   else
     {
       x = gen_rtx_UNSPEC (Pmode, gen_rtvec (2, x, GEN_INT (reloc)),
-			  UNSPEC_RELOC16);
+        		  UNSPEC_RELOC16);
       x = gen_rtx_CONST (Pmode, x);
 
       x = gen_rtx_PLUS (Pmode, base_reg, x);
@@ -2302,8 +2511,9 @@ m68k_wrap_symbol (rtx x, enum m68k_reloc
 
 static rtx
 m68k_unwrap_symbol_1 (rtx orig, bool unwrap_reloc32_p,
-		      enum m68k_reloc *reloc_ptr)
+        	      enum m68k_reloc *reloc_ptr)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (GET_CODE (orig) == CONST)
     {
       rtx x;
@@ -2312,34 +2522,34 @@ m68k_unwrap_symbol_1 (rtx orig, bool unw
       x = XEXP (orig, 0);
 
       if (reloc_ptr == NULL)
-	reloc_ptr = &dummy;
+        reloc_ptr = &dummy;
 
       /* Handle an addend.  */
       if ((GET_CODE (x) == PLUS || GET_CODE (x) == MINUS)
-	  && CONST_INT_P (XEXP (x, 1)))
-	x = XEXP (x, 0);
+          && CONST_INT_P (XEXP (x, 1)))
+        x = XEXP (x, 0);
 
       if (GET_CODE (x) == UNSPEC)
-	{
-	  switch (XINT (x, 1))
-	    {
-	    case UNSPEC_RELOC16:
-	      orig = XVECEXP (x, 0, 0);
-	      *reloc_ptr = (enum m68k_reloc) INTVAL (XVECEXP (x, 0, 1));
-	      break;
-
-	    case UNSPEC_RELOC32:
-	      if (unwrap_reloc32_p)
-		{
-		  orig = XVECEXP (x, 0, 0);
-		  *reloc_ptr = (enum m68k_reloc) INTVAL (XVECEXP (x, 0, 1));
-		}
-	      break;
-
-	    default:
-	      break;
-	    }
-	}
+        {
+          switch (XINT (x, 1))
+            {
+            case UNSPEC_RELOC16:
+              orig = XVECEXP (x, 0, 0);
+              *reloc_ptr = (enum m68k_reloc) INTVAL (XVECEXP (x, 0, 1));
+              break;
+
+            case UNSPEC_RELOC32:
+              if (unwrap_reloc32_p)
+        	{
+        	  orig = XVECEXP (x, 0, 0);
+        	  *reloc_ptr = (enum m68k_reloc) INTVAL (XVECEXP (x, 0, 1));
+        	}
+              break;
+
+            default:
+              break;
+            }
+        }
     }
 
   return orig;
@@ -2351,6 +2561,7 @@ m68k_unwrap_symbol_1 (rtx orig, bool unw
 rtx
 m68k_unwrap_symbol (rtx orig, bool unwrap_reloc32_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   return m68k_unwrap_symbol_1 (orig, unwrap_reloc32_p, NULL);
 }
 
@@ -2359,6 +2570,7 @@ m68k_unwrap_symbol (rtx orig, bool unwra
 static int
 m68k_final_prescan_insn_1 (rtx *x_ptr, void *data ATTRIBUTE_UNUSED)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx x = *x_ptr;
 
   if (m68k_unwrap_symbol (x, true) != x)
@@ -2370,26 +2582,26 @@ m68k_final_prescan_insn_1 (rtx *x_ptr, v
       plus = XEXP (x, 0);
 
       if (GET_CODE (plus) == PLUS || GET_CODE (plus) == MINUS)
-	{
-	  rtx unspec;
-	  rtx addend;
+        {
+          rtx unspec;
+          rtx addend;
 
-	  unspec = XEXP (plus, 0);
-	  gcc_assert (GET_CODE (unspec) == UNSPEC);
-	  addend = XEXP (plus, 1);
-	  gcc_assert (CONST_INT_P (addend));
+          unspec = XEXP (plus, 0);
+          gcc_assert (GET_CODE (unspec) == UNSPEC);
+          addend = XEXP (plus, 1);
+          gcc_assert (CONST_INT_P (addend));
 
-	  /* We now have all the pieces, rearrange them.  */
+          /* We now have all the pieces, rearrange them.  */
 
-	  /* Move symbol to plus.  */
-	  XEXP (plus, 0) = XVECEXP (unspec, 0, 0);
+          /* Move symbol to plus.  */
+          XEXP (plus, 0) = XVECEXP (unspec, 0, 0);
 
-	  /* Move plus inside unspec.  */
-	  XVECEXP (unspec, 0, 0) = plus;
+          /* Move plus inside unspec.  */
+          XVECEXP (unspec, 0, 0) = plus;
 
-	  /* Move unspec to top level of const.  */
-	  XEXP (x, 0) = unspec;
-	}
+          /* Move unspec to top level of const.  */
+          XEXP (x, 0) = unspec;
+        }
 
       return -1;
     }
@@ -2401,8 +2613,9 @@ m68k_final_prescan_insn_1 (rtx *x_ptr, v
 
 void
 m68k_final_prescan_insn (rtx insn ATTRIBUTE_UNUSED,
-			 rtx *operands, int n_operands)
+        		 rtx *operands, int n_operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int i;
 
   /* Combine and, possibly, other optimizations may do good job
@@ -2438,6 +2651,7 @@ m68k_final_prescan_insn (rtx insn ATTRIB
 static rtx
 m68k_move_to_reg (rtx x, rtx orig, rtx reg)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx insn;
 
   if (reg == NULL_RTX)
@@ -2460,6 +2674,7 @@ m68k_move_to_reg (rtx x, rtx orig, rtx r
 static rtx
 m68k_wrap_symbol_into_got_ref (rtx x, enum m68k_reloc reloc, rtx temp_reg)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   x = m68k_wrap_symbol (x, reloc, m68k_get_gp (), temp_reg);
 
   x = gen_rtx_MEM (Pmode, x);
@@ -2490,19 +2705,19 @@ m68k_wrap_symbol_into_got_ref (rtx x, en
    When not generating PIC code to store the value 12345 into _foo
    we would generate the following code:
 
-	movel #12345, _foo
+        movel #12345, _foo
 
    When generating PIC two transformations are made.  First, the compiler
    loads the address of foo into a register.  So the first transformation makes:
 
-	lea	_foo, a0
-	movel   #12345, a0@
+        lea	_foo, a0
+        movel   #12345, a0@
 
    The code in movsi will intercept the lea instruction and call this
    routine which will transform the instructions into:
 
-	movel   a5@(_foo:w), a0
-	movel   #12345, a0@
+        movel   a5@(_foo:w), a0
+        movel   #12345, a0@
    
 
    That (in a nutshell) is how *all* symbol and label references are 
@@ -2510,8 +2725,9 @@ m68k_wrap_symbol_into_got_ref (rtx x, en
 
 rtx
 legitimize_pic_address (rtx orig, enum machine_mode mode ATTRIBUTE_UNUSED,
-		        rtx reg)
+        	        rtx reg)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx pic_ref = orig;
 
   /* First handle a simple SYMBOL_REF or LABEL_REF */
@@ -2528,7 +2744,7 @@ legitimize_pic_address (rtx orig, enum m
 
       /* Make sure this has not already been legitimized.  */
       if (m68k_unwrap_symbol (orig, true) != orig)
-	return orig;
+        return orig;
 
       gcc_assert (reg);
 
@@ -2537,12 +2753,12 @@ legitimize_pic_address (rtx orig, enum m
       
       base = legitimize_pic_address (XEXP (XEXP (orig, 0), 0), Pmode, reg);
       orig = legitimize_pic_address (XEXP (XEXP (orig, 0), 1), Pmode,
-				     base == reg ? 0 : reg);
+        			     base == reg ? 0 : reg);
 
       if (GET_CODE (orig) == CONST_INT)
-	pic_ref = plus_constant (base, INTVAL (orig));
+        pic_ref = plus_constant (base, INTVAL (orig));
       else
-	pic_ref = gen_rtx_PLUS (Pmode, base, orig);
+        pic_ref = gen_rtx_PLUS (Pmode, base, orig);
     }
 
   return pic_ref;
@@ -2556,6 +2772,7 @@ static GTY(()) rtx m68k_tls_get_addr;
 static rtx
 m68k_get_tls_get_addr (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (m68k_tls_get_addr == NULL_RTX)
     m68k_tls_get_addr = init_one_libfunc ("__tls_get_addr");
 
@@ -2574,6 +2791,7 @@ static bool m68k_libcall_value_in_a0_p =
 static rtx
 m68k_call_tls_get_addr (rtx x, rtx eqv, enum m68k_reloc reloc)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx a0;
   rtx insns;
   rtx dest;
@@ -2600,7 +2818,7 @@ m68k_call_tls_get_addr (rtx x, rtx eqv,
 
   m68k_libcall_value_in_a0_p = true;
   a0 = emit_library_call_value (m68k_get_tls_get_addr (), NULL_RTX, LCT_PURE,
-				Pmode, 1, x, Pmode);
+        			Pmode, 1, x, Pmode);
   m68k_libcall_value_in_a0_p = false;
   
   insns = get_insns ();
@@ -2621,6 +2839,7 @@ static GTY(()) rtx m68k_read_tp;
 static rtx
 m68k_get_m68k_read_tp (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (m68k_read_tp == NULL_RTX)
     m68k_read_tp = init_one_libfunc ("__m68k_read_tp");
 
@@ -2633,6 +2852,7 @@ m68k_get_m68k_read_tp (void)
 static rtx 
 m68k_call_m68k_read_tp (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx a0;
   rtx eqv;
   rtx insns;
@@ -2649,7 +2869,7 @@ m68k_call_m68k_read_tp (void)
   /* Emit the call sequence.  */
   m68k_libcall_value_in_a0_p = true;
   a0 = emit_library_call_value (m68k_get_m68k_read_tp (), NULL_RTX, LCT_PURE,
-				Pmode, 0);
+        			Pmode, 0);
   m68k_libcall_value_in_a0_p = false;
   insns = get_insns ();
   end_sequence ();
@@ -2672,6 +2892,7 @@ m68k_call_m68k_read_tp (void)
 rtx
 m68k_legitimize_tls_address (rtx orig)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   switch (SYMBOL_REF_TLS_MODEL (orig))
     {
     case TLS_MODEL_GLOBAL_DYNAMIC:
@@ -2680,57 +2901,57 @@ m68k_legitimize_tls_address (rtx orig)
 
     case TLS_MODEL_LOCAL_DYNAMIC:
       {
-	rtx eqv;
-	rtx a0;
-	rtx x;
+        rtx eqv;
+        rtx a0;
+        rtx x;
  
-	/* Attach a unique REG_EQUIV, to allow the RTL optimizers to
-	   share the LDM result with other LD model accesses.  */
-	eqv = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, const0_rtx),
-			      UNSPEC_RELOC32);
+        /* Attach a unique REG_EQUIV, to allow the RTL optimizers to
+           share the LDM result with other LD model accesses.  */
+        eqv = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, const0_rtx),
+        		      UNSPEC_RELOC32);
 
-	a0 = m68k_call_tls_get_addr (orig, eqv, RELOC_TLSLDM);
+        a0 = m68k_call_tls_get_addr (orig, eqv, RELOC_TLSLDM);
 
-	x = m68k_wrap_symbol (orig, RELOC_TLSLDO, a0, NULL_RTX);
+        x = m68k_wrap_symbol (orig, RELOC_TLSLDO, a0, NULL_RTX);
 
-	if (can_create_pseudo_p ())
-	  x = m68k_move_to_reg (x, orig, NULL_RTX);
+        if (can_create_pseudo_p ())
+          x = m68k_move_to_reg (x, orig, NULL_RTX);
 
-	orig = x;
-	break;
+        orig = x;
+        break;
       }
 
     case TLS_MODEL_INITIAL_EXEC:
       {
-	rtx a0;
-	rtx x;
+        rtx a0;
+        rtx x;
 
-	a0 = m68k_call_m68k_read_tp ();
+        a0 = m68k_call_m68k_read_tp ();
 
-	x = m68k_wrap_symbol_into_got_ref (orig, RELOC_TLSIE, NULL_RTX);
-	x = gen_rtx_PLUS (Pmode, x, a0);
+        x = m68k_wrap_symbol_into_got_ref (orig, RELOC_TLSIE, NULL_RTX);
+        x = gen_rtx_PLUS (Pmode, x, a0);
 
-	if (can_create_pseudo_p ())
-	  x = m68k_move_to_reg (x, orig, NULL_RTX);
+        if (can_create_pseudo_p ())
+          x = m68k_move_to_reg (x, orig, NULL_RTX);
 
-	orig = x;
-	break;
+        orig = x;
+        break;
       }
 
     case TLS_MODEL_LOCAL_EXEC:
       {
-	rtx a0;
-	rtx x;
+        rtx a0;
+        rtx x;
 
-	a0 = m68k_call_m68k_read_tp ();
+        a0 = m68k_call_m68k_read_tp ();
 
-	x = m68k_wrap_symbol (orig, RELOC_TLSLE, a0, NULL_RTX);
+        x = m68k_wrap_symbol (orig, RELOC_TLSLE, a0, NULL_RTX);
 
-	if (can_create_pseudo_p ())
-	  x = m68k_move_to_reg (x, orig, NULL_RTX);
+        if (can_create_pseudo_p ())
+          x = m68k_move_to_reg (x, orig, NULL_RTX);
 
-	orig = x;
-	break;
+        orig = x;
+        break;
       }
 
     default:
@@ -2745,6 +2966,7 @@ m68k_legitimize_tls_address (rtx orig)
 static bool
 m68k_tls_symbol_p (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (!TARGET_HAVE_TLS)
     return false;
 
@@ -2759,6 +2981,7 @@ m68k_tls_symbol_p (rtx x)
 static int
 m68k_tls_reference_p_1 (rtx *x_ptr, void *data ATTRIBUTE_UNUSED)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   /* Note: this is not the same as m68k_tls_symbol_p.  */
   if (GET_CODE (*x_ptr) == SYMBOL_REF)
     return SYMBOL_REF_TLS_MODEL (*x_ptr) != 0 ? 1 : 0;
@@ -2777,6 +3000,7 @@ m68k_tls_reference_p_1 (rtx *x_ptr, void
 bool
 m68k_tls_reference_p (rtx x, bool legitimate_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (!TARGET_HAVE_TLS)
     return false;
 
@@ -2787,7 +3011,7 @@ m68k_tls_reference_p (rtx x, bool legiti
       enum m68k_reloc reloc = RELOC_GOT;
 
       return (m68k_unwrap_symbol_1 (x, true, &reloc) != x
-	      && TLS_RELOC_P (reloc));
+              && TLS_RELOC_P (reloc));
     }
 }
 
@@ -2800,6 +3024,7 @@ m68k_tls_reference_p (rtx x, bool legiti
 M68K_CONST_METHOD
 m68k_const_method (HOST_WIDE_INT i)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   unsigned u;
 
   if (USE_MOVQ (i))
@@ -2810,15 +3035,15 @@ m68k_const_method (HOST_WIDE_INT i)
   if (!TARGET_COLDFIRE)
     {
       /* if -256 < N < 256 but N is not in range for a moveq
-	 N^ff will be, so use moveq #N^ff, dreg; not.b dreg.  */
+         N^ff will be, so use moveq #N^ff, dreg; not.b dreg.  */
       if (USE_MOVQ (i ^ 0xff))
-	return NOTB;
+        return NOTB;
       /* Likewise, try with not.w */
       if (USE_MOVQ (i ^ 0xffff))
-	return NOTW;
+        return NOTW;
       /* This is the only value where neg.w is useful */
       if (i == -65408)
-	return NEGW;
+        return NEGW;
     }
 
   /* Try also with swap.  */
@@ -2830,9 +3055,9 @@ m68k_const_method (HOST_WIDE_INT i)
     {
       /* Try using MVZ/MVS with an immediate value to load constants.  */
       if (i >= 0 && i <= 65535)
-	return MVZ;
+        return MVZ;
       if (i >= -32768 && i <= 32767)
-	return MVS;
+        return MVS;
     }
 
   /* Otherwise, use move.l */
@@ -2844,6 +3069,7 @@ m68k_const_method (HOST_WIDE_INT i)
 static int
 const_int_cost (HOST_WIDE_INT i)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   switch (m68k_const_method (i))
     {
     case MOVQ:
@@ -2866,14 +3092,15 @@ const_int_cost (HOST_WIDE_INT i)
 
 static bool
 m68k_rtx_costs (rtx x, int code, int outer_code, int *total,
-		bool speed ATTRIBUTE_UNUSED)
+        	bool speed ATTRIBUTE_UNUSED)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   switch (code)
     {
     case CONST_INT:
       /* Constant zero is super cheap due to clr instruction.  */
       if (x == const0_rtx)
-	*total = 0;
+        *total = 0;
       else
         *total = const_int_cost (INTVAL (x));
       return true;
@@ -2889,9 +3116,9 @@ m68k_rtx_costs (rtx x, int code, int out
          encourage creating tstsf and tstdf insns.  */
       if (outer_code == COMPARE
           && (x == CONST0_RTX (SFmode) || x == CONST0_RTX (DFmode)))
-	*total = 4;
+        *total = 4;
       else
-	*total = 5;
+        *total = 5;
       return true;
 
     /* These are vaguely right for a 68020.  */
@@ -2925,62 +3152,62 @@ m68k_rtx_costs (rtx x, int code, int out
     case PLUS:
       /* An lea costs about three times as much as a simple add.  */
       if (GET_MODE (x) == SImode
-	  && GET_CODE (XEXP (x, 1)) == REG
-	  && GET_CODE (XEXP (x, 0)) == MULT
-	  && GET_CODE (XEXP (XEXP (x, 0), 0)) == REG
-	  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
-	  && (INTVAL (XEXP (XEXP (x, 0), 1)) == 2
-	      || INTVAL (XEXP (XEXP (x, 0), 1)) == 4
-	      || INTVAL (XEXP (XEXP (x, 0), 1)) == 8))
-	{
-	    /* lea an@(dx:l:i),am */
-	    *total = COSTS_N_INSNS (TARGET_COLDFIRE ? 2 : 3);
-	    return true;
-	}
+          && GET_CODE (XEXP (x, 1)) == REG
+          && GET_CODE (XEXP (x, 0)) == MULT
+          && GET_CODE (XEXP (XEXP (x, 0), 0)) == REG
+          && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+          && (INTVAL (XEXP (XEXP (x, 0), 1)) == 2
+              || INTVAL (XEXP (XEXP (x, 0), 1)) == 4
+              || INTVAL (XEXP (XEXP (x, 0), 1)) == 8))
+        {
+            /* lea an@(dx:l:i),am */
+            *total = COSTS_N_INSNS (TARGET_COLDFIRE ? 2 : 3);
+            return true;
+        }
       return false;
 
     case ASHIFT:
     case ASHIFTRT:
     case LSHIFTRT:
       if (TUNE_68060)
-	{
+        {
           *total = COSTS_N_INSNS(1);
-	  return true;
-	}
+          return true;
+        }
       if (TUNE_68000_10)
         {
-	  if (GET_CODE (XEXP (x, 1)) == CONST_INT)
-	    {
-	      if (INTVAL (XEXP (x, 1)) < 16)
-	        *total = COSTS_N_INSNS (2) + INTVAL (XEXP (x, 1)) / 2;
-	      else
-	        /* We're using clrw + swap for these cases.  */
-	        *total = COSTS_N_INSNS (4) + (INTVAL (XEXP (x, 1)) - 16) / 2;
-	    }
-	  else
-	    *total = COSTS_N_INSNS (10); /* Worst case.  */
-	  return true;
+          if (GET_CODE (XEXP (x, 1)) == CONST_INT)
+            {
+              if (INTVAL (XEXP (x, 1)) < 16)
+                *total = COSTS_N_INSNS (2) + INTVAL (XEXP (x, 1)) / 2;
+              else
+                /* We're using clrw + swap for these cases.  */
+                *total = COSTS_N_INSNS (4) + (INTVAL (XEXP (x, 1)) - 16) / 2;
+            }
+          else
+            *total = COSTS_N_INSNS (10); /* Worst case.  */
+          return true;
         }
       /* A shift by a big integer takes an extra instruction.  */
       if (GET_CODE (XEXP (x, 1)) == CONST_INT
-	  && (INTVAL (XEXP (x, 1)) == 16))
-	{
-	  *total = COSTS_N_INSNS (2);	 /* clrw;swap */
-	  return true;
-	}
+          && (INTVAL (XEXP (x, 1)) == 16))
+        {
+          *total = COSTS_N_INSNS (2);	 /* clrw;swap */
+          return true;
+        }
       if (GET_CODE (XEXP (x, 1)) == CONST_INT
-	  && !(INTVAL (XEXP (x, 1)) > 0
-	       && INTVAL (XEXP (x, 1)) <= 8))
-	{
-	  *total = COSTS_N_INSNS (TARGET_COLDFIRE ? 1 : 3);	 /* lsr #i,dn */
-	  return true;
-	}
+          && !(INTVAL (XEXP (x, 1)) > 0
+               && INTVAL (XEXP (x, 1)) <= 8))
+        {
+          *total = COSTS_N_INSNS (TARGET_COLDFIRE ? 1 : 3);	 /* lsr #i,dn */
+          return true;
+        }
       return false;
 
     case MULT:
       if ((GET_CODE (XEXP (x, 0)) == ZERO_EXTEND
-	   || GET_CODE (XEXP (x, 0)) == SIGN_EXTEND)
-	  && GET_MODE (x) == SImode)
+           || GET_CODE (XEXP (x, 0)) == SIGN_EXTEND)
+          && GET_MODE (x) == SImode)
         *total = COSTS_N_INSNS (MULW_COST);
       else if (GET_MODE (x) == QImode || GET_MODE (x) == HImode)
         *total = COSTS_N_INSNS (MULW_COST);
@@ -2997,7 +3224,7 @@ m68k_rtx_costs (rtx x, int code, int out
       else if (TARGET_CF_HWDIV)
         *total = COSTS_N_INSNS (18);
       else
-	*total = COSTS_N_INSNS (43);		/* div.l */
+        *total = COSTS_N_INSNS (43);		/* div.l */
       return true;
 
     case ZERO_EXTRACT:
@@ -3016,6 +3243,7 @@ m68k_rtx_costs (rtx x, int code, int out
 static const char *
 output_move_const_into_data_reg (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   HOST_WIDE_INT i;
 
   i = INTVAL (operands[1]);
@@ -3040,10 +3268,10 @@ output_move_const_into_data_reg (rtx *op
       return "moveq #-128,%0\n\tneg%.w %0";
     case SWAP:
       {
-	unsigned u = i;
+        unsigned u = i;
 
-	operands[1] = GEN_INT ((u << 16) | (u >> 16));
-	return "moveq %1,%0\n\tswap %0";
+        operands[1] = GEN_INT ((u << 16) | (u >> 16));
+        return "moveq %1,%0\n\tswap %0";
       }
     case MOVL:
       return "move%.l %1,%0";
@@ -3057,6 +3285,7 @@ output_move_const_into_data_reg (rtx *op
 bool
 valid_mov3q_const (HOST_WIDE_INT i)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   return TARGET_ISAB && (i == -1 || IN_RANGE (i, 1, 7));
 }
 
@@ -3066,6 +3295,7 @@ valid_mov3q_const (HOST_WIDE_INT i)
 static const char *
 output_move_simode_const (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx dest;
   HOST_WIDE_INT src;
 
@@ -3075,7 +3305,7 @@ output_move_simode_const (rtx *operands)
       && (DATA_REG_P (dest) || MEM_P (dest))
       /* clr insns on 68000 read before writing.  */
       && ((TARGET_68010 || TARGET_COLDFIRE)
-	  || !(MEM_P (dest) && MEM_VOLATILE_P (dest))))
+          || !(MEM_P (dest) && MEM_VOLATILE_P (dest))))
     return "clr%.l %0";
   else if (GET_MODE (dest) == SImode && valid_mov3q_const (src))
     return "mov3q%.l %1,%0";
@@ -3090,9 +3320,9 @@ output_move_simode_const (rtx *operands)
       return "move%.w %1,%0";
     }
   else if (MEM_P (dest)
-	   && GET_CODE (XEXP (dest, 0)) == PRE_DEC
-	   && REGNO (XEXP (XEXP (dest, 0), 0)) == STACK_POINTER_REGNUM
-	   && IN_RANGE (src, -0x8000, 0x7fff))
+           && GET_CODE (XEXP (dest, 0)) == PRE_DEC
+           && REGNO (XEXP (XEXP (dest, 0), 0)) == STACK_POINTER_REGNUM
+           && IN_RANGE (src, -0x8000, 0x7fff))
     {
       if (valid_mov3q_const (src))
         return "mov3q%.l %1,%-";
@@ -3104,15 +3334,16 @@ output_move_simode_const (rtx *operands)
 const char *
 output_move_simode (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (GET_CODE (operands[1]) == CONST_INT)
     return output_move_simode_const (operands);
   else if ((GET_CODE (operands[1]) == SYMBOL_REF
-	    || GET_CODE (operands[1]) == CONST)
-	   && push_operand (operands[0], SImode))
+            || GET_CODE (operands[1]) == CONST)
+           && push_operand (operands[0], SImode))
     return "pea %a1";
   else if ((GET_CODE (operands[1]) == SYMBOL_REF
-	    || GET_CODE (operands[1]) == CONST)
-	   && ADDRESS_REG_P (operands[0]))
+            || GET_CODE (operands[1]) == CONST)
+           && ADDRESS_REG_P (operands[0]))
     return "lea %a1,%0";
   return "move%.l %1,%0";
 }
@@ -3120,26 +3351,27 @@ output_move_simode (rtx *operands)
 const char *
 output_move_himode (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
  if (GET_CODE (operands[1]) == CONST_INT)
     {
       if (operands[1] == const0_rtx
-	  && (DATA_REG_P (operands[0])
-	      || GET_CODE (operands[0]) == MEM)
-	  /* clr insns on 68000 read before writing.  */
-	  && ((TARGET_68010 || TARGET_COLDFIRE)
-	      || !(GET_CODE (operands[0]) == MEM
-		   && MEM_VOLATILE_P (operands[0]))))
-	return "clr%.w %0";
+          && (DATA_REG_P (operands[0])
+              || GET_CODE (operands[0]) == MEM)
+          /* clr insns on 68000 read before writing.  */
+          && ((TARGET_68010 || TARGET_COLDFIRE)
+              || !(GET_CODE (operands[0]) == MEM
+        	   && MEM_VOLATILE_P (operands[0]))))
+        return "clr%.w %0";
       else if (operands[1] == const0_rtx
-	       && ADDRESS_REG_P (operands[0]))
-	return "sub%.l %0,%0";
+               && ADDRESS_REG_P (operands[0]))
+        return "sub%.l %0,%0";
       else if (DATA_REG_P (operands[0])
-	       && INTVAL (operands[1]) < 128
-	       && INTVAL (operands[1]) >= -128)
-	return "moveq %1,%0";
+               && INTVAL (operands[1]) < 128
+               && INTVAL (operands[1]) >= -128)
+        return "moveq %1,%0";
       else if (INTVAL (operands[1]) < 0x8000
-	       && INTVAL (operands[1]) >= -0x8000)
-	return "move%.w %1,%0";
+               && INTVAL (operands[1]) >= -0x8000)
+        return "move%.w %1,%0";
     }
   else if (CONSTANT_P (operands[1]))
     return "move%.l %1,%0";
@@ -3149,30 +3381,31 @@ output_move_himode (rtx *operands)
 const char *
 output_move_qimode (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   /* 68k family always modifies the stack pointer by at least 2, even for
      byte pushes.  The 5200 (ColdFire) does not do this.  */
   
   /* This case is generated by pushqi1 pattern now.  */
   gcc_assert (!(GET_CODE (operands[0]) == MEM
-		&& GET_CODE (XEXP (operands[0], 0)) == PRE_DEC
-		&& XEXP (XEXP (operands[0], 0), 0) == stack_pointer_rtx
-		&& ! ADDRESS_REG_P (operands[1])
-		&& ! TARGET_COLDFIRE));
+        	&& GET_CODE (XEXP (operands[0], 0)) == PRE_DEC
+        	&& XEXP (XEXP (operands[0], 0), 0) == stack_pointer_rtx
+        	&& ! ADDRESS_REG_P (operands[1])
+        	&& ! TARGET_COLDFIRE));
 
   /* clr and st insns on 68000 read before writing.  */
   if (!ADDRESS_REG_P (operands[0])
       && ((TARGET_68010 || TARGET_COLDFIRE)
-	  || !(GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0]))))
+          || !(GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0]))))
     {
       if (operands[1] == const0_rtx)
-	return "clr%.b %0";
+        return "clr%.b %0";
       if ((!TARGET_COLDFIRE || DATA_REG_P (operands[0]))
-	  && GET_CODE (operands[1]) == CONST_INT
-	  && (INTVAL (operands[1]) & 255) == 255)
-	{
-	  CC_STATUS_INIT;
-	  return "st %0";
-	}
+          && GET_CODE (operands[1]) == CONST_INT
+          && (INTVAL (operands[1]) & 255) == 255)
+        {
+          CC_STATUS_INIT;
+          return "st %0";
+        }
     }
   if (GET_CODE (operands[1]) == CONST_INT
       && DATA_REG_P (operands[0])
@@ -3193,10 +3426,11 @@ output_move_qimode (rtx *operands)
 const char *
 output_move_stricthi (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (operands[1] == const0_rtx
       /* clr insns on 68000 read before writing.  */
       && ((TARGET_68010 || TARGET_COLDFIRE)
-	  || !(GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0]))))
+          || !(GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0]))))
     return "clr%.w %0";
   return "move%.w %1,%0";
 }
@@ -3204,6 +3438,7 @@ output_move_stricthi (rtx *operands)
 const char *
 output_move_strictqi (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (operands[1] == const0_rtx
       /* clr insns on 68000 read before writing.  */
       && ((TARGET_68010 || TARGET_COLDFIRE)
@@ -3218,6 +3453,7 @@ output_move_strictqi (rtx *operands)
 static const char *
 singlemove_string (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (GET_CODE (operands[1]) == CONST_INT)
     return output_move_simode_const (operands);
   return "move%.l %1,%0";
@@ -3233,10 +3469,11 @@ singlemove_string (rtx *operands)
 
 static void
 handle_move_double (rtx operands[2],
-		    void (*handle_reg_adjust) (rtx, int),
-		    void (*handle_compadr) (rtx [2]),
-		    void (*handle_movsi) (rtx [2]))
+        	    void (*handle_reg_adjust) (rtx, int),
+        	    void (*handle_compadr) (rtx [2]),
+        	    void (*handle_movsi) (rtx [2]))
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   enum
     {
       REGOP, OFFSOP, MEMOP, PUSHOP, POPOP, CNSTOP, RNDOP
@@ -3297,11 +3534,11 @@ handle_move_double (rtx operands[2],
       handle_reg_adjust (operands[0], -size);
 
       if (GET_MODE (operands[1]) == XFmode)
-	operands[0] = gen_rtx_MEM (XFmode, operands[0]);
+        operands[0] = gen_rtx_MEM (XFmode, operands[0]);
       else if (GET_MODE (operands[0]) == DFmode)
-	operands[0] = gen_rtx_MEM (DFmode, operands[0]);
+        operands[0] = gen_rtx_MEM (DFmode, operands[0]);
       else
-	operands[0] = gen_rtx_MEM (DImode, operands[0]);
+        operands[0] = gen_rtx_MEM (DImode, operands[0]);
       optype0 = OFFSOP;
     }
   if (optype0 == POPOP && optype1 == PUSHOP)
@@ -3311,11 +3548,11 @@ handle_move_double (rtx operands[2],
       handle_reg_adjust (operands[1], -size);
 
       if (GET_MODE (operands[1]) == XFmode)
-	operands[1] = gen_rtx_MEM (XFmode, operands[1]);
+        operands[1] = gen_rtx_MEM (XFmode, operands[1]);
       else if (GET_MODE (operands[1]) == DFmode)
-	operands[1] = gen_rtx_MEM (DFmode, operands[1]);
+        operands[1] = gen_rtx_MEM (DFmode, operands[1]);
       else
-	operands[1] = gen_rtx_MEM (DImode, operands[1]);
+        operands[1] = gen_rtx_MEM (DImode, operands[1]);
       optype1 = OFFSOP;
     }
 
@@ -3340,75 +3577,75 @@ handle_move_double (rtx operands[2],
   if (size == 12)
     {
       if (optype0 == REGOP)
-	{
-	  latehalf[0] = gen_rtx_REG (SImode, REGNO (operands[0]) + 2);
-	  middlehalf[0] = gen_rtx_REG (SImode, REGNO (operands[0]) + 1);
-	}
+        {
+          latehalf[0] = gen_rtx_REG (SImode, REGNO (operands[0]) + 2);
+          middlehalf[0] = gen_rtx_REG (SImode, REGNO (operands[0]) + 1);
+        }
       else if (optype0 == OFFSOP)
-	{
-	  middlehalf[0] = adjust_address (operands[0], SImode, 4);
-	  latehalf[0] = adjust_address (operands[0], SImode, size - 4);
-	}
+        {
+          middlehalf[0] = adjust_address (operands[0], SImode, 4);
+          latehalf[0] = adjust_address (operands[0], SImode, size - 4);
+        }
       else
-	{
-	  middlehalf[0] = adjust_address (operands[0], SImode, 0);
-	  latehalf[0] = adjust_address (operands[0], SImode, 0);
-	}
+        {
+          middlehalf[0] = adjust_address (operands[0], SImode, 0);
+          latehalf[0] = adjust_address (operands[0], SImode, 0);
+        }
 
       if (optype1 == REGOP)
-	{
-	  latehalf[1] = gen_rtx_REG (SImode, REGNO (operands[1]) + 2);
-	  middlehalf[1] = gen_rtx_REG (SImode, REGNO (operands[1]) + 1);
-	}
+        {
+          latehalf[1] = gen_rtx_REG (SImode, REGNO (operands[1]) + 2);
+          middlehalf[1] = gen_rtx_REG (SImode, REGNO (operands[1]) + 1);
+        }
       else if (optype1 == OFFSOP)
-	{
-	  middlehalf[1] = adjust_address (operands[1], SImode, 4);
-	  latehalf[1] = adjust_address (operands[1], SImode, size - 4);
-	}
+        {
+          middlehalf[1] = adjust_address (operands[1], SImode, 4);
+          latehalf[1] = adjust_address (operands[1], SImode, size - 4);
+        }
       else if (optype1 == CNSTOP)
-	{
-	  if (GET_CODE (operands[1]) == CONST_DOUBLE)
-	    {
-	      REAL_VALUE_TYPE r;
-	      long l[3];
-
-	      REAL_VALUE_FROM_CONST_DOUBLE (r, operands[1]);
-	      REAL_VALUE_TO_TARGET_LONG_DOUBLE (r, l);
-	      operands[1] = GEN_INT (l[0]);
-	      middlehalf[1] = GEN_INT (l[1]);
-	      latehalf[1] = GEN_INT (l[2]);
-	    }
-	  else
-	    {
-	      /* No non-CONST_DOUBLE constant should ever appear
-		 here.  */
-	      gcc_assert (!CONSTANT_P (operands[1]));
-	    }
-	}
+        {
+          if (GET_CODE (operands[1]) == CONST_DOUBLE)
+            {
+              REAL_VALUE_TYPE r;
+              long l[3];
+
+              REAL_VALUE_FROM_CONST_DOUBLE (r, operands[1]);
+              REAL_VALUE_TO_TARGET_LONG_DOUBLE (r, l);
+              operands[1] = GEN_INT (l[0]);
+              middlehalf[1] = GEN_INT (l[1]);
+              latehalf[1] = GEN_INT (l[2]);
+            }
+          else
+            {
+              /* No non-CONST_DOUBLE constant should ever appear
+        	 here.  */
+              gcc_assert (!CONSTANT_P (operands[1]));
+            }
+        }
       else
-	{
-	  middlehalf[1] = adjust_address (operands[1], SImode, 0);
-	  latehalf[1] = adjust_address (operands[1], SImode, 0);
-	}
+        {
+          middlehalf[1] = adjust_address (operands[1], SImode, 0);
+          latehalf[1] = adjust_address (operands[1], SImode, 0);
+        }
     }
   else
     /* size is not 12: */
     {
       if (optype0 == REGOP)
-	latehalf[0] = gen_rtx_REG (SImode, REGNO (operands[0]) + 1);
+        latehalf[0] = gen_rtx_REG (SImode, REGNO (operands[0]) + 1);
       else if (optype0 == OFFSOP)
-	latehalf[0] = adjust_address (operands[0], SImode, size - 4);
+        latehalf[0] = adjust_address (operands[0], SImode, size - 4);
       else
-	latehalf[0] = adjust_address (operands[0], SImode, 0);
+        latehalf[0] = adjust_address (operands[0], SImode, 0);
 
       if (optype1 == REGOP)
-	latehalf[1] = gen_rtx_REG (SImode, REGNO (operands[1]) + 1);
+        latehalf[1] = gen_rtx_REG (SImode, REGNO (operands[1]) + 1);
       else if (optype1 == OFFSOP)
-	latehalf[1] = adjust_address (operands[1], SImode, size - 4);
+        latehalf[1] = adjust_address (operands[1], SImode, size - 4);
       else if (optype1 == CNSTOP)
-	split_double (operands[1], &operands[1], &latehalf[1]);
+        split_double (operands[1], &operands[1], &latehalf[1]);
       else
-	latehalf[1] = adjust_address (operands[1], SImode, 0);
+        latehalf[1] = adjust_address (operands[1], SImode, 0);
     }
 
   /* If insn is effectively movd N(sp),-(sp) then we will do the
@@ -3430,56 +3667,56 @@ handle_move_double (rtx operands[2],
       rtx testlow = gen_rtx_REG (SImode, REGNO (operands[0]));
 
       if (reg_overlap_mentioned_p (testlow, XEXP (operands[1], 0))
-	  && reg_overlap_mentioned_p (latehalf[0], XEXP (operands[1], 0)))
-	{
-	  /* If both halves of dest are used in the src memory address,
-	     compute the address into latehalf of dest.
-	     Note that this can't happen if the dest is two data regs.  */
-	compadr:
-	  xops[0] = latehalf[0];
-	  xops[1] = XEXP (operands[1], 0);
-
-	  handle_compadr (xops);
-	  if (GET_MODE (operands[1]) == XFmode)
-	    {
-	      operands[1] = gen_rtx_MEM (XFmode, latehalf[0]);
-	      middlehalf[1] = adjust_address (operands[1], DImode, size - 8);
-	      latehalf[1] = adjust_address (operands[1], DImode, size - 4);
-	    }
-	  else
-	    {
-	      operands[1] = gen_rtx_MEM (DImode, latehalf[0]);
-	      latehalf[1] = adjust_address (operands[1], DImode, size - 4);
-	    }
-	}
+          && reg_overlap_mentioned_p (latehalf[0], XEXP (operands[1], 0)))
+        {
+          /* If both halves of dest are used in the src memory address,
+             compute the address into latehalf of dest.
+             Note that this can't happen if the dest is two data regs.  */
+        compadr:
+          xops[0] = latehalf[0];
+          xops[1] = XEXP (operands[1], 0);
+
+          handle_compadr (xops);
+          if (GET_MODE (operands[1]) == XFmode)
+            {
+              operands[1] = gen_rtx_MEM (XFmode, latehalf[0]);
+              middlehalf[1] = adjust_address (operands[1], DImode, size - 8);
+              latehalf[1] = adjust_address (operands[1], DImode, size - 4);
+            }
+          else
+            {
+              operands[1] = gen_rtx_MEM (DImode, latehalf[0]);
+              latehalf[1] = adjust_address (operands[1], DImode, size - 4);
+            }
+        }
       else if (size == 12
-	       && reg_overlap_mentioned_p (middlehalf[0],
-					   XEXP (operands[1], 0)))
-	{
-	  /* Check for two regs used by both source and dest.
-	     Note that this can't happen if the dest is all data regs.
-	     It can happen if the dest is d6, d7, a0.
-	     But in that case, latehalf is an addr reg, so
-	     the code at compadr does ok.  */
-
-	  if (reg_overlap_mentioned_p (testlow, XEXP (operands[1], 0))
-	      || reg_overlap_mentioned_p (latehalf[0], XEXP (operands[1], 0)))
-	    goto compadr;
-
-	  /* JRV says this can't happen: */
-	  gcc_assert (!addreg0 && !addreg1);
-
-	  /* Only the middle reg conflicts; simply put it last.  */
-	  handle_movsi (operands);
-	  handle_movsi (latehalf);
-	  handle_movsi (middlehalf);
+               && reg_overlap_mentioned_p (middlehalf[0],
+        				   XEXP (operands[1], 0)))
+        {
+          /* Check for two regs used by both source and dest.
+             Note that this can't happen if the dest is all data regs.
+             It can happen if the dest is d6, d7, a0.
+             But in that case, latehalf is an addr reg, so
+             the code at compadr does ok.  */
+
+          if (reg_overlap_mentioned_p (testlow, XEXP (operands[1], 0))
+              || reg_overlap_mentioned_p (latehalf[0], XEXP (operands[1], 0)))
+            goto compadr;
+
+          /* JRV says this can't happen: */
+          gcc_assert (!addreg0 && !addreg1);
+
+          /* Only the middle reg conflicts; simply put it last.  */
+          handle_movsi (operands);
+          handle_movsi (latehalf);
+          handle_movsi (middlehalf);
 
-	  return;
-	}
+          return;
+        }
       else if (reg_overlap_mentioned_p (testlow, XEXP (operands[1], 0)))
-	/* If the low half of dest is mentioned in the source memory
-	   address, the arrange to emit the move late half first.  */
-	dest_overlapped_low = 1;
+        /* If the low half of dest is mentioned in the source memory
+           address, the arrange to emit the move late half first.  */
+        dest_overlapped_low = 1;
     }
 
   /* If one or both operands autodecrementing,
@@ -3492,34 +3729,34 @@ handle_move_double (rtx operands[2],
 
   if (optype0 == PUSHOP || optype1 == PUSHOP
       || (optype0 == REGOP && optype1 == REGOP
-	  && ((middlehalf[1] && REGNO (operands[0]) == REGNO (middlehalf[1]))
-	      || REGNO (operands[0]) == REGNO (latehalf[1])))
+          && ((middlehalf[1] && REGNO (operands[0]) == REGNO (middlehalf[1]))
+              || REGNO (operands[0]) == REGNO (latehalf[1])))
       || dest_overlapped_low)
     {
       /* Make any unoffsettable addresses point at high-numbered word.  */
       if (addreg0)
-	handle_reg_adjust (addreg0, size - 4);
+        handle_reg_adjust (addreg0, size - 4);
       if (addreg1)
-	handle_reg_adjust (addreg1, size - 4);
+        handle_reg_adjust (addreg1, size - 4);
 
       /* Do that word.  */
       handle_movsi (latehalf);
 
       /* Undo the adds we just did.  */
       if (addreg0)
-	handle_reg_adjust (addreg0, -4);
+        handle_reg_adjust (addreg0, -4);
       if (addreg1)
-	handle_reg_adjust (addreg1, -4);
+        handle_reg_adjust (addreg1, -4);
 
       if (size == 12)
-	{
-	  handle_movsi (middlehalf);
+        {
+          handle_movsi (middlehalf);
 
-	  if (addreg0)
-	    handle_reg_adjust (addreg0, -4);
-	  if (addreg1)
-	    handle_reg_adjust (addreg1, -4);
-	}
+          if (addreg0)
+            handle_reg_adjust (addreg0, -4);
+          if (addreg1)
+            handle_reg_adjust (addreg1, -4);
+        }
 
       /* Do low-numbered word.  */
 
@@ -3536,9 +3773,9 @@ handle_move_double (rtx operands[2],
   if (size == 12)
     {
       if (addreg0)
-	handle_reg_adjust (addreg0, 4);
+        handle_reg_adjust (addreg0, 4);
       if (addreg1)
-	handle_reg_adjust (addreg1, 4);
+        handle_reg_adjust (addreg1, 4);
 
       m68k_final_prescan_insn (NULL, middlehalf, 2);
       handle_movsi (middlehalf);
@@ -3567,10 +3804,11 @@ handle_move_double (rtx operands[2],
 static void
 output_reg_adjust (rtx reg, int n)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   const char *s;
 
   gcc_assert (GET_MODE (reg) == SImode
-	      && -12 <= n && n != 0 && n <= 12);
+              && -12 <= n && n != 0 && n <= 12);
 
   switch (n)
     {
@@ -3610,10 +3848,11 @@ output_reg_adjust (rtx reg, int n)
 static void
 emit_reg_adjust (rtx reg1, int n)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx reg2;
 
   gcc_assert (GET_MODE (reg1) == SImode
-	      && -12 <= n && n != 0 && n <= 12);
+              && -12 <= n && n != 0 && n <= 12);
 
   reg1 = copy_rtx (reg1);
   reg2 = copy_rtx (reg1);
@@ -3630,6 +3869,7 @@ emit_reg_adjust (rtx reg1, int n)
 static void
 output_compadr (rtx operands[2])
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   output_asm_insn ("lea %a1,%0", operands);
 }
 
@@ -3638,6 +3878,7 @@ output_compadr (rtx operands[2])
 static void
 output_movsi (rtx operands[2])
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   output_asm_insn (singlemove_string (operands), operands);
 }
 
@@ -3645,17 +3886,18 @@ output_movsi (rtx operands[2])
 static rtx
 copy_operand (rtx op, enum machine_mode mode)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   /* ??? This looks really ugly.  There must be a better way
      to change a mode on the operand.  */
   if (GET_MODE (op) != VOIDmode)
     {
       if (REG_P (op))
-	op = gen_rtx_REG (mode, REGNO (op));
+        op = gen_rtx_REG (mode, REGNO (op));
       else
-	{
-	  op = copy_rtx (op);
-	  PUT_MODE (op, mode);
-	}
+        {
+          op = copy_rtx (op);
+          PUT_MODE (op, mode);
+        }
     }
 
   return op;
@@ -3665,6 +3907,7 @@ copy_operand (rtx op, enum machine_mode
 static void
 emit_movsi (rtx operands[2])
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   operands[0] = copy_operand (operands[0], SImode);
   operands[1] = copy_operand (operands[1], SImode);
 
@@ -3676,8 +3919,9 @@ emit_movsi (rtx operands[2])
 const char *
 output_move_double (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   handle_move_double (operands,
-		      output_reg_adjust, output_compadr, output_movsi);
+        	      output_reg_adjust, output_compadr, output_movsi);
 
   return "";
 }
@@ -3687,6 +3931,7 @@ output_move_double (rtx *operands)
 void
 m68k_emit_move_double (rtx operands[2])
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   handle_move_double (operands, emit_reg_adjust, emit_movsi, emit_movsi);
 }
 
@@ -3696,6 +3941,7 @@ m68k_emit_move_double (rtx operands[2])
 static rtx
 force_mode (enum machine_mode mode, rtx orig)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (mode == GET_MODE (orig))
     return orig;
 
@@ -3708,6 +3954,7 @@ force_mode (enum machine_mode mode, rtx
 static int
 fp_reg_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   return reg_renumber && FP_REG_P (op);
 }
 
@@ -3724,6 +3971,7 @@ fp_reg_operand (rtx op, enum machine_mod
 int
 emit_move_sequence (rtx *operands, enum machine_mode mode, rtx scratch_reg)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   register rtx operand0 = operands[0];
   register rtx operand1 = operands[1];
   register rtx tem;
@@ -3733,15 +3981,15 @@ emit_move_sequence (rtx *operands, enum
       && REGNO (operand0) >= FIRST_PSEUDO_REGISTER)
     operand0 = reg_equiv_mem[REGNO (operand0)];
   else if (scratch_reg
-	   && reload_in_progress && GET_CODE (operand0) == SUBREG
-	   && GET_CODE (SUBREG_REG (operand0)) == REG
-	   && REGNO (SUBREG_REG (operand0)) >= FIRST_PSEUDO_REGISTER)
+           && reload_in_progress && GET_CODE (operand0) == SUBREG
+           && GET_CODE (SUBREG_REG (operand0)) == REG
+           && REGNO (SUBREG_REG (operand0)) >= FIRST_PSEUDO_REGISTER)
     {
      /* We must not alter SUBREG_BYTE (operand0) since that would confuse
-	the code which tracks sets/uses for delete_output_reload.  */
+        the code which tracks sets/uses for delete_output_reload.  */
       rtx temp = gen_rtx_SUBREG (GET_MODE (operand0),
-				 reg_equiv_mem [REGNO (SUBREG_REG (operand0))],
-				 SUBREG_BYTE (operand0));
+        			 reg_equiv_mem [REGNO (SUBREG_REG (operand0))],
+        			 SUBREG_BYTE (operand0));
       operand0 = alter_subreg (&temp);
     }
 
@@ -3750,93 +3998,93 @@ emit_move_sequence (rtx *operands, enum
       && REGNO (operand1) >= FIRST_PSEUDO_REGISTER)
     operand1 = reg_equiv_mem[REGNO (operand1)];
   else if (scratch_reg
-	   && reload_in_progress && GET_CODE (operand1) == SUBREG
-	   && GET_CODE (SUBREG_REG (operand1)) == REG
-	   && REGNO (SUBREG_REG (operand1)) >= FIRST_PSEUDO_REGISTER)
+           && reload_in_progress && GET_CODE (operand1) == SUBREG
+           && GET_CODE (SUBREG_REG (operand1)) == REG
+           && REGNO (SUBREG_REG (operand1)) >= FIRST_PSEUDO_REGISTER)
     {
      /* We must not alter SUBREG_BYTE (operand0) since that would confuse
-	the code which tracks sets/uses for delete_output_reload.  */
+        the code which tracks sets/uses for delete_output_reload.  */
       rtx temp = gen_rtx_SUBREG (GET_MODE (operand1),
-				 reg_equiv_mem [REGNO (SUBREG_REG (operand1))],
-				 SUBREG_BYTE (operand1));
+        			 reg_equiv_mem [REGNO (SUBREG_REG (operand1))],
+        			 SUBREG_BYTE (operand1));
       operand1 = alter_subreg (&temp);
     }
 
   if (scratch_reg && reload_in_progress && GET_CODE (operand0) == MEM
       && ((tem = find_replacement (&XEXP (operand0, 0)))
-	  != XEXP (operand0, 0)))
+          != XEXP (operand0, 0)))
     operand0 = gen_rtx_MEM (GET_MODE (operand0), tem);
   if (scratch_reg && reload_in_progress && GET_CODE (operand1) == MEM
       && ((tem = find_replacement (&XEXP (operand1, 0)))
-	  != XEXP (operand1, 0)))
+          != XEXP (operand1, 0)))
     operand1 = gen_rtx_MEM (GET_MODE (operand1), tem);
 
   /* Handle secondary reloads for loads/stores of FP registers where
      the address is symbolic by using the scratch register */
   if (fp_reg_operand (operand0, mode)
       && ((GET_CODE (operand1) == MEM
-	   && ! memory_address_p (DFmode, XEXP (operand1, 0)))
-	  || ((GET_CODE (operand1) == SUBREG
-	       && GET_CODE (XEXP (operand1, 0)) == MEM
-	       && !memory_address_p (DFmode, XEXP (XEXP (operand1, 0), 0)))))
+           && ! memory_address_p (DFmode, XEXP (operand1, 0)))
+          || ((GET_CODE (operand1) == SUBREG
+               && GET_CODE (XEXP (operand1, 0)) == MEM
+               && !memory_address_p (DFmode, XEXP (XEXP (operand1, 0), 0)))))
       && scratch_reg)
     {
       if (GET_CODE (operand1) == SUBREG)
-	operand1 = XEXP (operand1, 0);
+        operand1 = XEXP (operand1, 0);
 
       /* SCRATCH_REG will hold an address.  We want
-	 it in SImode regardless of what mode it was originally given
-	 to us.  */
+         it in SImode regardless of what mode it was originally given
+         to us.  */
       scratch_reg = force_mode (SImode, scratch_reg);
 
       /* D might not fit in 14 bits either; for such cases load D into
-	 scratch reg.  */
+         scratch reg.  */
       if (!memory_address_p (Pmode, XEXP (operand1, 0)))
-	{
-	  emit_move_insn (scratch_reg, XEXP (XEXP (operand1, 0), 1));
-	  emit_move_insn (scratch_reg, gen_rtx_fmt_ee (GET_CODE (XEXP (operand1, 0)),
-						       Pmode,
-						       XEXP (XEXP (operand1, 0), 0),
-						       scratch_reg));
-	}
+        {
+          emit_move_insn (scratch_reg, XEXP (XEXP (operand1, 0), 1));
+          emit_move_insn (scratch_reg, gen_rtx_fmt_ee (GET_CODE (XEXP (operand1, 0)),
+        					       Pmode,
+        					       XEXP (XEXP (operand1, 0), 0),
+        					       scratch_reg));
+        }
       else
-	emit_move_insn (scratch_reg, XEXP (operand1, 0));
+        emit_move_insn (scratch_reg, XEXP (operand1, 0));
       emit_insn (gen_rtx_SET (VOIDmode, operand0,
-			      gen_rtx_MEM (mode, scratch_reg)));
+        		      gen_rtx_MEM (mode, scratch_reg)));
       return 1;
     }
   else if (fp_reg_operand (operand1, mode)
-	   && ((GET_CODE (operand0) == MEM
-		&& ! memory_address_p (DFmode, XEXP (operand0, 0)))
-	       || ((GET_CODE (operand0) == SUBREG)
-		   && GET_CODE (XEXP (operand0, 0)) == MEM
-		   && !memory_address_p (DFmode, XEXP (XEXP (operand0, 0), 0))))
-	   && scratch_reg)
+           && ((GET_CODE (operand0) == MEM
+        	&& ! memory_address_p (DFmode, XEXP (operand0, 0)))
+               || ((GET_CODE (operand0) == SUBREG)
+        	   && GET_CODE (XEXP (operand0, 0)) == MEM
+        	   && !memory_address_p (DFmode, XEXP (XEXP (operand0, 0), 0))))
+           && scratch_reg)
     {
       if (GET_CODE (operand0) == SUBREG)
-	operand0 = XEXP (operand0, 0);
+        operand0 = XEXP (operand0, 0);
 
       /* SCRATCH_REG will hold an address and maybe the actual data.  We want
-	 it in SIMODE regardless of what mode it was originally given
-	 to us.  */
+         it in SIMODE regardless of what mode it was originally given
+         to us.  */
       scratch_reg = force_mode (SImode, scratch_reg);
 
       /* D might not fit in 14 bits either; for such cases load D into
-	 scratch reg.  */
+         scratch reg.  */
       if (!memory_address_p (Pmode, XEXP (operand0, 0)))
-	{
-	  emit_move_insn (scratch_reg, XEXP (XEXP (operand0, 0), 1));
-	  emit_move_insn (scratch_reg, gen_rtx_fmt_ee (GET_CODE (XEXP (operand0,
-								        0)),
-						       Pmode,
-						       XEXP (XEXP (operand0, 0),
-								   0),
-						       scratch_reg));
-	}
+        {
+          emit_move_insn (scratch_reg, XEXP (XEXP (operand0, 0), 1));
+          emit_move_insn (scratch_reg, gen_rtx_fmt_ee (GET_CODE (XEXP (operand0,
+        							        0)),
+        					       Pmode,
+        					       XEXP (XEXP (operand0, 0),
+        							   0),
+        					       scratch_reg));
+        }
       else
-	emit_move_insn (scratch_reg, XEXP (operand0, 0));
+        emit_move_insn (scratch_reg, XEXP (operand0, 0));
       emit_insn (gen_rtx_SET (VOIDmode, gen_rtx_MEM (mode, scratch_reg),
-			      operand1));
+        		      operand1));
       return 1;
     }
   /* Handle secondary reloads for loads of FP registers from constant
@@ -3849,25 +4097,25 @@ emit_move_sequence (rtx *operands, enum
      containing only FP registers.  Doing so unfortunately creates
      more problems than it solves.   Fix this for 2.5.  */
   else if (fp_reg_operand (operand0, mode)
-	   && CONSTANT_P (operand1)
-	   && scratch_reg)
+           && CONSTANT_P (operand1)
+           && scratch_reg)
     {
       rtx xoperands[2];
 
       /* SCRATCH_REG will hold an address and maybe the actual data.  We want
-	 it in SIMODE regardless of what mode it was originally given
-	 to us.  */
+         it in SIMODE regardless of what mode it was originally given
+         to us.  */
       scratch_reg = force_mode (SImode, scratch_reg);
 
       /* Force the constant into memory and put the address of the
-	 memory location into scratch_reg.  */
+         memory location into scratch_reg.  */
       xoperands[0] = scratch_reg;
       xoperands[1] = XEXP (force_const_mem (mode, operand1), 0);
       emit_insn (gen_rtx_SET (mode, scratch_reg, xoperands[1]));
 
       /* Now load the destination register.  */
       emit_insn (gen_rtx_SET (mode, operand0,
-			      gen_rtx_MEM (mode, scratch_reg)));
+        		      gen_rtx_MEM (mode, scratch_reg)));
       return 1;
     }
 
@@ -3884,26 +4132,27 @@ emit_move_sequence (rtx *operands, enum
 void
 split_di (rtx operands[], int num, rtx lo_half[], rtx hi_half[])
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   while (num--)
     {
       rtx op = operands[num];
 
       /* simplify_subreg refuses to split volatile memory addresses,
-	 but we still have to handle it.  */
+         but we still have to handle it.  */
       if (GET_CODE (op) == MEM)
-	{
-	  lo_half[num] = adjust_address (op, SImode, 4);
-	  hi_half[num] = adjust_address (op, SImode, 0);
-	}
+        {
+          lo_half[num] = adjust_address (op, SImode, 4);
+          hi_half[num] = adjust_address (op, SImode, 0);
+        }
       else
-	{
-	  lo_half[num] = simplify_gen_subreg (SImode, op,
-					      GET_MODE (op) == VOIDmode
-					      ? DImode : GET_MODE (op), 4);
-	  hi_half[num] = simplify_gen_subreg (SImode, op,
-					      GET_MODE (op) == VOIDmode
-					      ? DImode : GET_MODE (op), 0);
-	}
+        {
+          lo_half[num] = simplify_gen_subreg (SImode, op,
+        				      GET_MODE (op) == VOIDmode
+        				      ? DImode : GET_MODE (op), 4);
+          hi_half[num] = simplify_gen_subreg (SImode, op,
+        				      GET_MODE (op) == VOIDmode
+        				      ? DImode : GET_MODE (op), 0);
+        }
     }
 }
 
@@ -3913,6 +4162,7 @@ split_di (rtx operands[], int num, rtx l
 static void
 m68k_split_offset (rtx x, rtx *base, HOST_WIDE_INT *offset)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   *offset = 0;
   if (GET_CODE (x) == PLUS && GET_CODE (XEXP (x, 1)) == CONST_INT)
     {
@@ -3935,8 +4185,9 @@ m68k_split_offset (rtx x, rtx *base, HOS
 
 bool
 m68k_movem_pattern_p (rtx pattern, rtx automod_base,
-		      HOST_WIDE_INT automod_offset, bool store_p)
+        	      HOST_WIDE_INT automod_offset, bool store_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx base, mem_base, set, mem, reg, last_reg;
   HOST_WIDE_INT offset, mem_offset;
   int i, first, len;
@@ -3949,7 +4200,7 @@ m68k_movem_pattern_p (rtx pattern, rtx a
     {
       /* Stores must be pre-decrement and loads must be post-increment.  */
       if (store_p != (automod_offset < 0))
-	return false;
+        return false;
 
       /* Work out the base and offset for lowest memory location.  */
       base = automod_base;
@@ -3969,56 +4220,56 @@ m68k_movem_pattern_p (rtx pattern, rtx a
       /* We need a plain SET.  */
       set = XVECEXP (pattern, 0, i);
       if (GET_CODE (set) != SET)
-	return false;
+        return false;
 
       /* Check that we have a memory location...  */
       mem = XEXP (set, !store_p);
       if (!MEM_P (mem) || !memory_operand (mem, VOIDmode))
-	return false;
+        return false;
 
       /* ...with the right address.  */
       if (base == NULL)
-	{
-	  m68k_split_offset (XEXP (mem, 0), &base, &offset);
-	  /* The ColdFire instruction only allows (An) and (d16,An) modes.
-	     There are no mode restrictions for 680x0 besides the
-	     automodification rules enforced above.  */
-	  if (TARGET_COLDFIRE
-	      && !m68k_legitimate_base_reg_p (base, reload_completed))
-	    return false;
-	}
+        {
+          m68k_split_offset (XEXP (mem, 0), &base, &offset);
+          /* The ColdFire instruction only allows (An) and (d16,An) modes.
+             There are no mode restrictions for 680x0 besides the
+             automodification rules enforced above.  */
+          if (TARGET_COLDFIRE
+              && !m68k_legitimate_base_reg_p (base, reload_completed))
+            return false;
+        }
       else
-	{
-	  m68k_split_offset (XEXP (mem, 0), &mem_base, &mem_offset);
-	  if (!rtx_equal_p (base, mem_base) || offset != mem_offset)
-	    return false;
-	}
+        {
+          m68k_split_offset (XEXP (mem, 0), &mem_base, &mem_offset);
+          if (!rtx_equal_p (base, mem_base) || offset != mem_offset)
+            return false;
+        }
 
       /* Check that we have a register of the required mode and class.  */
       reg = XEXP (set, store_p);
       if (!REG_P (reg)
-	  || !HARD_REGISTER_P (reg)
-	  || GET_MODE (reg) != reg_raw_mode[REGNO (reg)])
-	return false;
+          || !HARD_REGISTER_P (reg)
+          || GET_MODE (reg) != reg_raw_mode[REGNO (reg)])
+        return false;
 
       if (last_reg)
-	{
-	  /* The register must belong to RCLASS and have a higher number
-	     than the register in the previous SET.  */
-	  if (!TEST_HARD_REG_BIT (reg_class_contents[rclass], REGNO (reg))
-	      || REGNO (last_reg) >= REGNO (reg))
-	    return false;
-	}
+        {
+          /* The register must belong to RCLASS and have a higher number
+             than the register in the previous SET.  */
+          if (!TEST_HARD_REG_BIT (reg_class_contents[rclass], REGNO (reg))
+              || REGNO (last_reg) >= REGNO (reg))
+            return false;
+        }
       else
-	{
-	  /* Work out which register class we need.  */
-	  if (INT_REGNO_P (REGNO (reg)))
-	    rclass = GENERAL_REGS;
-	  else if (FP_REGNO_P (REGNO (reg)))
-	    rclass = FP_REGS;
-	  else
-	    return false;
-	}
+        {
+          /* Work out which register class we need.  */
+          if (INT_REGNO_P (REGNO (reg)))
+            rclass = GENERAL_REGS;
+          else if (FP_REGNO_P (REGNO (reg)))
+            rclass = FP_REGS;
+          else
+            return false;
+        }
 
       last_reg = reg;
       offset += GET_MODE_SIZE (GET_MODE (reg));
@@ -4045,8 +4296,9 @@ m68k_movem_pattern_p (rtx pattern, rtx a
 
 const char *
 m68k_output_movem (rtx *operands, rtx pattern,
-		   HOST_WIDE_INT automod_offset, bool store_p)
+        	   HOST_WIDE_INT automod_offset, bool store_p)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   unsigned int mask;
   int i, first;
 
@@ -4056,29 +4308,29 @@ m68k_output_movem (rtx *operands, rtx pa
   for (i = first; i < XVECLEN (pattern, 0); i++)
     {
       /* When using movem with pre-decrement addressing, register X + D0_REG
-	 is controlled by bit 15 - X.  For all other addressing modes,
-	 register X + D0_REG is controlled by bit X.  Confusingly, the
-	 register mask for fmovem is in the opposite order to that for
-	 movem.  */
+         is controlled by bit 15 - X.  For all other addressing modes,
+         register X + D0_REG is controlled by bit X.  Confusingly, the
+         register mask for fmovem is in the opposite order to that for
+         movem.  */
       unsigned int regno;
 
       gcc_assert (MEM_P (XEXP (XVECEXP (pattern, 0, i), !store_p)));
       gcc_assert (REG_P (XEXP (XVECEXP (pattern, 0, i), store_p)));
       regno = REGNO (XEXP (XVECEXP (pattern, 0, i), store_p));
       if (automod_offset < 0)
-	{
-	  if (FP_REGNO_P (regno))
-	    mask |= 1 << (regno - FP0_REG);
-	  else
-	    mask |= 1 << (15 - (regno - D0_REG));
-	}
+        {
+          if (FP_REGNO_P (regno))
+            mask |= 1 << (regno - FP0_REG);
+          else
+            mask |= 1 << (15 - (regno - D0_REG));
+        }
       else
-	{
-	  if (FP_REGNO_P (regno))
-	    mask |= 1 << (7 - (regno - FP0_REG));
-	  else
-	    mask |= 1 << (regno - D0_REG);
-	}
+        {
+          if (FP_REGNO_P (regno))
+            mask |= 1 << (7 - (regno - FP0_REG));
+          else
+            mask |= 1 << (regno - D0_REG);
+        }
     }
   CC_STATUS_INIT;
 
@@ -4092,16 +4344,16 @@ m68k_output_movem (rtx *operands, rtx pa
   if (FP_REGNO_P (REGNO (XEXP (XVECEXP (pattern, 0, first), store_p))))
     {
       if (store_p)
-	return "fmovem %1,%a0";
+        return "fmovem %1,%a0";
       else
-	return "fmovem %a0,%1";
+        return "fmovem %a0,%1";
     }
   else
     {
       if (store_p)
-	return "movem%.l %1,%a0";
+        return "movem%.l %1,%a0";
       else
-	return "movem%.l %a0,%1";
+        return "movem%.l %a0,%1";
     }
 }
 
@@ -4111,18 +4363,19 @@ m68k_output_movem (rtx *operands, rtx pa
 static rtx
 find_addr_reg (rtx addr)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   while (GET_CODE (addr) == PLUS)
     {
       if (GET_CODE (XEXP (addr, 0)) == REG)
-	addr = XEXP (addr, 0);
+        addr = XEXP (addr, 0);
       else if (GET_CODE (XEXP (addr, 1)) == REG)
-	addr = XEXP (addr, 1);
+        addr = XEXP (addr, 1);
       else if (CONSTANT_P (XEXP (addr, 0)))
-	addr = XEXP (addr, 1);
+        addr = XEXP (addr, 1);
       else if (CONSTANT_P (XEXP (addr, 1)))
-	addr = XEXP (addr, 0);
+        addr = XEXP (addr, 0);
       else
-	gcc_unreachable ();
+        gcc_unreachable ();
     }
   gcc_assert (GET_CODE (addr) == REG);
   return addr;
@@ -4133,63 +4386,64 @@ find_addr_reg (rtx addr)
 const char *
 output_addsi3 (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (! operands_match_p (operands[0], operands[1]))
     {
       if (!ADDRESS_REG_P (operands[1]))
-	{
-	  rtx tmp = operands[1];
+        {
+          rtx tmp = operands[1];
 
-	  operands[1] = operands[2];
-	  operands[2] = tmp;
-	}
+          operands[1] = operands[2];
+          operands[2] = tmp;
+        }
 
       /* These insns can result from reloads to access
-	 stack slots over 64k from the frame pointer.  */
+         stack slots over 64k from the frame pointer.  */
       if (GET_CODE (operands[2]) == CONST_INT
-	  && (INTVAL (operands[2]) < -32768 || INTVAL (operands[2]) > 32767))
+          && (INTVAL (operands[2]) < -32768 || INTVAL (operands[2]) > 32767))
         return "move%.l %2,%0\n\tadd%.l %1,%0";
       if (GET_CODE (operands[2]) == REG)
-	return MOTOROLA ? "lea (%1,%2.l),%0" : "lea %1@(0,%2:l),%0";
+        return MOTOROLA ? "lea (%1,%2.l),%0" : "lea %1@(0,%2:l),%0";
       return MOTOROLA ? "lea (%c2,%1),%0" : "lea %1@(%c2),%0";
     }
   if (GET_CODE (operands[2]) == CONST_INT)
     {
       if (INTVAL (operands[2]) > 0
-	  && INTVAL (operands[2]) <= 8)
-	return "addq%.l %2,%0";
+          && INTVAL (operands[2]) <= 8)
+        return "addq%.l %2,%0";
       if (INTVAL (operands[2]) < 0
-	  && INTVAL (operands[2]) >= -8)
+          && INTVAL (operands[2]) >= -8)
         {
-	  operands[2] = GEN_INT (- INTVAL (operands[2]));
-	  return "subq%.l %2,%0";
-	}
+          operands[2] = GEN_INT (- INTVAL (operands[2]));
+          return "subq%.l %2,%0";
+        }
       /* On the CPU32 it is faster to use two addql instructions to
-	 add a small integer (8 < N <= 16) to a register.
-	 Likewise for subql.  */
+         add a small integer (8 < N <= 16) to a register.
+         Likewise for subql.  */
       if (TUNE_CPU32 && REG_P (operands[0]))
-	{
-	  if (INTVAL (operands[2]) > 8
-	      && INTVAL (operands[2]) <= 16)
-	    {
-	      operands[2] = GEN_INT (INTVAL (operands[2]) - 8);
-	      return "addq%.l #8,%0\n\taddq%.l %2,%0";
-	    }
-	  if (INTVAL (operands[2]) < -8
-	      && INTVAL (operands[2]) >= -16)
-	    {
-	      operands[2] = GEN_INT (- INTVAL (operands[2]) - 8);
-	      return "subq%.l #8,%0\n\tsubq%.l %2,%0";
-	    }
-	}
+        {
+          if (INTVAL (operands[2]) > 8
+              && INTVAL (operands[2]) <= 16)
+            {
+              operands[2] = GEN_INT (INTVAL (operands[2]) - 8);
+              return "addq%.l #8,%0\n\taddq%.l %2,%0";
+            }
+          if (INTVAL (operands[2]) < -8
+              && INTVAL (operands[2]) >= -16)
+            {
+              operands[2] = GEN_INT (- INTVAL (operands[2]) - 8);
+              return "subq%.l #8,%0\n\tsubq%.l %2,%0";
+            }
+        }
       if (ADDRESS_REG_P (operands[0])
-	  && INTVAL (operands[2]) >= -0x8000
-	  && INTVAL (operands[2]) < 0x8000)
-	{
-	  if (TUNE_68040)
-	    return "add%.w %2,%0";
-	  else
-	    return MOTOROLA ? "lea (%c2,%0),%0" : "lea %0@(%c2),%0";
-	}
+          && INTVAL (operands[2]) >= -0x8000
+          && INTVAL (operands[2]) < 0x8000)
+        {
+          if (TUNE_68040)
+            return "add%.w %2,%0";
+          else
+            return MOTOROLA ? "lea (%c2,%0),%0" : "lea %0@(%c2),%0";
+        }
     }
   return "add%.l %2,%0";
 }
@@ -4206,59 +4460,60 @@ output_addsi3 (rtx *operands)
 void
 notice_update_cc (rtx exp, rtx insn)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (GET_CODE (exp) == SET)
     {
       if (GET_CODE (SET_SRC (exp)) == CALL)
-	CC_STATUS_INIT; 
+        CC_STATUS_INIT; 
       else if (ADDRESS_REG_P (SET_DEST (exp)))
-	{
-	  if (cc_status.value1 && modified_in_p (cc_status.value1, insn))
-	    cc_status.value1 = 0;
-	  if (cc_status.value2 && modified_in_p (cc_status.value2, insn))
-	    cc_status.value2 = 0; 
-	}
+        {
+          if (cc_status.value1 && modified_in_p (cc_status.value1, insn))
+            cc_status.value1 = 0;
+          if (cc_status.value2 && modified_in_p (cc_status.value2, insn))
+            cc_status.value2 = 0; 
+        }
       /* fmoves to memory or data registers do not set the condition
-	 codes.  Normal moves _do_ set the condition codes, but not in
-	 a way that is appropriate for comparison with 0, because -0.0
-	 would be treated as a negative nonzero number.  Note that it
-	 isn't appropriate to conditionalize this restriction on
-	 HONOR_SIGNED_ZEROS because that macro merely indicates whether
-	 we care about the difference between -0.0 and +0.0.  */
+         codes.  Normal moves _do_ set the condition codes, but not in
+         a way that is appropriate for comparison with 0, because -0.0
+         would be treated as a negative nonzero number.  Note that it
+         isn't appropriate to conditionalize this restriction on
+         HONOR_SIGNED_ZEROS because that macro merely indicates whether
+         we care about the difference between -0.0 and +0.0.  */
       else if (!FP_REG_P (SET_DEST (exp))
-	       && SET_DEST (exp) != cc0_rtx
-	       && (FP_REG_P (SET_SRC (exp))
-		   || GET_CODE (SET_SRC (exp)) == FIX
-		   || FLOAT_MODE_P (GET_MODE (SET_DEST (exp)))))
-	CC_STATUS_INIT; 
+               && SET_DEST (exp) != cc0_rtx
+               && (FP_REG_P (SET_SRC (exp))
+        	   || GET_CODE (SET_SRC (exp)) == FIX
+        	   || FLOAT_MODE_P (GET_MODE (SET_DEST (exp)))))
+        CC_STATUS_INIT; 
       /* A pair of move insns doesn't produce a useful overall cc.  */
       else if (!FP_REG_P (SET_DEST (exp))
-	       && !FP_REG_P (SET_SRC (exp))
-	       && GET_MODE_SIZE (GET_MODE (SET_SRC (exp))) > 4
-	       && (GET_CODE (SET_SRC (exp)) == REG
-		   || GET_CODE (SET_SRC (exp)) == MEM
-		   || GET_CODE (SET_SRC (exp)) == CONST_DOUBLE))
-	CC_STATUS_INIT; 
+               && !FP_REG_P (SET_SRC (exp))
+               && GET_MODE_SIZE (GET_MODE (SET_SRC (exp))) > 4
+               && (GET_CODE (SET_SRC (exp)) == REG
+        	   || GET_CODE (SET_SRC (exp)) == MEM
+        	   || GET_CODE (SET_SRC (exp)) == CONST_DOUBLE))
+        CC_STATUS_INIT; 
       else if (SET_DEST (exp) != pc_rtx)
-	{
-	  cc_status.flags = 0;
-	  cc_status.value1 = SET_DEST (exp);
-	  cc_status.value2 = SET_SRC (exp);
-	}
+        {
+          cc_status.flags = 0;
+          cc_status.value1 = SET_DEST (exp);
+          cc_status.value2 = SET_SRC (exp);
+        }
     }
   else if (GET_CODE (exp) == PARALLEL
-	   && GET_CODE (XVECEXP (exp, 0, 0)) == SET)
+           && GET_CODE (XVECEXP (exp, 0, 0)) == SET)
     {
       rtx dest = SET_DEST (XVECEXP (exp, 0, 0));
       rtx src  = SET_SRC  (XVECEXP (exp, 0, 0));
 
       if (ADDRESS_REG_P (dest))
-	CC_STATUS_INIT;
+        CC_STATUS_INIT;
       else if (dest != pc_rtx)
-	{
-	  cc_status.flags = 0;
-	  cc_status.value1 = dest;
-	  cc_status.value2 = src;
-	}
+        {
+          cc_status.flags = 0;
+          cc_status.value1 = dest;
+          cc_status.value2 = src;
+        }
     }
   else
     CC_STATUS_INIT;
@@ -4271,25 +4526,25 @@ notice_update_cc (rtx exp, rtx insn)
       {
       case ASHIFT: case ASHIFTRT: case LSHIFTRT:
       case ROTATE: case ROTATERT:
-	/* These instructions always clear the overflow bit, and set
-	   the carry to the bit shifted out.  */
-	cc_status.flags |= CC_OVERFLOW_UNUSABLE | CC_NO_CARRY;
-	break;
+        /* These instructions always clear the overflow bit, and set
+           the carry to the bit shifted out.  */
+        cc_status.flags |= CC_OVERFLOW_UNUSABLE | CC_NO_CARRY;
+        break;
 
       case PLUS: case MINUS: case MULT:
       case DIV: case UDIV: case MOD: case UMOD: case NEG:
-	if (GET_MODE (cc_status.value2) != VOIDmode)
-	  cc_status.flags |= CC_NO_OVERFLOW;
-	break;
+        if (GET_MODE (cc_status.value2) != VOIDmode)
+          cc_status.flags |= CC_NO_OVERFLOW;
+        break;
       case ZERO_EXTEND:
-	/* (SET r1 (ZERO_EXTEND r2)) on this machine
-	   ends with a move insn moving r2 in r2's mode.
-	   Thus, the cc's are set for r2.
-	   This can set N bit spuriously.  */
-	cc_status.flags |= CC_NOT_NEGATIVE; 
+        /* (SET r1 (ZERO_EXTEND r2)) on this machine
+           ends with a move insn moving r2 in r2's mode.
+           Thus, the cc's are set for r2.
+           This can set N bit spuriously.  */
+        cc_status.flags |= CC_NOT_NEGATIVE; 
 
       default:
-	break;
+        break;
       }
   if (cc_status.value1 && GET_CODE (cc_status.value1) == REG
       && cc_status.value2
@@ -4303,13 +4558,14 @@ notice_update_cc (rtx exp, rtx insn)
     {
       cc_status.flags = CC_IN_68881;
       if (!FP_REG_P (XEXP (cc_status.value2, 0)))
-	cc_status.flags |= CC_REVERSED;
+        cc_status.flags |= CC_REVERSED;
     }
 }
 
 const char *
 output_move_const_double (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int code = standard_68881_constant_p (operands[1]);
 
   if (code != 0)
@@ -4325,6 +4581,7 @@ output_move_const_double (rtx *operands)
 const char *
 output_move_const_single (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int code = standard_68881_constant_p (operands[1]);
 
   if (code != 0)
@@ -4374,6 +4631,7 @@ REAL_VALUE_TYPE values_68881[7];
 void
 init_68881_table (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int i;
   REAL_VALUE_TYPE r;
   enum machine_mode mode;
@@ -4392,6 +4650,7 @@ init_68881_table (void)
 int
 standard_68881_constant_p (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   REAL_VALUE_TYPE r;
   int i;
 
@@ -4430,6 +4689,7 @@ standard_68881_constant_p (rtx x)
 int
 floating_exact_log2 (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   REAL_VALUE_TYPE r, r1;
   int exp;
 
@@ -4494,10 +4754,11 @@ floating_exact_log2 (rtx x)
 void
 print_operand (FILE *file, rtx op, int letter)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (letter == '.')
     {
       if (MOTOROLA)
-	fprintf (file, ".");
+        fprintf (file, ".");
     }
   else if (letter == '#')
     asm_fprintf (file, "%I");
@@ -4512,12 +4773,12 @@ print_operand (FILE *file, rtx op, int l
   else if (letter == '$')
     {
       if (TARGET_68040)
-	fprintf (file, "s");
+        fprintf (file, "s");
     }
   else if (letter == '&')
     {
       if (TARGET_68040)
-	fprintf (file, "d");
+        fprintf (file, "d");
     }
   else if (letter == '/')
     asm_fprintf (file, "%R");
@@ -4527,26 +4788,26 @@ print_operand (FILE *file, rtx op, int l
     {
       output_addr_const (file, op);
       if (!(GET_CODE (op) == SYMBOL_REF && SYMBOL_REF_LOCAL_P (op)))
-	fprintf (file, "@PLTPC");
+        fprintf (file, "@PLTPC");
     }
   else if (GET_CODE (op) == REG)
     {
       if (letter == 'R')
-	/* Print out the second register name of a register pair.
-	   I.e., R (6) => 7.  */
-	fputs (M68K_REGNAME(REGNO (op) + 1), file);
+        /* Print out the second register name of a register pair.
+           I.e., R (6) => 7.  */
+        fputs (M68K_REGNAME(REGNO (op) + 1), file);
       else
-	fputs (M68K_REGNAME(REGNO (op)), file);
+        fputs (M68K_REGNAME(REGNO (op)), file);
     }
   else if (GET_CODE (op) == MEM)
     {
       output_address (XEXP (op, 0));
       if (letter == 'd' && ! TARGET_68020
-	  && CONSTANT_ADDRESS_P (XEXP (op, 0))
-	  && !(GET_CODE (XEXP (op, 0)) == CONST_INT
-	       && INTVAL (XEXP (op, 0)) < 0x8000
-	       && INTVAL (XEXP (op, 0)) >= -0x8000))
-	fprintf (file, MOTOROLA ? ".l" : ":l");
+          && CONSTANT_ADDRESS_P (XEXP (op, 0))
+          && !(GET_CODE (XEXP (op, 0)) == CONST_INT
+               && INTVAL (XEXP (op, 0)) < 0x8000
+               && INTVAL (XEXP (op, 0)) >= -0x8000))
+        fprintf (file, MOTOROLA ? ".l" : ":l");
     }
   else if (GET_CODE (op) == CONST_DOUBLE && GET_MODE (op) == SFmode)
     {
@@ -4563,7 +4824,7 @@ print_operand (FILE *file, rtx op, int l
       REAL_VALUE_FROM_CONST_DOUBLE (r, op);
       REAL_VALUE_TO_TARGET_LONG_DOUBLE (r, l);
       asm_fprintf (file, "%I0x%lx%08lx%08lx", l[0] & 0xFFFFFFFF,
-		   l[1] & 0xFFFFFFFF, l[2] & 0xFFFFFFFF);
+        	   l[1] & 0xFFFFFFFF, l[2] & 0xFFFFFFFF);
     }
   else if (GET_CODE (op) == CONST_DOUBLE && GET_MODE (op) == DFmode)
     {
@@ -4576,13 +4837,13 @@ print_operand (FILE *file, rtx op, int l
   else
     {
       /* Use `print_operand_address' instead of `output_addr_const'
-	 to ensure that we print relevant PIC stuff.  */
+         to ensure that we print relevant PIC stuff.  */
       asm_fprintf (file, "%I");
       if (TARGET_PCREL
-	  && (GET_CODE (op) == SYMBOL_REF || GET_CODE (op) == CONST))
-	print_operand_address (file, op);
+          && (GET_CODE (op) == SYMBOL_REF || GET_CODE (op) == CONST))
+        print_operand_address (file, op);
       else
-	output_addr_const (file, op);
+        output_addr_const (file, op);
     }
 }
 
@@ -4591,6 +4852,7 @@ print_operand (FILE *file, rtx op, int l
 static const char *
 m68k_get_reloc_decoration (enum m68k_reloc reloc)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   /* To my knowledge, !MOTOROLA assemblers don't support TLS.  */
   gcc_assert (MOTOROLA || reloc == RELOC_GOT);
 
@@ -4598,27 +4860,27 @@ m68k_get_reloc_decoration (enum m68k_rel
     {
     case RELOC_GOT:
       if (MOTOROLA)
-	{
-	  if (flag_pic == 1 && TARGET_68020)
-	    return "@GOT.w";
-	  else
-	    return "@GOT";
-	}
+        {
+          if (flag_pic == 1 && TARGET_68020)
+            return "@GOT.w";
+          else
+            return "@GOT";
+        }
       else
-	{
-	  if (TARGET_68020)
-	    {
-	      switch (flag_pic)
-		{
-		case 1:
-		  return ":w";
-		case 2:
-		  return ":l";
-		default:
-		  return "";
-		}
-	    }
-	}
+        {
+          if (TARGET_68020)
+            {
+              switch (flag_pic)
+        	{
+        	case 1:
+        	  return ":w";
+        	case 2:
+        	  return ":l";
+        	default:
+        	  return "";
+        	}
+            }
+        }
 
     case RELOC_TLSGD:
       return "@TLSGD";
@@ -4645,20 +4907,21 @@ m68k_get_reloc_decoration (enum m68k_rel
 bool
 m68k_output_addr_const_extra (FILE *file, rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (GET_CODE (x) == UNSPEC)
     {
       switch (XINT (x, 1))
-	{
-	case UNSPEC_RELOC16:
-	case UNSPEC_RELOC32:
-	  output_addr_const (file, XVECEXP (x, 0, 0));
-	  fputs (m68k_get_reloc_decoration
-		 ((enum m68k_reloc) INTVAL (XVECEXP (x, 0, 1))), file);
-	  return true;
+        {
+        case UNSPEC_RELOC16:
+        case UNSPEC_RELOC32:
+          output_addr_const (file, XVECEXP (x, 0, 0));
+          fputs (m68k_get_reloc_decoration
+        	 ((enum m68k_reloc) INTVAL (XVECEXP (x, 0, 1))), file);
+          return true;
 
-	default:
-	  break;
-	}
+        default:
+          break;
+        }
     }
 
   return false;
@@ -4669,6 +4932,7 @@ m68k_output_addr_const_extra (FILE *file
 static void
 m68k_output_dwarf_dtprel (FILE *file, int size, rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   gcc_assert (size == 4);
   fputs ("\t.long\t", file);
   output_addr_const (file, x);
@@ -4682,6 +4946,7 @@ m68k_output_dwarf_dtprel (FILE *file, in
 static rtx
 m68k_delegitimize_address (rtx orig_x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx x;
   struct m68k_address addr;
   rtx unspec;
@@ -4704,7 +4969,7 @@ m68k_delegitimize_address (rtx orig_x)
     unspec = XEXP (unspec, 0);
   if (GET_CODE (unspec) != UNSPEC 
       || (XINT (unspec, 1) != UNSPEC_RELOC16
-	  && XINT (unspec, 1) != UNSPEC_RELOC32))
+          && XINT (unspec, 1) != UNSPEC_RELOC32))
     return orig_x;
   x = XVECEXP (unspec, 0, 0);
   gcc_assert (GET_CODE (x) == SYMBOL_REF || GET_CODE (x) == LABEL_REF);
@@ -4714,7 +4979,7 @@ m68k_delegitimize_address (rtx orig_x)
     {
       rtx idx = addr.index;
       if (addr.scale != 1)
-	idx = gen_rtx_MULT (Pmode, idx, GEN_INT (addr.scale));
+        idx = gen_rtx_MULT (Pmode, idx, GEN_INT (addr.scale));
       x = gen_rtx_PLUS (Pmode, idx, x);
     }
   if (addr.base)
@@ -4745,6 +5010,7 @@ m68k_delegitimize_address (rtx orig_x)
 void
 print_operand_address (FILE *file, rtx addr)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   struct m68k_address address;
 
   if (!m68k_decompose_address (QImode, addr, true, &address))
@@ -4752,113 +5018,113 @@ print_operand_address (FILE *file, rtx a
 
   if (address.code == PRE_DEC)
     fprintf (file, MOTOROLA ? "-(%s)" : "%s@-",
-	     M68K_REGNAME (REGNO (address.base)));
+             M68K_REGNAME (REGNO (address.base)));
   else if (address.code == POST_INC)
     fprintf (file, MOTOROLA ? "(%s)+" : "%s@+",
-	     M68K_REGNAME (REGNO (address.base)));
+             M68K_REGNAME (REGNO (address.base)));
   else if (!address.base && !address.index)
     {
       /* A constant address.  */
       gcc_assert (address.offset == addr);
       if (GET_CODE (addr) == CONST_INT)
-	{
-	  /* (xxx).w or (xxx).l.  */
-	  if (IN_RANGE (INTVAL (addr), -0x8000, 0x7fff))
-	    fprintf (file, MOTOROLA ? "%d.w" : "%d:w", (int) INTVAL (addr));
-	  else
-	    fprintf (file, HOST_WIDE_INT_PRINT_DEC, INTVAL (addr));
-	}
+        {
+          /* (xxx).w or (xxx).l.  */
+          if (IN_RANGE (INTVAL (addr), -0x8000, 0x7fff))
+            fprintf (file, MOTOROLA ? "%d.w" : "%d:w", (int) INTVAL (addr));
+          else
+            fprintf (file, HOST_WIDE_INT_PRINT_DEC, INTVAL (addr));
+        }
       else if (TARGET_PCREL)
-	{
-	  /* (d16,PC) or (bd,PC,Xn) (with suppressed index register).  */
-	  fputc ('(', file);
-	  output_addr_const (file, addr);
-	  asm_fprintf (file, flag_pic == 1 ? ":w,%Rpc)" : ":l,%Rpc)");
-	}
+        {
+          /* (d16,PC) or (bd,PC,Xn) (with suppressed index register).  */
+          fputc ('(', file);
+          output_addr_const (file, addr);
+          asm_fprintf (file, flag_pic == 1 ? ":w,%Rpc)" : ":l,%Rpc)");
+        }
       else
-	{
-	  /* (xxx).l.  We need a special case for SYMBOL_REF if the symbol
-	     name ends in `.<letter>', as the last 2 characters can be
-	     mistaken as a size suffix.  Put the name in parentheses.  */
-	  if (GET_CODE (addr) == SYMBOL_REF
-	      && strlen (XSTR (addr, 0)) > 2
-	      && XSTR (addr, 0)[strlen (XSTR (addr, 0)) - 2] == '.')
-	    {
-	      putc ('(', file);
-	      output_addr_const (file, addr);
-	      putc (')', file);
-	    }
-	  else
-	    output_addr_const (file, addr);
-	}
+        {
+          /* (xxx).l.  We need a special case for SYMBOL_REF if the symbol
+             name ends in `.<letter>', as the last 2 characters can be
+             mistaken as a size suffix.  Put the name in parentheses.  */
+          if (GET_CODE (addr) == SYMBOL_REF
+              && strlen (XSTR (addr, 0)) > 2
+              && XSTR (addr, 0)[strlen (XSTR (addr, 0)) - 2] == '.')
+            {
+              putc ('(', file);
+              output_addr_const (file, addr);
+              putc (')', file);
+            }
+          else
+            output_addr_const (file, addr);
+        }
     }
   else
     {
       int labelno;
 
       /* If ADDR is a (d8,pc,Xn) address, this is the number of the
-	 label being accessed, otherwise it is -1.  */
+         label being accessed, otherwise it is -1.  */
       labelno = (address.offset
-		 && !address.base
-		 && GET_CODE (address.offset) == LABEL_REF
-		 ? CODE_LABEL_NUMBER (XEXP (address.offset, 0))
-		 : -1);
+        	 && !address.base
+        	 && GET_CODE (address.offset) == LABEL_REF
+        	 ? CODE_LABEL_NUMBER (XEXP (address.offset, 0))
+        	 : -1);
       if (MOTOROLA)
-	{
-	  /* Print the "offset(base" component.  */
-	  if (labelno >= 0)
-	    asm_fprintf (file, "%LL%d(%Rpc,", labelno);
-	  else
-	    {
-	      if (address.offset)
-		output_addr_const (file, address.offset);
-
-	      putc ('(', file);
-	      if (address.base)
-		fputs (M68K_REGNAME (REGNO (address.base)), file);
-	    }
-	  /* Print the ",index" component, if any.  */
-	  if (address.index)
-	    {
-	      if (address.base)
-		putc (',', file);
-	      fprintf (file, "%s.%c",
-		       M68K_REGNAME (REGNO (address.index)),
-		       GET_MODE (address.index) == HImode ? 'w' : 'l');
-	      if (address.scale != 1)
-		fprintf (file, "*%d", address.scale);
-	    }
-	  putc (')', file);
-	}
+        {
+          /* Print the "offset(base" component.  */
+          if (labelno >= 0)
+            asm_fprintf (file, "%LL%d(%Rpc,", labelno);
+          else
+            {
+              if (address.offset)
+        	output_addr_const (file, address.offset);
+
+              putc ('(', file);
+              if (address.base)
+        	fputs (M68K_REGNAME (REGNO (address.base)), file);
+            }
+          /* Print the ",index" component, if any.  */
+          if (address.index)
+            {
+              if (address.base)
+        	putc (',', file);
+              fprintf (file, "%s.%c",
+        	       M68K_REGNAME (REGNO (address.index)),
+        	       GET_MODE (address.index) == HImode ? 'w' : 'l');
+              if (address.scale != 1)
+        	fprintf (file, "*%d", address.scale);
+            }
+          putc (')', file);
+        }
       else /* !MOTOROLA */
-	{
-	  if (!address.offset && !address.index)
-	    fprintf (file, "%s@", M68K_REGNAME (REGNO (address.base)));
-	  else
-	    {
-	      /* Print the "base@(offset" component.  */
-	      if (labelno >= 0)
-		asm_fprintf (file, "%Rpc@(%LL%d", labelno);
-	      else
-		{
-		  if (address.base)
-		    fputs (M68K_REGNAME (REGNO (address.base)), file);
-		  fprintf (file, "@(");
-		  if (address.offset)
-		    output_addr_const (file, address.offset);
-		}
-	      /* Print the ",index" component, if any.  */
-	      if (address.index)
-		{
-		  fprintf (file, ",%s:%c",
-			   M68K_REGNAME (REGNO (address.index)),
-			   GET_MODE (address.index) == HImode ? 'w' : 'l');
-		  if (address.scale != 1)
-		    fprintf (file, ":%d", address.scale);
-		}
-	      putc (')', file);
-	    }
-	}
+        {
+          if (!address.offset && !address.index)
+            fprintf (file, "%s@", M68K_REGNAME (REGNO (address.base)));
+          else
+            {
+              /* Print the "base@(offset" component.  */
+              if (labelno >= 0)
+        	asm_fprintf (file, "%Rpc@(%LL%d", labelno);
+              else
+        	{
+        	  if (address.base)
+        	    fputs (M68K_REGNAME (REGNO (address.base)), file);
+        	  fprintf (file, "@(");
+        	  if (address.offset)
+        	    output_addr_const (file, address.offset);
+        	}
+              /* Print the ",index" component, if any.  */
+              if (address.index)
+        	{
+        	  fprintf (file, ",%s:%c",
+        		   M68K_REGNAME (REGNO (address.index)),
+        		   GET_MODE (address.index) == HImode ? 'w' : 'l');
+        	  if (address.scale != 1)
+        	    fprintf (file, ":%d", address.scale);
+        	}
+              putc (')', file);
+            }
+        }
     }
 }
 
@@ -4874,47 +5140,48 @@ bool
 strict_low_part_peephole_ok (enum machine_mode mode, rtx first_insn,
                              rtx target)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx p = first_insn;
 
   while ((p = PREV_INSN (p)))
     {
       if (NOTE_INSN_BASIC_BLOCK_P (p))
-	return false;
+        return false;
 
       if (NOTE_P (p))
-	continue;
+        continue;
 
       /* If it isn't an insn, then give up.  */
       if (!INSN_P (p))
-	return false;
+        return false;
 
       if (reg_set_p (target, p))
-	{
-	  rtx set = single_set (p);
-	  rtx dest;
+        {
+          rtx set = single_set (p);
+          rtx dest;
 
-	  /* If it isn't an easy to recognize insn, then give up.  */
-	  if (! set)
-	    return false;
-
-	  dest = SET_DEST (set);
-
-	  /* If this sets the entire target register to zero, then our
-	     first_insn is redundant.  */
-	  if (rtx_equal_p (dest, target)
-	      && SET_SRC (set) == const0_rtx)
-	    return true;
-	  else if (GET_CODE (dest) == STRICT_LOW_PART
-		   && GET_CODE (XEXP (dest, 0)) == REG
-		   && REGNO (XEXP (dest, 0)) == REGNO (target)
-		   && (GET_MODE_SIZE (GET_MODE (XEXP (dest, 0)))
-		       <= GET_MODE_SIZE (mode)))
-	    /* This is a strict low part set which modifies less than
-	       we are using, so it is safe.  */
-	    ;
-	  else
-	    return false;
-	}
+          /* If it isn't an easy to recognize insn, then give up.  */
+          if (! set)
+            return false;
+
+          dest = SET_DEST (set);
+
+          /* If this sets the entire target register to zero, then our
+             first_insn is redundant.  */
+          if (rtx_equal_p (dest, target)
+              && SET_SRC (set) == const0_rtx)
+            return true;
+          else if (GET_CODE (dest) == STRICT_LOW_PART
+        	   && GET_CODE (XEXP (dest, 0)) == REG
+        	   && REGNO (XEXP (dest, 0)) == REGNO (target)
+        	   && (GET_MODE_SIZE (GET_MODE (XEXP (dest, 0)))
+        	       <= GET_MODE_SIZE (mode)))
+            /* This is a strict low part set which modifies less than
+               we are using, so it is safe.  */
+            ;
+          else
+            return false;
+        }
     }
 
   return false;
@@ -4947,12 +5214,12 @@ strict_low_part_peephole_ok (enum machin
    don't will cause an extra reload register to be allocated where one
    was not necessary:
 
-	lea (abc:w,%pc),%a0	; need to reload address
-	moveq &1,%d1		; since write to pc-relative space
-	movel %d1,%a0@		; is not allowed
-	...
-	lea (abc:w,%pc),%a1	; no need to reload address here
-	movel %a1@,%d0		; since "movel (abc:w,%pc),%d0" is ok
+        lea (abc:w,%pc),%a0	; need to reload address
+        moveq &1,%d1		; since write to pc-relative space
+        movel %d1,%a0@		; is not allowed
+        ...
+        lea (abc:w,%pc),%a1	; no need to reload address here
+        movel %a1@,%d0		; since "movel (abc:w,%pc),%d0" is ok
 
    For more info, consult tiemann@cygnus.com.
 
@@ -4980,11 +5247,12 @@ strict_low_part_peephole_ok (enum machin
 const char *
 output_andsi3 (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int logval;
   if (GET_CODE (operands[2]) == CONST_INT
       && (INTVAL (operands[2]) | 0xffff) == -1
       && (DATA_REG_P (operands[0])
-	  || offsettable_memref_p (operands[0]))
+          || offsettable_memref_p (operands[0]))
       && !TARGET_COLDFIRE)
     {
       if (GET_CODE (operands[0]) != REG)
@@ -5002,11 +5270,11 @@ output_andsi3 (rtx *operands)
           || offsettable_memref_p (operands[0])))
     {
       if (DATA_REG_P (operands[0]))
-	operands[1] = GEN_INT (logval);
+        operands[1] = GEN_INT (logval);
       else
         {
-	  operands[0] = adjust_address (operands[0], SImode, 3 - (logval / 8));
-	  operands[1] = GEN_INT (logval % 8);
+          operands[0] = adjust_address (operands[0], SImode, 3 - (logval / 8));
+          operands[1] = GEN_INT (logval % 8);
         }
       /* This does not set condition codes in a standard way.  */
       CC_STATUS_INIT;
@@ -5018,11 +5286,12 @@ output_andsi3 (rtx *operands)
 const char *
 output_iorsi3 (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   register int logval;
   if (GET_CODE (operands[2]) == CONST_INT
       && INTVAL (operands[2]) >> 16 == 0
       && (DATA_REG_P (operands[0])
-	  || offsettable_memref_p (operands[0]))
+          || offsettable_memref_p (operands[0]))
       && !TARGET_COLDFIRE)
     {
       if (GET_CODE (operands[0]) != REG)
@@ -5030,21 +5299,21 @@ output_iorsi3 (rtx *operands)
       /* Do not delete a following tstl %0 insn; that would be incorrect.  */
       CC_STATUS_INIT;
       if (INTVAL (operands[2]) == 0xffff)
-	return "mov%.w %2,%0";
+        return "mov%.w %2,%0";
       return "or%.w %2,%0";
     }
   if (GET_CODE (operands[2]) == CONST_INT
       && (logval = exact_log2 (INTVAL (operands[2]) & 0xffffffff)) >= 0
       && (DATA_REG_P (operands[0])
-	  || offsettable_memref_p (operands[0])))
+          || offsettable_memref_p (operands[0])))
     {
       if (DATA_REG_P (operands[0]))
-	operands[1] = GEN_INT (logval);
+        operands[1] = GEN_INT (logval);
       else
         {
-	  operands[0] = adjust_address (operands[0], SImode, 3 - (logval / 8));
-	  operands[1] = GEN_INT (logval % 8);
-	}
+          operands[0] = adjust_address (operands[0], SImode, 3 - (logval / 8));
+          operands[1] = GEN_INT (logval % 8);
+        }
       CC_STATUS_INIT;
       return "bset %1,%0";
     }
@@ -5054,6 +5323,7 @@ output_iorsi3 (rtx *operands)
 const char *
 output_xorsi3 (rtx *operands)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   register int logval;
   if (GET_CODE (operands[2]) == CONST_INT
       && INTVAL (operands[2]) >> 16 == 0
@@ -5061,25 +5331,25 @@ output_xorsi3 (rtx *operands)
       && !TARGET_COLDFIRE)
     {
       if (! DATA_REG_P (operands[0]))
-	operands[0] = adjust_address (operands[0], HImode, 2);
+        operands[0] = adjust_address (operands[0], HImode, 2);
       /* Do not delete a following tstl %0 insn; that would be incorrect.  */
       CC_STATUS_INIT;
       if (INTVAL (operands[2]) == 0xffff)
-	return "not%.w %0";
+        return "not%.w %0";
       return "eor%.w %2,%0";
     }
   if (GET_CODE (operands[2]) == CONST_INT
       && (logval = exact_log2 (INTVAL (operands[2]) & 0xffffffff)) >= 0
       && (DATA_REG_P (operands[0])
-	  || offsettable_memref_p (operands[0])))
+          || offsettable_memref_p (operands[0])))
     {
       if (DATA_REG_P (operands[0]))
-	operands[1] = GEN_INT (logval);
+        operands[1] = GEN_INT (logval);
       else
         {
-	  operands[0] = adjust_address (operands[0], SImode, 3 - (logval / 8));
-	  operands[1] = GEN_INT (logval % 8);
-	}
+          operands[0] = adjust_address (operands[0], SImode, 3 - (logval / 8));
+          operands[1] = GEN_INT (logval % 8);
+        }
       CC_STATUS_INIT;
       return "bchg %1,%0";
     }
@@ -5092,6 +5362,7 @@ output_xorsi3 (rtx *operands)
 const char *
 output_call (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (symbolic_operand (x, VOIDmode))
     return m68k_symbolic_call;
   else
@@ -5103,6 +5374,7 @@ output_call (rtx x)
 const char *
 output_sibcall (rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (symbolic_operand (x, VOIDmode))
     return m68k_symbolic_jump;
   else
@@ -5111,9 +5383,10 @@ output_sibcall (rtx x)
 
 static void
 m68k_output_mi_thunk (FILE *file, tree thunk ATTRIBUTE_UNUSED,
-		      HOST_WIDE_INT delta, HOST_WIDE_INT vcall_offset,
-		      tree function)
+        	      HOST_WIDE_INT delta, HOST_WIDE_INT vcall_offset,
+        	      tree function)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   rtx this_slot, offset, addr, mem, insn, tmp;
 
   /* Avoid clobbering the struct value reg by using the
@@ -5132,13 +5405,13 @@ m68k_output_mi_thunk (FILE *file, tree t
       /* Make the offset a legitimate operand for memory addition.  */
       offset = GEN_INT (delta);
       if ((delta < -8 || delta > 8)
-	  && (TARGET_COLDFIRE || USE_MOVQ (delta)))
-	{
-	  emit_move_insn (gen_rtx_REG (Pmode, D0_REG), offset);
-	  offset = gen_rtx_REG (Pmode, D0_REG);
-	}
+          && (TARGET_COLDFIRE || USE_MOVQ (delta)))
+        {
+          emit_move_insn (gen_rtx_REG (Pmode, D0_REG), offset);
+          offset = gen_rtx_REG (Pmode, D0_REG);
+        }
       emit_insn (gen_add3_insn (copy_rtx (this_slot),
-				copy_rtx (this_slot), offset));
+        			copy_rtx (this_slot), offset));
     }
 
   /* If needed, add *(*THIS + VCALL_OFFSET) to THIS.  */
@@ -5151,17 +5424,17 @@ m68k_output_mi_thunk (FILE *file, tree t
       /* Set ADDR to a legitimate address for *THIS + VCALL_OFFSET.  */
       addr = plus_constant (tmp, vcall_offset);
       if (!m68k_legitimate_address_p (Pmode, addr, true))
-	{
-	  emit_insn (gen_rtx_SET (VOIDmode, tmp, addr));
-	  addr = tmp;
-	}
+        {
+          emit_insn (gen_rtx_SET (VOIDmode, tmp, addr));
+          addr = tmp;
+        }
 
       /* Load the offset into %d0 and add it to THIS.  */
       emit_move_insn (gen_rtx_REG (Pmode, D0_REG),
-		      gen_rtx_MEM (Pmode, addr));
+        	      gen_rtx_MEM (Pmode, addr));
       emit_insn (gen_add3_insn (copy_rtx (this_slot),
-				copy_rtx (this_slot),
-				gen_rtx_REG (Pmode, D0_REG)));
+        			copy_rtx (this_slot),
+        			gen_rtx_REG (Pmode, D0_REG)));
     }
 
   /* Jump to the target function.  Use a sibcall if direct jumps are
@@ -5172,13 +5445,13 @@ m68k_output_mi_thunk (FILE *file, tree t
       gcc_assert (flag_pic);
 
       if (!TARGET_SEP_DATA)
-	{
-	  /* Use the static chain register as a temporary (call-clobbered)
-	     GOT pointer for this function.  We can use the static chain
-	     register because it isn't live on entry to the thunk.  */
-	  SET_REGNO (pic_offset_table_rtx, STATIC_CHAIN_REGNUM);
-	  emit_insn (gen_load_got (pic_offset_table_rtx));
-	}
+        {
+          /* Use the static chain register as a temporary (call-clobbered)
+             GOT pointer for this function.  We can use the static chain
+             register because it isn't live on entry to the thunk.  */
+          SET_REGNO (pic_offset_table_rtx, STATIC_CHAIN_REGNUM);
+          emit_insn (gen_load_got (pic_offset_table_rtx));
+        }
       legitimize_pic_address (XEXP (mem, 0), Pmode, tmp);
       mem = replace_equiv_address (mem, tmp);
     }
@@ -5200,20 +5473,28 @@ m68k_output_mi_thunk (FILE *file, tree t
     SET_REGNO (pic_offset_table_rtx, PIC_REG);
 }
 
+static bool
+m68k_promote_prototypes (const_tree t ATTRIBUTE_UNUSED)
+{
+    return !TARGET_FASTCALL;
+}
+
 /* Worker function for TARGET_STRUCT_VALUE_RTX.  */
 
 static rtx
 m68k_struct_value_rtx (tree fntype ATTRIBUTE_UNUSED,
-		       int incoming ATTRIBUTE_UNUSED)
+        	       int incoming ATTRIBUTE_UNUSED)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   return gen_rtx_REG (Pmode, M68K_STRUCT_VALUE_REGNUM);
 }
 
 /* Return nonzero if register old_reg can be renamed to register new_reg.  */
 int
 m68k_hard_regno_rename_ok (unsigned int old_reg ATTRIBUTE_UNUSED,
-			   unsigned int new_reg)
+        		   unsigned int new_reg)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
 
   /* Interrupt functions can only use registers that have already been
      saved by the prologue, even if they would normally be
@@ -5234,25 +5515,26 @@ m68k_hard_regno_rename_ok (unsigned int
 bool
 m68k_regno_mode_ok (int regno, enum machine_mode mode)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   if (DATA_REGNO_P (regno))
     {
       /* Data Registers, can hold aggregate if fits in.  */
       if (regno + GET_MODE_SIZE (mode) / 4 <= 8)
-	return true;
+        return true;
     }
   else if (ADDRESS_REGNO_P (regno))
     {
       if (regno + GET_MODE_SIZE (mode) / 4 <= 16)
-	return true;
+        return true;
     }
   else if (FP_REGNO_P (regno))
     {
       /* FPU registers, hold float or complex float of long double or
-	 smaller.  */
+         smaller.  */
       if ((GET_MODE_CLASS (mode) == MODE_FLOAT
-	   || GET_MODE_CLASS (mode) == MODE_COMPLEX_FLOAT)
-	  && GET_MODE_UNIT_SIZE (mode) <= TARGET_FP_REG_SIZE)
-	return true;
+           || GET_MODE_CLASS (mode) == MODE_COMPLEX_FLOAT)
+          && GET_MODE_UNIT_SIZE (mode) <= TARGET_FP_REG_SIZE)
+        return true;
     }
   return false;
 }
@@ -5261,8 +5543,9 @@ m68k_regno_mode_ok (int regno, enum mach
 
 enum reg_class
 m68k_secondary_reload_class (enum reg_class rclass,
-			     enum machine_mode mode, rtx x)
+        		     enum machine_mode mode, rtx x)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int regno;
 
   regno = true_regnum (x);
@@ -5289,6 +5572,7 @@ m68k_secondary_reload_class (enum reg_cl
 enum reg_class
 m68k_preferred_reload_class (rtx x, enum reg_class rclass)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   enum reg_class secondary_class;
 
   /* If RCLASS might need a secondary reload, try restricting it to
@@ -5309,7 +5593,7 @@ m68k_preferred_reload_class (rtx x, enum
       && GET_MODE_CLASS (GET_MODE (x)) == MODE_FLOAT)
     {
       if (TARGET_HARD_FLOAT && reg_class_subset_p (FP_REGS, rclass))
-	return FP_REGS;
+        return FP_REGS;
 
       return NO_REGS;
     }
@@ -5324,8 +5608,9 @@ m68k_preferred_reload_class (rtx x, enum
    If there is need for a hard-float ABI it is probably worth doing it
    properly and also passing function arguments in FP registers.  */
 rtx
-m68k_libcall_value (enum machine_mode mode)
+m68k_libcall_value (enum machine_mode mode, bool regs)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   switch (mode) {
   case SFmode:
   case DFmode:
@@ -5344,11 +5629,14 @@ m68k_libcall_value (enum machine_mode mo
    NOTE: Due to differences in ABIs, don't call this function directly,
    use FUNCTION_VALUE instead.  */
 rtx
-m68k_function_value (const_tree valtype, const_tree func ATTRIBUTE_UNUSED)
+m68k_function_value (const_tree valtype, const_tree func ATTRIBUTE_UNUSED, bool regs)
 {
-  enum machine_mode mode;
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  enum machine_mode mode = TYPE_MODE (valtype);
+  if (! regs)
+    if (!TARGET_FASTCALL)
+      return gen_rtx_REG (mode, D0_REG);
 
-  mode = TYPE_MODE (valtype);
   switch (mode) {
   case SFmode:
   case DFmode:
@@ -5361,7 +5649,7 @@ m68k_function_value (const_tree valtype,
   }
 
   /* If the function returns a pointer, push that into %a0.  */
-  if (func && POINTER_TYPE_P (TREE_TYPE (TREE_TYPE (func))))
+  if (regs && func && POINTER_TYPE_P (TREE_TYPE (TREE_TYPE (func))) && !TARGET_FASTCALL)
     /* For compatibility with the large body of existing code which
        does not always properly declare external functions returning
        pointer types, the m68k/SVR4 convention is to copy the value
@@ -5372,12 +5660,12 @@ m68k_function_value (const_tree valtype,
     return gen_rtx_PARALLEL
       (mode,
        gen_rtvec (2,
-		  gen_rtx_EXPR_LIST (VOIDmode,
-				     gen_rtx_REG (mode, A0_REG),
-				     const0_rtx),
-		  gen_rtx_EXPR_LIST (VOIDmode,
-				     gen_rtx_REG (mode, D0_REG),
-				     const0_rtx)));
+        	  gen_rtx_EXPR_LIST (VOIDmode,
+        			     gen_rtx_REG (mode, A0_REG),
+        			     const0_rtx),
+        	  gen_rtx_EXPR_LIST (VOIDmode,
+        			     gen_rtx_REG (mode, D0_REG),
+        			     const0_rtx)));
   else if (POINTER_TYPE_P (valtype))
     return gen_rtx_REG (mode, A0_REG);
   else
@@ -5389,6 +5677,7 @@ m68k_function_value (const_tree valtype,
 static bool
 m68k_return_in_memory (const_tree type, const_tree fntype ATTRIBUTE_UNUSED)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   enum machine_mode mode = TYPE_MODE (type);
 
   if (mode == BLKmode)
@@ -5406,1210 +5695,150 @@ m68k_return_in_memory (const_tree type,
 }
 #endif
 
-/* CPU to schedule the program for.  */
-enum attr_cpu m68k_sched_cpu;
 
-/* MAC to schedule the program for.  */
-enum attr_mac m68k_sched_mac;
-
-/* Operand type.  */
-enum attr_op_type
-  {
-    /* No operand.  */
-    OP_TYPE_NONE,
-
-    /* Integer register.  */
-    OP_TYPE_RN,
-
-    /* FP register.  */
-    OP_TYPE_FPN,
-
-    /* Implicit mem reference (e.g. stack).  */
-    OP_TYPE_MEM1,
-
-    /* Memory without offset or indexing.  EA modes 2, 3 and 4.  */
-    OP_TYPE_MEM234,
+/* We generate a two-instructions program at M_TRAMP :
+        movea.l &CHAIN_VALUE,%a0
+        jmp FNADDR
+   where %a0 can be modified by changing STATIC_CHAIN_REGNUM.  */
 
-    /* Memory with offset but without indexing.  EA mode 5.  */
-    OP_TYPE_MEM5,
+static void
+m68k_trampoline_init (rtx m_tramp, tree fndecl, rtx chain_value)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  rtx fnaddr = XEXP (DECL_RTL (fndecl), 0);
+  rtx mem;
 
-    /* Memory with indexing.  EA mode 6.  */
-    OP_TYPE_MEM6,
+  gcc_assert (ADDRESS_REGNO_P (STATIC_CHAIN_REGNUM));
 
-    /* Memory referenced by absolute address.  EA mode 7.  */
-    OP_TYPE_MEM7,
+  mem = adjust_address (m_tramp, HImode, 0);
+  emit_move_insn (mem, GEN_INT(0x207C + ((STATIC_CHAIN_REGNUM-8) << 9)));
+  mem = adjust_address (m_tramp, SImode, 2);
+  emit_move_insn (mem, chain_value);
 
-    /* Immediate operand that doesn't require extension word.  */
-    OP_TYPE_IMM_Q,
+  mem = adjust_address (m_tramp, HImode, 6);
+  emit_move_insn (mem, GEN_INT(0x4EF9));
+  mem = adjust_address (m_tramp, SImode, 8);
+  emit_move_insn (mem, fnaddr);
 
-    /* Immediate 16 bit operand.  */
-    OP_TYPE_IMM_W,
+  FINALIZE_TRAMPOLINE (XEXP (m_tramp, 0));
+}
 
-    /* Immediate 32 bit operand.  */
-    OP_TYPE_IMM_L
-  };
+/* On the 68000, the RTS insn cannot pop anything.
+   On the 68010, the RTD insn may be used to pop them if the number
+     of args is fixed, but if the number is variable then the caller
+     must pop them all.  RTD can't be used for library calls now
+     because the library is compiled with the Unix compiler.
+   Use of RTD is a selectable option, since it is incompatible with
+   standard Unix calling sequences.  If the option is not selected,
+   the caller must always pop the args.  */
 
-/* Return type of memory ADDR_RTX refers to.  */
-static enum attr_op_type
-sched_address_type (enum machine_mode mode, rtx addr_rtx)
+static int
+m68k_return_pops_args (tree fundecl, tree funtype, int size)
 {
-  struct m68k_address address;
-
-  if (symbolic_operand (addr_rtx, VOIDmode))
-    return OP_TYPE_MEM7;
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  return ((TARGET_RTD
+           && (!fundecl
+               || TREE_CODE (fundecl) != IDENTIFIER_NODE)
+           && (!stdarg_p (funtype)))
+          ? size : 0);
+}
 
-  if (!m68k_decompose_address (mode, addr_rtx,
-			       reload_completed, &address))
+/* Remember the last target of m68k_set_current_function.  */
+static GTY(()) tree m68k_previous_fndecl;
+
+/* Establish appropriate back-end context for processing the function
+   FNDECL.  The argument might be NULL to indicate processing at top
+   level, outside of any function scope.  */
+static void
+m68k_set_current_function (tree fndecl)
+{
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
+  /* Only change the context if the function changes.  This hook is called
+     several times in the course of compiling a function, and we don't want to
+     slow things down too much or call target_reinit when it isn't safe.  */
+  if (fndecl && fndecl != m68k_previous_fndecl)
     {
-      gcc_assert (!reload_completed);
-      /* Reload will likely fix the address to be in the register.  */
-      return OP_TYPE_MEM234;
-    }
+      tree old_tree = (m68k_previous_fndecl
+		       ? DECL_FUNCTION_SPECIFIC_TARGET (m68k_previous_fndecl)
+		       : NULL_TREE);
 
-  if (address.scale != 0)
-    return OP_TYPE_MEM6;
+      tree new_tree = (fndecl
+		       ? DECL_FUNCTION_SPECIFIC_TARGET (fndecl)
+		       : NULL_TREE);
 
-  if (address.base != NULL_RTX)
-    {
-      if (address.offset == NULL_RTX)
-	return OP_TYPE_MEM234;
+      m68k_previous_fndecl = fndecl;
+      if (old_tree == new_tree)
+	;
 
-      return OP_TYPE_MEM5;
-    }
+      else if (new_tree)
+	{
+	  cl_target_option_restore (&global_options,
+				    TREE_TARGET_OPTION (new_tree));
+	  target_reinit ();
+	}
 
-  gcc_assert (address.offset != NULL_RTX);
+      else if (old_tree)
+	{
+	  struct cl_target_option *def
+	    = TREE_TARGET_OPTION (target_option_current_node);
 
-  return OP_TYPE_MEM7;
+	  cl_target_option_restore (&global_options, def);
+	  target_reinit ();
+	}
+    }
 }
 
-/* Return X or Y (depending on OPX_P) operand of INSN.  */
-static rtx
-sched_get_operand (rtx insn, bool opx_p)
+void
+m68k_order_regs_for_local_alloc (void)
 {
+  DPRINTFA("Debug: %s\n", __FUNCTION__);
   int i;
-
-  if (recog_memoized (insn) < 0)
-    gcc_unreachable ();
-
-  extract_constrain_insn_cached (insn);
-
-  if (opx_p)
-    i = get_attr_opx (insn);
-  else
-    i = get_attr_opy (insn);
-
-  if (i >= recog_data.n_operands)
-    return NULL;
-
-  return recog_data.operand[i];
+  int pos = 0;
+  for (i = 0; i < 16; i ++)
+    if (call_used_regs [i] && !fixed_regs[i])
+      reg_alloc_order[pos++] = i;
+  for (i = 0; i < 16; i ++)
+    if (!(call_used_regs [i] && !fixed_regs[i]))
+      reg_alloc_order[pos++] = i;
+  reg_alloc_order[pos++] = 24;
+  for (i = 16; i < 24; i++)
+    reg_alloc_order[pos++] = i;
 }
 
-/* Return type of INSN's operand X (if OPX_P) or operand Y (if !OPX_P).
-   If ADDRESS_P is true, return type of memory location operand refers to.  */
-static enum attr_op_type
-sched_attr_op_type (rtx insn, bool opx_p, bool address_p)
-{
-  rtx op;
-
-  op = sched_get_operand (insn, opx_p);
+/* Make sure everything's fine if we *don't* have a given processor.
+   This assumes that putting a register in fixed_regs will keep the
+   compiler's mitts completely off it.  We don't bother to zero it out
+   of register classes.  */
 
-  if (op == NULL)
-    {
-      gcc_assert (!reload_completed);
-      return OP_TYPE_RN;
-    }
-
-  if (address_p)
-    return sched_address_type (QImode, op);
-
-  if (memory_operand (op, VOIDmode))
-    return sched_address_type (GET_MODE (op), XEXP (op, 0));
-
-  if (register_operand (op, VOIDmode))
-    {
-      if ((!reload_completed && FLOAT_MODE_P (GET_MODE (op)))
-	  || (reload_completed && FP_REG_P (op)))
-	return OP_TYPE_FPN;
-
-      return OP_TYPE_RN;
-    }
-
-  if (GET_CODE (op) == CONST_INT)
-    {
-      int ival;
-
-      ival = INTVAL (op);
-
-      /* Check for quick constants.  */
-      switch (get_attr_type (insn))
-	{
-	case TYPE_ALUQ_L:
-	  if (IN_RANGE (ival, 1, 8) || IN_RANGE (ival, -8, -1))
-	    return OP_TYPE_IMM_Q;
-
-	  gcc_assert (!reload_completed);
-	  break;
-
-	case TYPE_MOVEQ_L:
-	  if (USE_MOVQ (ival))
-	    return OP_TYPE_IMM_Q;
-
-	  gcc_assert (!reload_completed);
-	  break;
-
-	case TYPE_MOV3Q_L:
-	  if (valid_mov3q_const (ival))
-	    return OP_TYPE_IMM_Q;
-
-	  gcc_assert (!reload_completed);
-	  break;
-
-	default:
-	  break;
-	}
-
-      if (IN_RANGE (ival, -0x8000, 0x7fff))
-	return OP_TYPE_IMM_W;
-
-      return OP_TYPE_IMM_L;
-    }
-
-  if (GET_CODE (op) == CONST_DOUBLE)
-    {
-      switch (GET_MODE (op))
-	{
-	case SFmode:
-	  return OP_TYPE_IMM_W;
-
-	case VOIDmode:
-	case DFmode:
-	  return OP_TYPE_IMM_L;
-
-	default:
-	  gcc_unreachable ();
-	}
-    }
-
-  if (GET_CODE (op) == CONST
-      || symbolic_operand (op, VOIDmode)
-      || LABEL_P (op))
-    {
-      switch (GET_MODE (op))
-	{
-	case QImode:
-	  return OP_TYPE_IMM_Q;
-
-	case HImode:
-	  return OP_TYPE_IMM_W;
-
-	case SImode:
-	  return OP_TYPE_IMM_L;
-
-	default:
-	  if (symbolic_operand (m68k_unwrap_symbol (op, false), VOIDmode))
-	    /* Just a guess.  */
-	    return OP_TYPE_IMM_W;
-
-	  return OP_TYPE_IMM_L;
-	}
-    }
-
-  gcc_assert (!reload_completed);
-
-  if (FLOAT_MODE_P (GET_MODE (op)))
-    return OP_TYPE_FPN;
-
-  return OP_TYPE_RN;
-}
-
-/* Implement opx_type attribute.
-   Return type of INSN's operand X.
-   If ADDRESS_P is true, return type of memory location operand refers to.  */
-enum attr_opx_type
-m68k_sched_attr_opx_type (rtx insn, int address_p)
-{
-  switch (sched_attr_op_type (insn, true, address_p != 0))
-    {
-    case OP_TYPE_RN:
-      return OPX_TYPE_RN;
-
-    case OP_TYPE_FPN:
-      return OPX_TYPE_FPN;
-
-    case OP_TYPE_MEM1:
-      return OPX_TYPE_MEM1;
-
-    case OP_TYPE_MEM234:
-      return OPX_TYPE_MEM234;
-
-    case OP_TYPE_MEM5:
-      return OPX_TYPE_MEM5;
-
-    case OP_TYPE_MEM6:
-      return OPX_TYPE_MEM6;
-
-    case OP_TYPE_MEM7:
-      return OPX_TYPE_MEM7;
-
-    case OP_TYPE_IMM_Q:
-      return OPX_TYPE_IMM_Q;
-
-    case OP_TYPE_IMM_W:
-      return OPX_TYPE_IMM_W;
-
-    case OP_TYPE_IMM_L:
-      return OPX_TYPE_IMM_L;
-
-    default:
-      gcc_unreachable ();
-    }
-}
-
-/* Implement opy_type attribute.
-   Return type of INSN's operand Y.
-   If ADDRESS_P is true, return type of memory location operand refers to.  */
-enum attr_opy_type
-m68k_sched_attr_opy_type (rtx insn, int address_p)
-{
-  switch (sched_attr_op_type (insn, false, address_p != 0))
-    {
-    case OP_TYPE_RN:
-      return OPY_TYPE_RN;
-
-    case OP_TYPE_FPN:
-      return OPY_TYPE_FPN;
-
-    case OP_TYPE_MEM1:
-      return OPY_TYPE_MEM1;
-
-    case OP_TYPE_MEM234:
-      return OPY_TYPE_MEM234;
-
-    case OP_TYPE_MEM5:
-      return OPY_TYPE_MEM5;
-
-    case OP_TYPE_MEM6:
-      return OPY_TYPE_MEM6;
-
-    case OP_TYPE_MEM7:
-      return OPY_TYPE_MEM7;
-
-    case OP_TYPE_IMM_Q:
-      return OPY_TYPE_IMM_Q;
-
-    case OP_TYPE_IMM_W:
-      return OPY_TYPE_IMM_W;
-
-    case OP_TYPE_IMM_L:
-      return OPY_TYPE_IMM_L;
-
-    default:
-      gcc_unreachable ();
-    }
-}
-
-/* Return size of INSN as int.  */
-static int
-sched_get_attr_size_int (rtx insn)
-{
-  int size;
-
-  switch (get_attr_type (insn))
-    {
-    case TYPE_IGNORE:
-      /* There should be no references to m68k_sched_attr_size for 'ignore'
-	 instructions.  */
-      gcc_unreachable ();
-      return 0;
-
-    case TYPE_MUL_L:
-      size = 2;
-      break;
-
-    default:
-      size = 1;
-      break;
-    }
-
-  switch (get_attr_opx_type (insn))
-    {
-    case OPX_TYPE_NONE:
-    case OPX_TYPE_RN:
-    case OPX_TYPE_FPN:
-    case OPX_TYPE_MEM1:
-    case OPX_TYPE_MEM234:
-    case OPY_TYPE_IMM_Q:
-      break;
-
-    case OPX_TYPE_MEM5:
-    case OPX_TYPE_MEM6:
-      /* Here we assume that most absolute references are short.  */
-    case OPX_TYPE_MEM7:
-    case OPY_TYPE_IMM_W:
-      ++size;
-      break;
-
-    case OPY_TYPE_IMM_L:
-      size += 2;
-      break;
-
-    default:
-      gcc_unreachable ();
-    }
-
-  switch (get_attr_opy_type (insn))
-    {
-    case OPY_TYPE_NONE:
-    case OPY_TYPE_RN:
-    case OPY_TYPE_FPN:
-    case OPY_TYPE_MEM1:
-    case OPY_TYPE_MEM234:
-    case OPY_TYPE_IMM_Q:
-      break;
-
-    case OPY_TYPE_MEM5:
-    case OPY_TYPE_MEM6:
-      /* Here we assume that most absolute references are short.  */
-    case OPY_TYPE_MEM7:
-    case OPY_TYPE_IMM_W:
-      ++size;
-      break;
-
-    case OPY_TYPE_IMM_L:
-      size += 2;
-      break;
-
-    default:
-      gcc_unreachable ();
-    }
-
-  if (size > 3)
-    {
-      gcc_assert (!reload_completed);
-
-      size = 3;
-    }
-
-  return size;
-}
-
-/* Return size of INSN as attribute enum value.  */
-enum attr_size
-m68k_sched_attr_size (rtx insn)
-{
-  switch (sched_get_attr_size_int (insn))
-    {
-    case 1:
-      return SIZE_1;
-
-    case 2:
-      return SIZE_2;
-
-    case 3:
-      return SIZE_3;
-
-    default:
-      gcc_unreachable ();
-    }
-}
-
-/* Return operand X or Y (depending on OPX_P) of INSN,
-   if it is a MEM, or NULL overwise.  */
-static enum attr_op_type
-sched_get_opxy_mem_type (rtx insn, bool opx_p)
-{
-  if (opx_p)
-    {
-      switch (get_attr_opx_type (insn))
-	{
-	case OPX_TYPE_NONE:
-	case OPX_TYPE_RN:
-	case OPX_TYPE_FPN:
-	case OPX_TYPE_IMM_Q:
-	case OPX_TYPE_IMM_W:
-	case OPX_TYPE_IMM_L:
-	  return OP_TYPE_RN;
-
-	case OPX_TYPE_MEM1:
-	case OPX_TYPE_MEM234:
-	case OPX_TYPE_MEM5:
-	case OPX_TYPE_MEM7:
-	  return OP_TYPE_MEM1;
-
-	case OPX_TYPE_MEM6:
-	  return OP_TYPE_MEM6;
-
-	default:
-	  gcc_unreachable ();
-	}
-    }
-  else
-    {
-      switch (get_attr_opy_type (insn))
-	{
-	case OPY_TYPE_NONE:
-	case OPY_TYPE_RN:
-	case OPY_TYPE_FPN:
-	case OPY_TYPE_IMM_Q:
-	case OPY_TYPE_IMM_W:
-	case OPY_TYPE_IMM_L:
-	  return OP_TYPE_RN;
-
-	case OPY_TYPE_MEM1:
-	case OPY_TYPE_MEM234:
-	case OPY_TYPE_MEM5:
-	case OPY_TYPE_MEM7:
-	  return OP_TYPE_MEM1;
-
-	case OPY_TYPE_MEM6:
-	  return OP_TYPE_MEM6;
-
-	default:
-	  gcc_unreachable ();
-	}
-    }
-}
-
-/* Implement op_mem attribute.  */
-enum attr_op_mem
-m68k_sched_attr_op_mem (rtx insn)
-{
-  enum attr_op_type opx;
-  enum attr_op_type opy;
-
-  opx = sched_get_opxy_mem_type (insn, true);
-  opy = sched_get_opxy_mem_type (insn, false);
-
-  if (opy == OP_TYPE_RN && opx == OP_TYPE_RN)
-    return OP_MEM_00;
-
-  if (opy == OP_TYPE_RN && opx == OP_TYPE_MEM1)
-    {
-      switch (get_attr_opx_access (insn))
-	{
-	case OPX_ACCESS_R:
-	  return OP_MEM_10;
-
-	case OPX_ACCESS_W:
-	  return OP_MEM_01;
-
-	case OPX_ACCESS_RW:
-	  return OP_MEM_11;
-
-	default:
-	  gcc_unreachable ();
-	}
-    }
-
-  if (opy == OP_TYPE_RN && opx == OP_TYPE_MEM6)
-    {
-      switch (get_attr_opx_access (insn))
-	{
-	case OPX_ACCESS_R:
-	  return OP_MEM_I0;
-
-	case OPX_ACCESS_W:
-	  return OP_MEM_0I;
-
-	case OPX_ACCESS_RW:
-	  return OP_MEM_I1;
-
-	default:
-	  gcc_unreachable ();
-	}
-    }
-
-  if (opy == OP_TYPE_MEM1 && opx == OP_TYPE_RN)
-    return OP_MEM_10;
-
-  if (opy == OP_TYPE_MEM1 && opx == OP_TYPE_MEM1)
-    {
-      switch (get_attr_opx_access (insn))
-	{
-	case OPX_ACCESS_W:
-	  return OP_MEM_11;
-
-	default:
-	  gcc_assert (!reload_completed);
-	  return OP_MEM_11;
-	}
-    }
-
-  if (opy == OP_TYPE_MEM1 && opx == OP_TYPE_MEM6)
-    {
-      switch (get_attr_opx_access (insn))
-	{
-	case OPX_ACCESS_W:
-	  return OP_MEM_1I;
-
-	default:
-	  gcc_assert (!reload_completed);
-	  return OP_MEM_1I;
-	}
-    }
-
-  if (opy == OP_TYPE_MEM6 && opx == OP_TYPE_RN)
-    return OP_MEM_I0;
-
-  if (opy == OP_TYPE_MEM6 && opx == OP_TYPE_MEM1)
-    {
-      switch (get_attr_opx_access (insn))
-	{
-	case OPX_ACCESS_W:
-	  return OP_MEM_I1;
-
-	default:
-	  gcc_assert (!reload_completed);
-	  return OP_MEM_I1;
-	}
-    }
-
-  gcc_assert (opy == OP_TYPE_MEM6 && opx == OP_TYPE_MEM6);
-  gcc_assert (!reload_completed);
-  return OP_MEM_I1;
-}
-
-/* Jump instructions types.  Indexed by INSN_UID.
-   The same rtl insn can be expanded into different asm instructions
-   depending on the cc0_status.  To properly determine type of jump
-   instructions we scan instruction stream and map jumps types to this
-   array.  */
-static enum attr_type *sched_branch_type;
-
-/* Return the type of the jump insn.  */
-enum attr_type
-m68k_sched_branch_type (rtx insn)
-{
-  enum attr_type type;
-
-  type = sched_branch_type[INSN_UID (insn)];
-
-  gcc_assert (type != 0);
-
-  return type;
-}
-
-/* Data for ColdFire V4 index bypass.
-   Producer modifies register that is used as index in consumer with
-   specified scale.  */
-static struct
-{
-  /* Producer instruction.  */
-  rtx pro;
-
-  /* Consumer instruction.  */
-  rtx con;
-
-  /* Scale of indexed memory access within consumer.
-     Or zero if bypass should not be effective at the moment.  */
-  int scale;
-} sched_cfv4_bypass_data;
-
-/* An empty state that is used in m68k_sched_adjust_cost.  */
-static state_t sched_adjust_cost_state;
-
-/* Implement adjust_cost scheduler hook.
-   Return adjusted COST of dependency LINK between DEF_INSN and INSN.  */
-static int
-m68k_sched_adjust_cost (rtx insn, rtx link ATTRIBUTE_UNUSED, rtx def_insn,
-			int cost)
-{
-  int delay;
-
-  if (recog_memoized (def_insn) < 0
-      || recog_memoized (insn) < 0)
-    return cost;
-
-  if (sched_cfv4_bypass_data.scale == 1)
-    /* Handle ColdFire V4 bypass for indexed address with 1x scale.  */
-    {
-      /* haifa-sched.c: insn_cost () calls bypass_p () just before
-	 targetm.sched.adjust_cost ().  Hence, we can be relatively sure
-	 that the data in sched_cfv4_bypass_data is up to date.  */
-      gcc_assert (sched_cfv4_bypass_data.pro == def_insn
-		  && sched_cfv4_bypass_data.con == insn);
-
-      if (cost < 3)
-	cost = 3;
-
-      sched_cfv4_bypass_data.pro = NULL;
-      sched_cfv4_bypass_data.con = NULL;
-      sched_cfv4_bypass_data.scale = 0;
-    }
-  else
-    gcc_assert (sched_cfv4_bypass_data.pro == NULL
-		&& sched_cfv4_bypass_data.con == NULL
-		&& sched_cfv4_bypass_data.scale == 0);
-
-  /* Don't try to issue INSN earlier than DFA permits.
-     This is especially useful for instructions that write to memory,
-     as their true dependence (default) latency is better to be set to 0
-     to workaround alias analysis limitations.
-     This is, in fact, a machine independent tweak, so, probably,
-     it should be moved to haifa-sched.c: insn_cost ().  */
-  delay = min_insn_conflict_delay (sched_adjust_cost_state, def_insn, insn);
-  if (delay > cost)
-    cost = delay;
-
-  return cost;
-}
-
-/* Return maximal number of insns that can be scheduled on a single cycle.  */
-static int
-m68k_sched_issue_rate (void)
-{
-  switch (m68k_sched_cpu)
-    {
-    case CPU_CFV1:
-    case CPU_CFV2:
-    case CPU_CFV3:
-      return 1;
-
-    case CPU_CFV4:
-      return 2;
-
-    default:
-      gcc_unreachable ();
-      return 0;
-    }
-}
-
-/* Maximal length of instruction for current CPU.
-   E.g. it is 3 for any ColdFire core.  */
-static int max_insn_size;
-
-/* Data to model instruction buffer of CPU.  */
-struct _sched_ib
-{
-  /* True if instruction buffer model is modeled for current CPU.  */
-  bool enabled_p;
-
-  /* Size of the instruction buffer in words.  */
-  int size;
-
-  /* Number of filled words in the instruction buffer.  */
-  int filled;
-
-  /* Additional information about instruction buffer for CPUs that have
-     a buffer of instruction records, rather then a plain buffer
-     of instruction words.  */
-  struct _sched_ib_records
-  {
-    /* Size of buffer in records.  */
-    int n_insns;
-
-    /* Array to hold data on adjustements made to the size of the buffer.  */
-    int *adjust;
-
-    /* Index of the above array.  */
-    int adjust_index;
-  } records;
-
-  /* An insn that reserves (marks empty) one word in the instruction buffer.  */
-  rtx insn;
-};
-
-static struct _sched_ib sched_ib;
-
-/* ID of memory unit.  */
-static int sched_mem_unit_code;
-
-/* Implementation of the targetm.sched.variable_issue () hook.
-   It is called after INSN was issued.  It returns the number of insns
-   that can possibly get scheduled on the current cycle.
-   It is used here to determine the effect of INSN on the instruction
-   buffer.  */
-static int
-m68k_sched_variable_issue (FILE *sched_dump ATTRIBUTE_UNUSED,
-			   int sched_verbose ATTRIBUTE_UNUSED,
-			   rtx insn, int can_issue_more)
-{
-  int insn_size;
-
-  if (recog_memoized (insn) >= 0 && get_attr_type (insn) != TYPE_IGNORE)
-    {
-      switch (m68k_sched_cpu)
-	{
-	case CPU_CFV1:
-	case CPU_CFV2:
-	  insn_size = sched_get_attr_size_int (insn);
-	  break;
-
-	case CPU_CFV3:
-	  insn_size = sched_get_attr_size_int (insn);
-	  
-	  /* ColdFire V3 and V4 cores have instruction buffers that can
-	     accumulate up to 8 instructions regardless of instructions'
-	     sizes.  So we should take care not to "prefetch" 24 one-word
-	     or 12 two-words instructions.
-	     To model this behavior we temporarily decrease size of the
-	     buffer by (max_insn_size - insn_size) for next 7 instructions.  */
-	  {
-	    int adjust;
-
-	    adjust = max_insn_size - insn_size;
-	    sched_ib.size -= adjust;
-
-	    if (sched_ib.filled > sched_ib.size)
-	      sched_ib.filled = sched_ib.size;
-
-	    sched_ib.records.adjust[sched_ib.records.adjust_index] = adjust;
-	  }
-
-	  ++sched_ib.records.adjust_index;
-	  if (sched_ib.records.adjust_index == sched_ib.records.n_insns)
-	    sched_ib.records.adjust_index = 0;
-
-	  /* Undo adjustement we did 7 instructions ago.  */
-	  sched_ib.size
-	    += sched_ib.records.adjust[sched_ib.records.adjust_index];
-
-	  break;
-
-	case CPU_CFV4:
-	  gcc_assert (!sched_ib.enabled_p);
-	  insn_size = 0;
-	  break;
-
-	default:
-	  gcc_unreachable ();
-	}
-
-      gcc_assert (insn_size <= sched_ib.filled);
-      --can_issue_more;
-    }
-  else if (GET_CODE (PATTERN (insn)) == ASM_INPUT
-	   || asm_noperands (PATTERN (insn)) >= 0)
-    insn_size = sched_ib.filled;
-  else
-    insn_size = 0;
-
-  sched_ib.filled -= insn_size;
-
-  return can_issue_more;
-}
-
-/* Return how many instructions should scheduler lookahead to choose the
-   best one.  */
-static int
-m68k_sched_first_cycle_multipass_dfa_lookahead (void)
-{
-  return m68k_sched_issue_rate () - 1;
-}
-
-/* Implementation of targetm.sched.init_global () hook.
-   It is invoked once per scheduling pass and is used here
-   to initialize scheduler constants.  */
-static void
-m68k_sched_md_init_global (FILE *sched_dump ATTRIBUTE_UNUSED,
-			   int sched_verbose ATTRIBUTE_UNUSED,
-			   int n_insns ATTRIBUTE_UNUSED)
-{
-  /* Init branch types.  */
-  {
-    rtx insn;
-
-    sched_branch_type = XCNEWVEC (enum attr_type, get_max_uid () + 1);
-
-    for (insn = get_insns (); insn != NULL_RTX; insn = NEXT_INSN (insn))
-      {
-	if (JUMP_P (insn))
-	  /* !!! FIXME: Implement real scan here.  */
-	  sched_branch_type[INSN_UID (insn)] = TYPE_BCC;
-      }
-  }
-
-#ifdef ENABLE_CHECKING
-  /* Check that all instructions have DFA reservations and
-     that all instructions can be issued from a clean state.  */
-  {
-    rtx insn;
-    state_t state;
-
-    state = alloca (state_size ());
-
-    for (insn = get_insns (); insn != NULL_RTX; insn = NEXT_INSN (insn))
-      {
- 	if (INSN_P (insn) && recog_memoized (insn) >= 0)
-	  {
- 	    gcc_assert (insn_has_dfa_reservation_p (insn));
-
- 	    state_reset (state);
- 	    if (state_transition (state, insn) >= 0)
- 	      gcc_unreachable ();
- 	  }
-      }
-  }
-#endif
-
-  /* Setup target cpu.  */
-
-  /* ColdFire V4 has a set of features to keep its instruction buffer full
-     (e.g., a separate memory bus for instructions) and, hence, we do not model
-     buffer for this CPU.  */
-  sched_ib.enabled_p = (m68k_sched_cpu != CPU_CFV4);
-
-  switch (m68k_sched_cpu)
-    {
-    case CPU_CFV4:
-      sched_ib.filled = 0;
-
-      /* FALLTHRU */
-
-    case CPU_CFV1:
-    case CPU_CFV2:
-      max_insn_size = 3;
-      sched_ib.records.n_insns = 0;
-      sched_ib.records.adjust = NULL;
-      break;
-
-    case CPU_CFV3:
-      max_insn_size = 3;
-      sched_ib.records.n_insns = 8;
-      sched_ib.records.adjust = XNEWVEC (int, sched_ib.records.n_insns);
-      break;
-
-    default:
-      gcc_unreachable ();
-    }
-
-  sched_mem_unit_code = get_cpu_unit_code ("cf_mem1");
-
-  sched_adjust_cost_state = xmalloc (state_size ());
-  state_reset (sched_adjust_cost_state);
-
-  start_sequence ();
-  emit_insn (gen_ib ());
-  sched_ib.insn = get_insns ();
-  end_sequence ();
-}
-
-/* Scheduling pass is now finished.  Free/reset static variables.  */
-static void
-m68k_sched_md_finish_global (FILE *dump ATTRIBUTE_UNUSED,
-			     int verbose ATTRIBUTE_UNUSED)
-{
-  sched_ib.insn = NULL;
-
-  free (sched_adjust_cost_state);
-  sched_adjust_cost_state = NULL;
-
-  sched_mem_unit_code = 0;
-
-  free (sched_ib.records.adjust);
-  sched_ib.records.adjust = NULL;
-  sched_ib.records.n_insns = 0;
-  max_insn_size = 0;
-
-  free (sched_branch_type);
-  sched_branch_type = NULL;
-}
-
-/* Implementation of targetm.sched.init () hook.
-   It is invoked each time scheduler starts on the new block (basic block or
-   extended basic block).  */
-static void
-m68k_sched_md_init (FILE *sched_dump ATTRIBUTE_UNUSED,
-		    int sched_verbose ATTRIBUTE_UNUSED,
-		    int n_insns ATTRIBUTE_UNUSED)
-{
-  switch (m68k_sched_cpu)
-    {
-    case CPU_CFV1:
-    case CPU_CFV2:
-      sched_ib.size = 6;
-      break;
-
-    case CPU_CFV3:
-      sched_ib.size = sched_ib.records.n_insns * max_insn_size;
-
-      memset (sched_ib.records.adjust, 0,
-	      sched_ib.records.n_insns * sizeof (*sched_ib.records.adjust));
-      sched_ib.records.adjust_index = 0;
-      break;
-
-    case CPU_CFV4:
-      gcc_assert (!sched_ib.enabled_p);
-      sched_ib.size = 0;
-      break;
-
-    default:
-      gcc_unreachable ();
-    }
-
-  if (sched_ib.enabled_p)
-    /* haifa-sched.c: schedule_block () calls advance_cycle () just before
-       the first cycle.  Workaround that.  */
-    sched_ib.filled = -2;
-}
-
-/* Implementation of targetm.sched.dfa_pre_advance_cycle () hook.
-   It is invoked just before current cycle finishes and is used here
-   to track if instruction buffer got its two words this cycle.  */
-static void
-m68k_sched_dfa_pre_advance_cycle (void)
-{
-  if (!sched_ib.enabled_p)
-    return;
-
-  if (!cpu_unit_reservation_p (curr_state, sched_mem_unit_code))
-    {
-      sched_ib.filled += 2;
-
-      if (sched_ib.filled > sched_ib.size)
-	sched_ib.filled = sched_ib.size;
-    }
-}
-
-/* Implementation of targetm.sched.dfa_post_advance_cycle () hook.
-   It is invoked just after new cycle begins and is used here
-   to setup number of filled words in the instruction buffer so that
-   instructions which won't have all their words prefetched would be
-   stalled for a cycle.  */
 static void
-m68k_sched_dfa_post_advance_cycle (void)
+m68k_conditional_register_usage (void)
 {
   int i;
-
-  if (!sched_ib.enabled_p)
-    return;
-
-  /* Setup number of prefetched instruction words in the instruction
-     buffer.  */
-  i = max_insn_size - sched_ib.filled;
-
-  while (--i >= 0)
-    {
-      if (state_transition (curr_state, sched_ib.insn) >= 0)
-	gcc_unreachable ();
-    }
-}
-
-/* Return X or Y (depending on OPX_P) operand of INSN,
-   if it is an integer register, or NULL overwise.  */
-static rtx
-sched_get_reg_operand (rtx insn, bool opx_p)
-{
-  rtx op = NULL;
-
-  if (opx_p)
-    {
-      if (get_attr_opx_type (insn) == OPX_TYPE_RN)
-	{
-	  op = sched_get_operand (insn, true);
-	  gcc_assert (op != NULL);
-
-	  if (!reload_completed && !REG_P (op))
-	    return NULL;
-	}
-    }
-  else
-    {
-      if (get_attr_opy_type (insn) == OPY_TYPE_RN)
-	{
-	  op = sched_get_operand (insn, false);
-	  gcc_assert (op != NULL);
-
-	  if (!reload_completed && !REG_P (op))
-	    return NULL;
-	}
+  int num_of_dregs = (TARGET_FASTCALL) ? M68K_FASTCALL_USED_DATA_REGS : M68K_STD_USED_REGS;
+  int num_of_aregs = (TARGET_FASTCALL) ? M68K_FASTCALL_USED_ADDR_REGS : M68K_STD_USED_REGS;
+  for (i = 0; i < 8; i++)
+    {
+      call_used_regs[i] = (i < num_of_dregs) | fixed_regs[i];
+      call_used_regs[i + 8] = (i < num_of_aregs) | fixed_regs[i + 8];
+      call_used_regs[i + 16] = (i < num_of_dregs) | fixed_regs[i + 16];
     }
-
-  return op;
-}
-
-/* Return true, if X or Y (depending on OPX_P) operand of INSN
-   is a MEM.  */
-static bool
-sched_mem_operand_p (rtx insn, bool opx_p)
-{
-  switch (sched_get_opxy_mem_type (insn, opx_p))
-    {
-    case OP_TYPE_MEM1:
-    case OP_TYPE_MEM6:
-      return true;
-
-    default:
-      return false;
-    }
-}
-
-/* Return X or Y (depending on OPX_P) operand of INSN,
-   if it is a MEM, or NULL overwise.  */
-static rtx
-sched_get_mem_operand (rtx insn, bool must_read_p, bool must_write_p)
-{
-  bool opx_p;
-  bool opy_p;
-
-  opx_p = false;
-  opy_p = false;
-
-  if (must_read_p)
-    {
-      opx_p = true;
-      opy_p = true;
-    }
-
-  if (must_write_p)
-    {
-      opx_p = true;
-      opy_p = false;
-    }
-
-  if (opy_p && sched_mem_operand_p (insn, false))
-    return sched_get_operand (insn, false);
-
-  if (opx_p && sched_mem_operand_p (insn, true))
-    return sched_get_operand (insn, true);
-
-  gcc_unreachable ();
-  return NULL;
-}
-
-/* Return non-zero if PRO modifies register used as part of
-   address in CON.  */
-int
-m68k_sched_address_bypass_p (rtx pro, rtx con)
-{
-  rtx pro_x;
-  rtx con_mem_read;
-
-  pro_x = sched_get_reg_operand (pro, true);
-  if (pro_x == NULL)
-    return 0;
-
-  con_mem_read = sched_get_mem_operand (con, true, false);
-  gcc_assert (con_mem_read != NULL);
-
-  if (reg_mentioned_p (pro_x, con_mem_read))
-    return 1;
-
-  return 0;
-}
-
-/* Helper function for m68k_sched_indexed_address_bypass_p.
-   if PRO modifies register used as index in CON,
-   return scale of indexed memory access in CON.  Return zero overwise.  */
-static int
-sched_get_indexed_address_scale (rtx pro, rtx con)
-{
-  rtx reg;
-  rtx mem;
-  struct m68k_address address;
-
-  reg = sched_get_reg_operand (pro, true);
-  if (reg == NULL)
-    return 0;
-
-  mem = sched_get_mem_operand (con, true, false);
-  gcc_assert (mem != NULL && MEM_P (mem));
-
-  if (!m68k_decompose_address (GET_MODE (mem), XEXP (mem, 0), reload_completed,
-			       &address))
-    gcc_unreachable ();
-
-  if (REGNO (reg) == REGNO (address.index))
+  if (flag_pic)
+    fixed_regs[PIC_REG] = 1;
+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
     {
-      gcc_assert (address.scale != 0);
-      return address.scale;
+       call_used_regs[i] |= fixed_regs[i];
+       call_really_used_regs[i] = call_used_regs[i];
     }
-
-  return 0;
-}
-
-/* Return non-zero if PRO modifies register used
-   as index with scale 2 or 4 in CON.  */
-int
-m68k_sched_indexed_address_bypass_p (rtx pro, rtx con)
-{
-  gcc_assert (sched_cfv4_bypass_data.pro == NULL
-	      && sched_cfv4_bypass_data.con == NULL
-	      && sched_cfv4_bypass_data.scale == 0);
-
-  switch (sched_get_indexed_address_scale (pro, con))
-    {
-    case 1:
-      /* We can't have a variable latency bypass, so
-	 remember to adjust the insn cost in adjust_cost hook.  */
-      sched_cfv4_bypass_data.pro = pro;
-      sched_cfv4_bypass_data.con = con;
-      sched_cfv4_bypass_data.scale = 1;
-      return 0;
-
-    case 2:
-    case 4:
-      return 1;
-
-    default:
-      return 0;
-    }
-}
-
-/* We generate a two-instructions program at M_TRAMP :
-	movea.l &CHAIN_VALUE,%a0
-	jmp FNADDR
-   where %a0 can be modified by changing STATIC_CHAIN_REGNUM.  */
-
-static void
-m68k_trampoline_init (rtx m_tramp, tree fndecl, rtx chain_value)
-{
-  rtx fnaddr = XEXP (DECL_RTL (fndecl), 0);
-  rtx mem;
-
-  gcc_assert (ADDRESS_REGNO_P (STATIC_CHAIN_REGNUM));
-
-  mem = adjust_address (m_tramp, HImode, 0);
-  emit_move_insn (mem, GEN_INT(0x207C + ((STATIC_CHAIN_REGNUM-8) << 9)));
-  mem = adjust_address (m_tramp, SImode, 2);
-  emit_move_insn (mem, chain_value);
-
-  mem = adjust_address (m_tramp, HImode, 6);
-  emit_move_insn (mem, GEN_INT(0x4EF9));
-  mem = adjust_address (m_tramp, SImode, 8);
-  emit_move_insn (mem, fnaddr);
-
-  FINALIZE_TRAMPOLINE (XEXP (m_tramp, 0));
-}
-
-/* On the 68000, the RTS insn cannot pop anything.
-   On the 68010, the RTD insn may be used to pop them if the number
-     of args is fixed, but if the number is variable then the caller
-     must pop them all.  RTD can't be used for library calls now
-     because the library is compiled with the Unix compiler.
-   Use of RTD is a selectable option, since it is incompatible with
-   standard Unix calling sequences.  If the option is not selected,
-   the caller must always pop the args.  */
-
-static int
-m68k_return_pops_args (tree fundecl, tree funtype, int size)
-{
-  return ((TARGET_RTD
-	   && (!fundecl
-	       || TREE_CODE (fundecl) != IDENTIFIER_NODE)
-	   && (!stdarg_p (funtype)))
-	  ? size : 0);
-}
-
-/* Make sure everything's fine if we *don't* have a given processor.
-   This assumes that putting a register in fixed_regs will keep the
-   compiler's mitts completely off it.  We don't bother to zero it out
-   of register classes.  */
-
-static void
-m68k_conditional_register_usage (void)
-{
-  int i;
-  HARD_REG_SET x;
   if (!TARGET_HARD_FLOAT)
     {
+      HARD_REG_SET x;
       COPY_HARD_REG_SET (x, reg_class_contents[(int)FP_REGS]);
       for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
         if (TEST_HARD_REG_BIT (x, i))
-	  fixed_regs[i] = call_used_regs[i] = 1;
+          fixed_regs[i] = call_used_regs[i] = 1;
     }
-  if (flag_pic)
-    fixed_regs[PIC_REG] = call_used_regs[PIC_REG] = 1;
 }
 
+#include "m68k-sched.inc"
+
 #include "gt-m68k.h"
diff -rupN gcc-4.6.4.test/gcc/config/m68k/m68k.h gcc-4.6.4-fastcall/gcc/config/m68k/m68k.h
--- gcc-4.6.4.test/gcc/config/m68k/m68k.h	2017-05-01 04:31:40.687947001 +0200
+++ gcc-4.6.4-fastcall/gcc/config/m68k/m68k.h	2017-04-30 19:10:52.556947001 +0200
@@ -143,6 +143,16 @@ along with GCC; see the file COPYING3.
 	  builtin_define ("__M68881__"); /* Non-standard */		\
 	}								\
 									\
+      if (TARGET_SHORT)                                                 \
+        {                                                               \
+      	  builtin_define ("__SHORT__"); /* Non-standard */		\
+        }                                                               \
+                                                                        \
+      if (TARGET_FASTCALL)                                              \
+        {                                                               \
+      	  builtin_define ("__FASTCALL__"); /* Non-standard */		\
+        }                                                               \
+                                                                        \
       if (TARGET_COLDFIRE)						\
 	{								\
 	  const char *tmp;						\
@@ -307,7 +317,7 @@ along with GCC; see the file COPYING3.
 
 #define UNITS_PER_WORD 4
 
-#define PARM_BOUNDARY (TARGET_SHORT ? 16 : 32)
+#define PARM_BOUNDARY ((TARGET_SHORT ||  (TARGET_FASTCALL && TUNE_68000_10)) ? 16 : 32)
 #define STACK_BOUNDARY 16
 #define FUNCTION_BOUNDARY 16
 #define EMPTY_FIELD_BOUNDARY 16
@@ -390,8 +400,8 @@ along with GCC; see the file COPYING3.
   1 }
 
 #define REG_ALLOC_ORDER		\
-{ /* d0/d1/a0/a1 */		\
-  0, 1, 8, 9,			\
+{ /* d0/d1/d2/a0/a1 */		\
+  0, 1, 8, 9,		\
   /* d2-d7 */			\
   2, 3, 4, 5, 6, 7,		\
   /* a2-a7/arg */		\
@@ -443,12 +453,12 @@ along with GCC; see the file COPYING3.
  */
 #define ARG_POINTER_REGNUM 24
 
-#define STATIC_CHAIN_REGNUM A0_REG
-#define M68K_STATIC_CHAIN_REG_NAME REGISTER_PREFIX "a0"
+#define STATIC_CHAIN_REGNUM (TARGET_FASTCALL ? A2_REG : A0_REG)
+#define M68K_STATIC_CHAIN_REG_NAME (TARGET_FASTCALL ? REGISTER_PREFIX "a2" : REGISTER_PREFIX "a0")
 
 /* Register in which address to store a structure value
    is passed to a function.  */
-#define M68K_STRUCT_VALUE_REGNUM A1_REG
+#define M68K_STRUCT_VALUE_REGNUM (TARGET_FASTCALL ? A0_REG : A1_REG)
 
 
 
@@ -492,7 +502,7 @@ extern enum reg_class regno_reg_class[];
 /* On the m68k, this is the size of MODE in words,
    except in the FP regs, where a single reg is always enough.  */
 #define CLASS_MAX_NREGS(CLASS, MODE)	\
- ((CLASS) == FP_REGS ? 1 \
+ ((CLASS) == FP_REGS ? GET_MODE_NUNITS (MODE) \
   : ((GET_MODE_SIZE (MODE) + UNITS_PER_WORD - 1) / UNITS_PER_WORD))
 
 /* Moves between fp regs and other regs are two insns.  */
@@ -517,11 +527,13 @@ extern enum reg_class regno_reg_class[];
 #define FIRST_PARM_OFFSET(FNDECL) 8
 
 /* On the m68k the return value defaults to D0.  */
-#define FUNCTION_VALUE(VALTYPE, FUNC)  \
-  gen_rtx_REG (TYPE_MODE (VALTYPE), D0_REG)
+#undef FUNCTION_VALUE
+#define FUNCTION_VALUE(VALTYPE, FUNC)					\
+  m68k_function_value (VALTYPE, FUNC, false)
 
 /* On the m68k the return value defaults to D0.  */
-#define LIBCALL_VALUE(MODE)  gen_rtx_REG (MODE, D0_REG)
+#define LIBCALL_VALUE(MODE)						\
+  m68k_libcall_value (MODE, false)
 
 /* On the m68k, D0 is usually the only register used.  */
 #define FUNCTION_VALUE_REGNO_P(N) ((N) == D0_REG)
@@ -532,15 +544,58 @@ extern enum reg_class regno_reg_class[];
 #define NEEDS_UNTYPED_CALL 0
 
 /* On the m68k, all arguments are usually pushed on the stack.  */
-#define FUNCTION_ARG_REGNO_P(N) 0
+/* 1 if N is a possible register number for function argument passing.  */
+#define FUNCTION_ARG_REGNO_P(N)			\
+  ((((int)N) >= 0 && (N) < M68K_FASTCALL_DATA_PARM)		\
+   || ((N) >= 8 && (N) < 8 + M68K_FASTCALL_ADDR_PARM)	\
+   || (TARGET_68881 && (N) >= 16 && (N) < 16 + M68K_FASTCALL_DATA_PARM))
 
-/* On the m68k, this is a single integer, which is a number of bytes
-   of arguments scanned so far.  */
-#define CUMULATIVE_ARGS int
-
-/* On the m68k, the offset starts at 0.  */
-#define INIT_CUMULATIVE_ARGS(CUM, FNTYPE, LIBNAME, INDIRECT, N_NAMED_ARGS) \
- ((CUM) = 0)
+   
+/* The number of data/float registers and address registers to use for
+   fast calls. */
+#define M68K_FASTCALL_DATA_PARM 3
+#define M68K_FASTCALL_ADDR_PARM 2
+
+// Call clobbered regs.
+#define M68K_STD_USED_REGS 2
+#define M68K_FASTCALL_USED_DATA_REGS 3
+#define M68K_FASTCALL_USED_ADDR_REGS 2
+
+/* On the m68k, this is a structure:
+   regs_already_used: bitmask of the already used registers.
+   last_arg_reg - register number of the most recently passed argument.
+     -1 if passed on stack.
+   last_arg_len - number of registers used by the most recently passed
+     argument.
+*/
+
+struct m68k_args
+{
+  long regs_already_used;
+  int last_arg_reg;
+  int last_arg_len;
+};
+
+#define CUMULATIVE_ARGS struct m68k_args
+
+/* The default number of data, address and float registers to use when
+   user specified '-mregparm' switch, not '-mregparm=<value>' option.  */
+
+#define ADJUST_REG_ALLOC_ORDER m68k_order_regs_for_local_alloc ()
+
+#define INIT_CUMULATIVE_ARGS(CUM, FNTYPE, LIBNAME, FNDECL, N_NAMED_ARGS) \
+  (m68k_init_cumulative_args (&(CUM), (FNTYPE)))
+
+#define FUNCTION_ARG_ADVANCE(CUM, MODE, TYPE, NAMED)	\
+  (m68k_function_arg_advance (&(CUM)))
+
+/* On m68k all args are pushed, except if -mfastcall then d0-2, a0-1 and
+   fp0-2 are used for passing the first arguments.
+   Note: by default, the static-chain is passed in a0. Targets that want
+   to make full use of '-mfastcall' are advised to pass the static-chain
+   somewhere else.  */
+#define FUNCTION_ARG(CUM, MODE, TYPE, NAMED) \
+  (m68k_function_arg (&(CUM), (MODE), (TYPE), (NAMED)))
 
 #define FUNCTION_PROFILER(FILE, LABELNO)  \
   asm_fprintf (FILE, "\tlea %LLP%d,%Ra0\n\tjsr mcount\n", (LABELNO))
@@ -587,6 +642,7 @@ extern enum reg_class regno_reg_class[];
    The function name __transfer_from_trampoline is not actually used.
    The function definition just permits use of "asm with operands"
    (though the operand list is empty).  */
+	  /*
 #define TRANSFER_FROM_TRAMPOLINE				\
 void								\
 __transfer_from_trampoline ()					\
@@ -598,6 +654,7 @@ __transfer_from_trampoline ()					\
   asm volatile ("move%.l %1,%0" : "=a" (a0) : "m" (a0[18]));	\
   asm ("rts":);							\
 }
+*/
 
 /* There are two registers that can always be eliminated on the m68k.
    The frame pointer and the arg pointer can be replaced by either the
diff -rupN gcc-4.6.4.test/gcc/config/m68k/m68k.md gcc-4.6.4-fastcall/gcc/config/m68k/m68k.md
--- gcc-4.6.4.test/gcc/config/m68k/m68k.md	2009-12-31 00:03:46.000000000 +0100
+++ gcc-4.6.4-fastcall/gcc/config/m68k/m68k.md	2017-04-30 19:10:52.557947001 +0200
@@ -131,6 +131,7 @@
   [(D0_REG		0)
    (A0_REG		8)
    (A1_REG		9)
+   (A2_REG		10)
    (PIC_REG		13)
    (A6_REG		14)
    (SP_REG		15)
diff -rupN gcc-4.6.4.test/gcc/config/m68k/m68k.opt gcc-4.6.4-fastcall/gcc/config/m68k/m68k.opt
--- gcc-4.6.4.test/gcc/config/m68k/m68k.opt	2009-05-18 09:54:44.000000000 +0200
+++ gcc-4.6.4-fastcall/gcc/config/m68k/m68k.opt	2017-04-30 19:10:52.557947001 +0200
@@ -147,6 +147,10 @@ mnoshort
 Target RejectNegative InverseMask(SHORT)
 Consider type 'int' to be 32 bits wide
 
+mnofastcall
+Target RejectNegative InverseMask(FASTCALL)
+Use standard calling conventions. 
+
 mpcrel
 Target Report Mask(PCREL)
 Generate pc-relative code
@@ -167,6 +171,10 @@ mshort
 Target Report Mask(SHORT)
 Consider type 'int' to be 16 bits wide
 
+mfastcall
+Target Report Mask(FASTCALL)
+Use calling convention passing arguments in registers.
+
 msoft-float
 Target RejectNegative InverseMask(HARD_FLOAT)
 Generate code with library calls for floating point
diff -rupN gcc-4.6.4.test/gcc/config/m68k/m68kelf.h gcc-4.6.4-fastcall/gcc/config/m68k/m68kelf.h
--- gcc-4.6.4.test/gcc/config/m68k/m68kelf.h	2010-12-22 13:06:01.000000000 +0100
+++ gcc-4.6.4-fastcall/gcc/config/m68k/m68kelf.h	2017-04-30 19:10:52.557947001 +0200
@@ -86,9 +86,9 @@ do {								\
    structure return, so have to use a1 for the static chain.  */
 
 #undef STATIC_CHAIN_REGNUM
-#define STATIC_CHAIN_REGNUM A1_REG
+#define STATIC_CHAIN_REGNUM (TARGET_FASTCALL ? A2_REG : A1_REG)
 #undef M68K_STATIC_CHAIN_REG_NAME
-#define M68K_STATIC_CHAIN_REG_NAME REGISTER_PREFIX "a1"
+#define M68K_STATIC_CHAIN_REG_NAME (TARGET_FASTCALL ? REGISTER_PREFIX "a2" : REGISTER_PREFIX "a1")
 
 #define ASM_COMMENT_START "|"
 
diff -rupN gcc-4.6.4.test/gcc/config/m68k/m68kemb.h gcc-4.6.4-fastcall/gcc/config/m68k/m68kemb.h
--- gcc-4.6.4.test/gcc/config/m68k/m68kemb.h	2007-04-15 17:54:36.000000000 +0200
+++ gcc-4.6.4-fastcall/gcc/config/m68k/m68kemb.h	2017-04-30 19:10:52.557947001 +0200
@@ -23,7 +23,7 @@
 
 #undef LIBCALL_VALUE
 #define LIBCALL_VALUE(MODE)					\
-  m68k_libcall_value (MODE)
+  m68k_libcall_value (MODE, true)
 
 #undef FUNCTION_VALUE_REGNO_P
 #define FUNCTION_VALUE_REGNO_P(N)			\
diff -rupN gcc-4.6.4.test/gcc/config/m68k/netbsd-elf.h gcc-4.6.4-fastcall/gcc/config/m68k/netbsd-elf.h
--- gcc-4.6.4.test/gcc/config/m68k/netbsd-elf.h	2010-12-09 17:28:45.000000000 +0100
+++ gcc-4.6.4-fastcall/gcc/config/m68k/netbsd-elf.h	2017-04-30 19:10:52.557947001 +0200
@@ -230,9 +230,9 @@ while (0)
    regnum.  Make it a1 instead.  */
 
 #undef STATIC_CHAIN_REGNUM
-#define STATIC_CHAIN_REGNUM A1_REG
+#define STATIC_CHAIN_REGNUM (TARGET_FASTCALL ? A2_REG : A1_REG)
 #undef M68K_STATIC_CHAIN_REG_NAME
-#define M68K_STATIC_CHAIN_REG_NAME REGISTER_PREFIX "a1"
+#define M68K_STATIC_CHAIN_REG_NAME (TARGET_FASTCALL ? REGISTER_PREFIX "a2" : REGISTER_PREFIX "a1")
 
 
 /* Now to renumber registers for dbx and gdb.
@@ -270,7 +270,7 @@ while (0)
 
 #undef FUNCTION_VALUE
 #define FUNCTION_VALUE(VALTYPE, FUNC)					\
-  m68k_function_value (VALTYPE, FUNC)
+  m68k_function_value (VALTYPE, FUNC, true)
 
 
 /* Define how to find the value returned by a library function
@@ -280,7 +280,7 @@ while (0)
 
 #undef LIBCALL_VALUE
 #define LIBCALL_VALUE(MODE)						\
-  m68k_libcall_value (MODE)
+  m68k_libcall_value (MODE, true)
 
 
 /* Boundary (in *bits*) on which stack pointer should be aligned.
diff -rupN gcc-4.6.4.test/gcc/config/m68k/t-mint gcc-4.6.4-fastcall/gcc/config/m68k/t-mint
--- gcc-4.6.4.test/gcc/config/m68k/t-mint	2017-05-01 04:31:40.688947001 +0200
+++ gcc-4.6.4-fastcall/gcc/config/m68k/t-mint	2017-04-30 19:10:52.557947001 +0200
@@ -2,9 +2,9 @@
 # Use multiple libraries
 # 
 
-MULTILIB_OPTIONS = m68020-60/mcpu=5475 mshort
+MULTILIB_OPTIONS = m68020-60 mshort
 
-MULTILIB_DIRNAMES = m68020-60 m5475 mshort
+MULTILIB_DIRNAMES = m68020-60 mfastcall mshort
 
 MULTILIB_MATCHES = \
 	m68020-60=m68881 \
@@ -21,21 +21,7 @@ MULTILIB_MATCHES = \
 	m68020-60=march?68020 \
 	m68020-60=march?68030 \
 	m68020-60=march?68040 \
-	m68020-60=march?68060 \
-	mcpu?5475=mcfv4e \
-	mcpu?5475=mcpu?5470 \
-	mcpu?5475=mcpu?5471 \
-	mcpu?5475=mcpu?5472 \
-	mcpu?5475=mcpu?5473 \
-	mcpu?5475=mcpu?5474 \
-	mcpu?5475=mcpu?547x \
-	mcpu?5475=mcpu?5480 \
-	mcpu?5475=mcpu?5481 \
-	mcpu?5475=mcpu?5482 \
-	mcpu?5475=mcpu?5483 \
-	mcpu?5475=mcpu?5484 \
-	mcpu?5475=mcpu?5485 \
-	mcpu?5475=mcpu?548x
+	m68020-60=march?68060
 
 LIBGCC = stmp-multilib
 INSTALL_LIBGCC = install-multilib
diff -rupN gcc-4.6.4.test/gcc/expr.c gcc-4.6.4-fastcall/gcc/expr.c
--- gcc-4.6.4.test/gcc/expr.c	2013-04-03 20:01:51.000000000 +0200
+++ gcc-4.6.4-fastcall/gcc/expr.c	2017-04-30 19:10:52.710947001 +0200
@@ -2333,6 +2333,19 @@ use_group_regs (rtx *call_fusage, rtx re
     }
 }
 
+/* Add a CLOBBER expression for REG to the (possibly empty) list pointed
+   to by CALL_FUSAGE.  REG must denote a hard register.  */
+
+void
+clobber_reg (rtx *call_fusage, rtx reg)
+{
+  gcc_assert (REG_P (reg) && REGNO (reg) < FIRST_PSEUDO_REGISTER);
+
+  *call_fusage
+    = gen_rtx_EXPR_LIST (VOIDmode,
+			 gen_rtx_CLOBBER (VOIDmode, reg), *call_fusage);
+}
+
 /* Return the defining gimple statement for SSA_NAME NAME if it is an
    assigment and the code of the expresion on the RHS is CODE.  Return
    NULL otherwise.  */
diff -rupN gcc-4.6.4.test/gcc/expr.h gcc-4.6.4-fastcall/gcc/expr.h
--- gcc-4.6.4.test/gcc/expr.h	2012-01-06 15:56:46.000000000 +0100
+++ gcc-4.6.4-fastcall/gcc/expr.h	2017-04-30 19:10:52.710947001 +0200
@@ -336,6 +336,9 @@ extern void use_regs (rtx *, int, int);
 /* Mark a PARALLEL as holding a parameter for the next CALL_INSN.  */
 extern void use_group_regs (rtx *, rtx);
 
+/* Mark REG as being clobbered for the next CALL_INSN.  */
+extern void clobber_reg (rtx *, rtx);
+
 /* Write zeros through the storage of OBJECT.
    If OBJECT has BLKmode, SIZE is its length in bytes.  */
 extern rtx clear_storage (rtx, rtx, enum block_op_methods);
